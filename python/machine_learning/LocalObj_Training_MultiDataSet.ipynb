{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double check the Path for storing trajectories is correct\n"
     ]
    }
   ],
   "source": [
    "#Import Packages\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from multicontact_learning_local_objectives.python.machine_learning.ml_utils import *\n",
    "import matplotlib.pyplot as plt #Matplotlib\n",
    "import shutil\n",
    "\n",
    "print(\"Double check the Path for storing trajectories is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Check we provide the Correct Traj Path: \n",
      " /home/jiayu/Desktop/MLP_DataSet/LargeSlope_TimeTrack_Angle_17_26/\n"
     ]
    }
   ],
   "source": [
    "#Define Path for Storing Trajectories\n",
    "#Collect Data Points Path\n",
    "#workingDirectory = \"/home/jiayu/Desktop/multicontact_learning_local_objectives/data/large_slope_flat_patches/\"\n",
    "#workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_DaggerExact/\"\n",
    "#workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_Add2Steps\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_Add2Step_KeepOutlier\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_AddVarSteps_1to2StepbeforeFail_RemovebyClip/\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_Add2Steps_1StepbeforeFail_RemovebyClip/\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/LargeSlope_Angle_17_26/\"\n",
    "workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/LargeSlope_TimeTrack_Angle_17_26/\"\n",
    "\n",
    "#NOTE: need to have \"/\" at the end\n",
    "print(\"Double Check we provide the Correct Traj Path: \\n\", workingDirectory)\n",
    "\n",
    "#Define dataset folder\n",
    "TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingInit\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug1Time_1StepBeforeFail_TrackTrainingExtra_InitSet\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug1Time_1StepBeforeFail_TrackTrainingInit_InitSet\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug1Time_2StepBeforeFail_TrackTrainingExtra_InitSet\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug1Time_2StepBeforeFail_TrackTrainingInit_InitSet\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug1Time_3StepBeforeFail_TrackTrainingExtra_InitSet\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug1Time_3StepBeforeFail_TrackTrainingInit_InitSet\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug2Time_1StepBeforeFail_TrackTrainingExtra_Aug_1Time\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug2Time_1StepBeforeFail_TrackTrainingInit_Aug_1Time\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug2Time_2StepBeforeFail_TrackTrainingExtra_Aug_1Time\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug2Time_2StepBeforeFail_TrackTrainingInit_Aug_1Time\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug2Time_3StepBeforeFail_TrackTrainingExtra_Aug_1Time\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug2Time_3StepBeforeFail_TrackTrainingInit_Aug_1Time\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug3Time_1StepBeforeFail_TrackTrainingExtra_Aug_2Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug3Time_1StepBeforeFail_TrackTrainingInit_Aug_2Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug3Time_2StepBeforeFail_TrackTrainingExtra_Aug_2Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug3Time_2StepBeforeFail_TrackTrainingInit_Aug_2Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug3Time_3StepBeforeFail_TrackTrainingExtra_Aug_2Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug3Time_3StepBeforeFail_TrackTrainingInit_Aug_2Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug4Time_1StepBeforeFail_TrackTrainingExtra_Aug_3Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug4Time_1StepBeforeFail_TrackTrainingInit_Aug_3Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug4Time_2StepBeforeFail_TrackTrainingExtra_Aug_3Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug4Time_2StepBeforeFail_TrackTrainingInit_Aug_3Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug4Time_3StepBeforeFail_TrackTrainingExtra_Aug_3Time_EarlyStopping\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug4Time_3StepBeforeFail_TrackTrainingInit_Aug_3Time_EarlyStopping\",]\n",
    "\n",
    "# TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingSet_Initial\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"TrainingAug2Steps_1StepbeforeFail_1Time_RemovebyClip\",]\n",
    "\n",
    "# TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingSet\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_1Time\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_2Time\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_3Time\"]\n",
    "\n",
    "ValidationSetPath = workingDirectory + \"/DataSet/\"+\"ValidationSet\"\n",
    "TestSetPath = workingDirectory + \"/DataSet/\"+\"TestSet\"\n",
    "\n",
    "#Path to store ML Model, create one if we dont have\n",
    "ML_Model_Path = workingDirectory + \"/ML_Models/\"\n",
    "if not (os.path.isdir(ML_Model_Path)):\n",
    "    os.mkdir(ML_Model_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Code\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset:  0\n",
      "DataSet Sizes: \n",
      "(38880, 85)\n",
      "(38880, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  1\n",
      "DataSet Sizes: \n",
      "(41539, 85)\n",
      "(41539, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  2\n",
      "DataSet Sizes: \n",
      "(44392, 85)\n",
      "(44392, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  3\n",
      "DataSet Sizes: \n",
      "(47469, 85)\n",
      "(47469, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  4\n",
      "DataSet Sizes: \n",
      "(50616, 85)\n",
      "(50616, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  5\n",
      "DataSet Sizes: \n",
      "(52775, 85)\n",
      "(52775, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  6\n",
      "DataSet Sizes: \n",
      "(54944, 85)\n",
      "(54944, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  7\n",
      "DataSet Sizes: \n",
      "(57191, 85)\n",
      "(57191, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  8\n",
      "DataSet Sizes: \n",
      "(59783, 85)\n",
      "(59783, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  9\n",
      "DataSet Sizes: \n",
      "(62562, 85)\n",
      "(62562, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  10\n",
      "DataSet Sizes: \n",
      "(65760, 85)\n",
      "(65760, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  11\n",
      "DataSet Sizes: \n",
      "(67971, 85)\n",
      "(67971, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  12\n",
      "DataSet Sizes: \n",
      "(70483, 85)\n",
      "(70483, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  13\n",
      "DataSet Sizes: \n",
      "(72384, 85)\n",
      "(72384, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  14\n",
      "DataSet Sizes: \n",
      "(74605, 85)\n",
      "(74605, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  15\n",
      "DataSet Sizes: \n",
      "(77160, 85)\n",
      "(77160, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  16\n",
      "DataSet Sizes: \n",
      "(79944, 85)\n",
      "(79944, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  17\n",
      "DataSet Sizes: \n",
      "(82026, 85)\n",
      "(82026, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  18\n",
      "DataSet Sizes: \n",
      "(84151, 85)\n",
      "(84151, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  19\n",
      "DataSet Sizes: \n",
      "(86034, 85)\n",
      "(86034, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  20\n",
      "DataSet Sizes: \n",
      "(88037, 85)\n",
      "(88037, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  21\n",
      "DataSet Sizes: \n",
      "(90308, 85)\n",
      "(90308, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  22\n",
      "DataSet Sizes: \n",
      "(92893, 85)\n",
      "(92893, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  23\n",
      "DataSet Sizes: \n",
      "(94708, 85)\n",
      "(94708, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  24\n",
      "DataSet Sizes: \n",
      "(96718, 85)\n",
      "(96718, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "Final Data Set Size\n",
      "(96718, 85)\n",
      "(96718, 14)\n",
      " \n",
      "Set Up for Validation Set\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      "Validation Set Size\n",
      "(4860, 85)\n",
      "(4860, 14)\n",
      " \n",
      " \n",
      "Set Up for Test Set\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      "Test Set Size\n",
      "(4860, 85)\n",
      "(4860, 14)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Load DataSet File\n",
    "\n",
    "#For training set\n",
    "for trainingset_idx in range(len(TrainingSetPath)):\n",
    "    trainingset_file = TrainingSetPath[trainingset_idx] + \"/data\"+'.p'\n",
    "    trainingset = pickle.load(open(trainingset_file,\"rb\"))\n",
    "    \n",
    "    print(\"For dataset: \", trainingset_idx)\n",
    "    print(\"DataSet Sizes: \")\n",
    "    \n",
    "    if trainingset_idx == 0:\n",
    "        x_train = trainingset[\"input\"]\n",
    "        y_train = trainingset[\"output\"]\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train,trainingset[\"input\"]),axis=0)\n",
    "        y_train = np.concatenate((y_train,trainingset[\"output\"]),axis=0)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    print(\"World Frame Shift: \", trainingset[\"Shift_World_Frame_Type\"])\n",
    "    print(\"Contact Location Representation Type: \",trainingset[\"Contact_Representation_Type\"])\n",
    "    print(\"Scaling Factor of Variables: \",trainingset[\"VectorScaleFactor\"])\n",
    "    print(\"Number of Preview Steps: \", trainingset[\"NumPreviewSteps\"])\n",
    "    print(\"Pre Process Mode: \",trainingset[\"PreProcessMode\"])\n",
    "    print(\" \")\n",
    "\n",
    "print(\"Final Data Set Size\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\" \")\n",
    "\n",
    "#For validation and Test\n",
    "\n",
    "#Load Validation Set and Test Set\n",
    "validationset_file = ValidationSetPath + \"/data\"+'.p'\n",
    "validationset = pickle.load(open(validationset_file,\"rb\"))\n",
    "\n",
    "testset_file = TestSetPath + \"/data\"+'.p'\n",
    "testset = pickle.load(open(testset_file,\"rb\"))\n",
    "\n",
    "x_valid = validationset[\"input\"]\n",
    "y_valid = validationset[\"output\"]\n",
    "\n",
    "x_test = testset[\"input\"]\n",
    "y_test = testset[\"output\"]\n",
    "\n",
    "print(\"Set Up for Validation Set\")\n",
    "print(\"World Frame Shift: \", validationset[\"Shift_World_Frame_Type\"])\n",
    "print(\"Contact Location Representation Type: \",validationset[\"Contact_Representation_Type\"])\n",
    "print(\"Scaling Factor of Variables: \",validationset[\"VectorScaleFactor\"])\n",
    "print(\"Number of Preview Steps: \", validationset[\"NumPreviewSteps\"])\n",
    "print(\"Pre Process Mode: \",validationset[\"PreProcessMode\"])\n",
    "print(\"Validation Set Size\")\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(\" \")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Set Up for Test Set\")\n",
    "print(\"World Frame Shift: \", testset[\"Shift_World_Frame_Type\"])\n",
    "print(\"Contact Location Representation Type: \",testset[\"Contact_Representation_Type\"])\n",
    "print(\"Scaling Factor of Variables: \",testset[\"VectorScaleFactor\"])\n",
    "print(\"Number of Preview Steps: \", testset[\"NumPreviewSteps\"])\n",
    "print(\"Pre Process Mode: \",testset[\"PreProcessMode\"])\n",
    "print(\"Test Set Size\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim:  85\n",
      "output dim: 14\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Decide input and outpu dimensionality\n",
    "d_in = x_train[0].shape[0]\n",
    "print(\"input dim: \", d_in)\n",
    "d_out = y_train[0].shape[0]\n",
    "print(\"output dim:\", d_out)\n",
    "print(\" \")\n",
    "\n",
    "# #Double check with mean and std\n",
    "# print(\"Inputs: \")\n",
    "# print(\"Input Mean: \", x_train.mean(axis=0))\n",
    "# print(\"Input Std: \", x_train.std(axis=0))\n",
    "# print(\"Input Max: \", x_train.max(axis=0))\n",
    "# print(\"Input Min: \", x_train.min(axis=0))\n",
    "# print(\" \")\n",
    "\n",
    "\n",
    "# print(\"Output Mean: \", y_train.mean(axis=0))\n",
    "# print(\"Output Std: \", y_train.std(axis=0))\n",
    "# print(\"Output Max: \", y_train.max(axis=0))\n",
    "# print(\"Output Min: \", y_train.min(axis=0))\n",
    "\n",
    "# print(\"Final Data Set Size\")\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define learning model\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,)),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(d_out)\n",
    "# ])\n",
    "# loss: 4.6886e-04 - val_loss: 5.4786e-04\n",
    "\n",
    "# #True code\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,)), #tanh\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(d_out, activation='linear')\n",
    "# ])\n",
    "\n",
    "# #True code\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,), kernel_regularizer='l1'), #tanh\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(d_out, activation='linear')\n",
    "# ])\n",
    "\n",
    "#True code\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(d_in,), ), #tanh\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(d_out, activation='linear')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 2/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 3/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 4/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 5/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 6/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 7/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 8/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 9/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 10/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 11/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 12/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 13/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 14/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 15/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 16/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 17/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 18/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 19/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 20/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 21/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 22/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 23/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 24/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 25/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 26/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 27/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 28/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 29/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 30/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 31/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 32/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 33/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 34/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 35/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 36/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 37/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 38/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 39/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 40/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 41/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 42/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 43/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 44/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 45/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 46/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 47/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 48/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 49/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 50/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 51/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 52/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 53/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 54/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 55/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 56/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 57/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 58/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 59/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 60/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 61/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 62/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 63/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 64/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 65/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 66/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 67/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 68/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 69/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 70/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 71/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 72/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 73/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 74/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 75/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 76/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 77/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 78/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 79/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 80/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 81/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 82/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 84/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 85/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 86/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 87/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 88/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 89/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 90/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 91/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 92/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 93/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 94/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 95/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 96/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 97/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 98/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 99/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 100/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 101/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 102/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 103/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 104/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 105/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 106/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 107/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 108/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 109/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 110/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 111/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 112/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 113/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 114/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 115/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 116/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 117/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 118/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 119/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 120/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 121/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 122/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 123/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 124/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 125/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 126/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 127/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 128/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 129/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 130/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 131/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 132/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 133/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 134/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 135/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 136/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 137/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 138/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 139/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 140/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 141/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 142/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 143/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 144/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 145/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 146/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 147/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 148/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 149/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 150/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 151/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 152/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 153/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 154/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 155/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 156/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 157/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 158/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 159/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 160/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 161/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 162/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 163/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 164/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 165/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 166/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 167/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 168/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 169/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 170/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 171/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 172/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 173/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 174/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 175/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 176/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 177/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 178/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 179/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 180/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 181/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 182/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 183/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 184/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 185/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 186/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 187/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 188/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 189/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 190/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 191/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 192/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 193/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 194/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 195/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 196/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 197/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 198/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 199/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 200/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 201/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 202/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 203/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 204/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 205/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 206/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 207/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 208/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0019\n",
      "Epoch 209/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 210/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 211/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 212/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 213/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 214/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 215/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 216/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 217/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 218/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 219/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 220/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 221/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 222/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 223/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 224/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 225/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 226/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 227/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 228/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 229/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 230/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 231/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 232/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 233/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 234/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 235/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 236/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 237/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 238/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 239/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 240/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 241/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 242/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 243/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 244/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 246/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 247/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 248/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 249/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 250/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 251/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 252/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 253/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 254/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 255/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 256/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 257/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 258/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 259/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 260/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 261/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 262/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 263/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 264/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 265/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 266/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 267/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 268/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 269/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 270/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 271/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 272/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 273/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 274/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 275/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 276/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 277/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 278/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 279/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 280/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 281/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 282/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 283/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 284/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 285/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 286/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 287/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 288/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 289/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 290/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 291/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 292/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 293/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 294/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 295/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 296/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 297/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 298/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 299/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 300/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 301/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 302/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 303/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 304/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 305/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 306/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 307/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 308/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 309/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 310/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 311/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 312/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 313/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 314/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 315/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 316/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 317/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 318/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 319/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 320/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 321/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 322/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 323/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 324/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 325/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 326/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 327/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 328/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 329/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 330/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 331/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 332/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 333/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 334/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 335/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 336/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 337/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 338/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 339/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 340/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 341/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 342/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 343/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 344/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 345/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 346/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 347/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 348/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 349/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 350/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 351/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 352/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 353/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 354/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 355/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 356/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 357/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 358/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 359/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 360/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 361/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 362/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 363/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 364/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 365/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 366/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 367/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 368/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 369/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 370/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 371/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 372/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 373/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 374/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 375/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 376/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 377/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 378/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 379/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 380/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 381/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 382/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 383/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 384/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 385/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 386/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 387/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 388/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 389/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 390/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 391/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 392/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 393/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 394/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 395/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 396/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 397/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 398/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 399/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 400/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 401/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 402/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 403/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 404/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 405/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 406/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 408/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 409/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 410/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 411/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 412/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 413/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 414/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 415/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 416/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 417/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 418/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 419/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 420/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 421/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 422/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 423/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 424/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 425/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 426/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 427/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 428/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 429/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 430/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 431/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 432/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 433/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 434/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 435/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 436/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 437/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 438/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 439/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 440/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 441/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 442/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 443/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 444/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 445/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 446/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 447/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 448/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 449/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 450/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 451/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 452/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 453/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 454/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 455/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 456/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 457/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 458/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 459/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 460/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 461/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 462/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 463/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 464/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 465/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 466/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 467/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 468/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 469/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 470/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 471/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 472/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 473/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 474/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 475/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 476/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 477/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 478/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 479/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 480/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 481/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 482/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 483/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 484/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 485/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 486/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 487/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 488/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 489/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 490/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 491/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 492/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 493/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 494/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 495/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 496/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 497/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 498/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 499/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 500/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "#Train Learning Model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0000075), loss='mse') #0.0001\n",
    "\n",
    "#history = model.fit(x_train, y_train, epochs = 50000, validation_split=0.0, batch_size = x_train.shape[0])\n",
    "#history = model.fit(x_train, y_train, epochs = 3000, validation_split=0.0, batch_size = 1280) #1280\n",
    "#Batch size = 1280 for remove outlier, 2560 for keep outlier\n",
    "history = model.fit(x = x_train, y = y_train, epochs =500, batch_size = 5120, validation_data = (x_valid, y_valid),shuffle=True) #1280, 1000 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018422575667500496\n"
     ]
    }
   ],
   "source": [
    "#Plot Training Progree\n",
    "plt.plot(history.history['loss'], 'r', label='loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(history.history['val_loss'], 'b', label='val_loss') if 'val_loss' in history.history else None\n",
    "plt.legend()\n",
    "plt.axhline(y=0.0018, xmin=0, xmax=5, linewidth=2, color = 'k')\n",
    "plt.show()\n",
    "print(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jiayu/Desktop/MLP_DataSet/LargeSlope_TimeTrack_Angle_17_26//ML_Models/NN_Model_Aug_4Time/assets\n"
     ]
    }
   ],
   "source": [
    "#Save Trained Model\n",
    "#MLmodel_name = \"NN_Model_Valid_\" + trainingset[\"PreProcessMode\"] + \"_Dagger_InitSet_2Iter\"\n",
    "#MLmodel_name = \"NN_Model\" + \"_\" + \"AugVarStep_1to2StepbeforeFail_3Time_RemovebyClip_SmallThre\"\n",
    "MLmodel_name = \"NN_Model\" + \"_\" + \"Aug_4Time\"\n",
    "model.save(ML_Model_Path + MLmodel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DataSet Setttings\n",
    "datasetSettings = {\"Shift_World_Frame_Type\":trainingset[\"Shift_World_Frame_Type\"],\n",
    "                   \"VectorScaleFactor\":trainingset[\"VectorScaleFactor\"],\n",
    "                   \"NumPreviewSteps\":trainingset[\"NumPreviewSteps\"],\n",
    "                   \"Contact_Representation_Type\":trainingset[\"Contact_Representation_Type\"],\n",
    "                   \"TrainingLoss\":history.history['loss']}\n",
    "#Validation loss\n",
    "datasetSettings[\"ValidationLoss\"] = history.history['val_loss'] if 'val_loss' in history.history else None\n",
    "\n",
    "#ProProcess\n",
    "datasetSettings[\"PreProcessMode\"] = trainingset[\"PreProcessMode\"]\n",
    "datasetSettings[\"Scaler_X\"] = trainingset[\"Scaler_X\"]\n",
    "\n",
    "datasetSettings[\"Scaler_Y\"] = trainingset[\"Scaler_Y\"]\n",
    "\n",
    "#Dump File\n",
    "pickle.dump(datasetSettings, open(ML_Model_Path + MLmodel_name+ '/datasetSettings' +'.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.16898236e-01  1.36893514e-02  7.78614581e-01  1.60702856e-01\n",
      " -1.00131502e-01 -2.82221670e-02 -4.23586116e-08 -4.66310963e-08\n",
      "  3.62160760e-09 -3.04074891e-01  1.84197779e-01  8.60246897e-02\n",
      "  1.00000000e+00  1.10000022e-01  1.12409997e+00  1.44698797e-01\n",
      " -4.64999978e-01  1.11437414e+00  2.23371473e-01 -4.64999978e-01\n",
      "  1.21929106e-01  1.00681383e-01  1.10000022e-01  1.31654942e-01\n",
      "  2.20087066e-02  1.10000022e-01  1.34375748e-01  1.02889919e-13\n",
      " -4.64999978e-01  1.34375748e-01  1.02889919e-13 -4.64999978e-01\n",
      " -8.73236732e-01  1.02876041e-13  1.10000022e-01 -8.73236732e-01\n",
      "  1.02876041e-13  6.85000022e-01  1.11519838e+00  2.16704166e-01\n",
      "  1.10000022e-01  1.12327573e+00  1.51366104e-01  1.10000022e-01\n",
      "  1.30830702e-01  2.86760141e-02  6.85000022e-01  1.22753345e-01\n",
      "  9.40140759e-02  6.85000022e-01  1.22180474e-01  9.86480555e-02\n",
      "  1.10000022e-01  1.31403573e-01  2.40420345e-02  1.10000022e-01\n",
      " -8.61041459e-01 -9.86480555e-02  6.85000022e-01 -8.70264558e-01\n",
      " -2.40420345e-02  1.26000002e+00  1.10686565e+00  2.84107910e-01\n",
      "  6.85000022e-01  1.10686565e+00  2.84107910e-01  6.85000022e-01\n",
      "  1.39163427e-01 -3.87277304e-02  1.26000002e+00  1.39163427e-01\n",
      " -3.87277304e-02  1.26000002e+00  1.54408070e-01 -1.62042256e-01\n",
      "  6.85000022e-01  1.54408070e-01 -1.62042256e-01  6.85000022e-01\n",
      " -8.93269055e-01  1.62042256e-01  1.26000002e+00 -8.93269055e-01\n",
      "  1.62042256e-01]\n",
      "Data Kept Original Form, But need to scale back to meters\n",
      "predicted result: \n",
      " [[ 0.12671407  0.04833624  0.7616413   0.16457236  0.12195302  0.00737609\n",
      "  -0.01008304 -0.00139124  0.00711869  0.31094134  0.09039401  0.46945179\n",
      "   0.709082    0.49556854]]\n",
      "true value: \n",
      " [ 1.12492340e-01  3.59975140e-02  7.32270816e-01  1.37151240e-01\n",
      "  1.02645252e-01 -1.49915180e-02  2.79827864e-04 -8.92999454e-05\n",
      "  4.78495838e-05  2.66982340e-01  6.00000357e-02  4.99998848e-01\n",
      "  7.00000290e-01  4.99999393e-01]\n",
      "diff: \n",
      " [[0.01422173 0.01233873 0.02937051 0.02742112 0.01930777 0.02236761\n",
      "  0.01036286 0.00130194 0.00707084 0.043959   0.03039398 0.03054706\n",
      "  0.00908172 0.00443085]]\n",
      "Neural Network Time:  0.04674839973449707\n"
     ]
    }
   ],
   "source": [
    "#Show Prediction Result for Training\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "datapoint_num = 0\n",
    "\n",
    "x_test = np.array([x_train[datapoint_num]])\n",
    "\n",
    "start = time.time()\n",
    "y_pred_temp = model.predict(x_test)\n",
    "end = time.time()\n",
    "\n",
    "print(x_train[datapoint_num])\n",
    "\n",
    "#Recover to original format\n",
    "if trainingset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_originalform = y_pred_temp/trainingset[\"VectorScaleFactor\"]\n",
    "    y_true_originalform = y_train[datapoint_num]/trainingset[\"VectorScaleFactor\"]\n",
    "elif trainingset[\"PreProcessMode\"] == \"Standarization\" or trainingset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    y_pred_originalform = dataset[\"Scaler_Y\"].inverse_transform(y_pred_temp)\n",
    "    y_true_originalform = dataset[\"Scaler_Y\"].inverse_transform(np.array([y_train[datapoint_num]]))\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "\n",
    "print(\"predicted result: \\n\",y_pred_originalform)\n",
    "print(\"true value: \\n\",y_true_originalform)\n",
    "print(\"diff: \\n\", np.absolute(y_pred_originalform - y_true_originalform))\n",
    "\n",
    "\n",
    "print(\"Neural Network Time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.39350669 0.39361919 0.39368588 0.39377377 0.39378715 0.39379598\n",
      " 0.39382639 0.39399616 0.39401449 0.39404428 0.39409824 0.39419678\n",
      " 0.39422182 0.39424177 0.39427567 0.39427646 0.39433654 0.39445311\n",
      " 0.3944786  0.3945325  0.39454502 0.39461596 0.39471665 0.39477476\n",
      " 0.3948205  0.3949957  0.39507741 0.39523784 0.39545445 0.39550179\n",
      " 0.39568079 0.39573832 0.39588872 0.39591768 0.39591924 0.39598398\n",
      " 0.3960172  0.39605065 0.39613502 0.39615698 0.39616735 0.39618027\n",
      " 0.3962296  0.39625033 0.39629595 0.39634794 0.39640241 0.39657865\n",
      " 0.39666362 0.39677161 0.39696105 0.39699708 0.39701704 0.39704756\n",
      " 0.3970667  0.39717595 0.39721997 0.39725666 0.39731583 0.39739774\n",
      " 0.39744326 0.3974725  0.39756463 0.39773263 0.39788975 0.39811118\n",
      " 0.39812111 0.39824355 0.39825574 0.39831421 0.39841442 0.39842047\n",
      " 0.398486   0.39866567 0.39868162 0.39876236 0.39876498 0.39881217\n",
      " 0.39885314 0.3988729  0.39895233 0.39904697 0.39904778 0.39915182\n",
      " 0.39916741 0.39919494 0.39942552 0.39945944 0.39947101 0.39950779\n",
      " 0.39960881 0.3997341  0.39984396 0.39986794 0.39992862 0.39997741\n",
      " 0.40010022 0.40017122 0.40021634 0.4002286  0.4002348  0.4002599\n",
      " 0.40026239 0.40034134 0.40042288 0.40077748 0.40077897 0.40078471\n",
      " 0.40102225 0.40103775 0.40105472 0.40111804 0.40112588 0.4012058\n",
      " 0.40121726 0.40121801 0.4012312  0.40124507 0.40130465 0.40131288\n",
      " 0.40135456 0.40136435 0.40144406 0.40165597 0.40169898 0.40170204\n",
      " 0.40188609 0.40195212 0.40201048 0.40203271 0.40213091 0.40218199\n",
      " 0.40225811 0.40237685 0.40238272 0.40252546 0.40271278 0.40283029\n",
      " 0.40283166 0.4028532  0.40296254 0.40298098 0.40299094 0.40309594\n",
      " 0.40310501 0.40311924 0.40312985 0.40317284 0.40322309 0.40325779\n",
      " 0.40334185 0.40334841 0.40344231 0.4034614  0.40347752 0.40353385\n",
      " 0.40358684 0.40362848 0.40367208 0.40393878 0.40395832 0.40401409\n",
      " 0.40401561 0.40401606 0.40408274 0.40409731 0.40413196 0.40414295\n",
      " 0.40421915 0.40427982 0.40433285 0.40437767 0.40440565 0.40440581\n",
      " 0.40446063 0.40449784 0.4045475  0.40462275 0.40469171 0.40473448\n",
      " 0.40477216 0.4050637  0.40510753 0.40513537 0.40514277 0.40536547\n",
      " 0.40546879 0.40551456 0.405587   0.40559219 0.40560585 0.40560626\n",
      " 0.40562114 0.40575908 0.40582993 0.40590234 0.40615423 0.40619877\n",
      " 0.40633624 0.40654282 0.40655087 0.40663091 0.40667085 0.40667614\n",
      " 0.40684695 0.40692202 0.40701721 0.4070564  0.40710301 0.40710464\n",
      " 0.40710889 0.40717448 0.40720711 0.40724005 0.40730518 0.40731075\n",
      " 0.40731963 0.40737541 0.40738121 0.40741367 0.40751131 0.40755678\n",
      " 0.4075997  0.40760269 0.40760853 0.40762673 0.40768838 0.40770446\n",
      " 0.40775941 0.40790021 0.40797044 0.40809633 0.40811669 0.40818414\n",
      " 0.40835484 0.40836207 0.40845684 0.4084844  0.40854286 0.40864062\n",
      " 0.40870814 0.40872162 0.40872598 0.40891935 0.40898036 0.40899494\n",
      " 0.40906067 0.40908652 0.40925582 0.40925833 0.409277   0.40930254\n",
      " 0.40944152 0.40987347 0.40990182 0.41006701 0.41012472 0.41014052\n",
      " 0.41020145 0.41030362 0.41031061 0.41032214 0.41040401 0.41049069\n",
      " 0.41050057 0.41058425 0.41059296 0.41067173 0.41067845 0.41081335\n",
      " 0.41083303 0.41089617 0.41095614 0.41097333 0.41098248 0.41101566\n",
      " 0.41132977 0.41136034 0.41156339 0.41172298 0.4119576  0.41196883\n",
      " 0.4119723  0.41214376 0.41252334 0.41254038 0.41260708 0.41261385\n",
      " 0.41268262 0.41282817 0.41294277 0.41297655 0.41301168 0.41303566\n",
      " 0.41307774 0.41314212 0.41314381 0.41320958 0.41322137 0.41327591\n",
      " 0.41335761 0.4134154  0.41347403 0.41350302 0.41351918 0.413707\n",
      " 0.41371012 0.41378098 0.41385926 0.41443454 0.41448108 0.41452119\n",
      " 0.41462632 0.41464099 0.41467638 0.41470426 0.41480464 0.41485987\n",
      " 0.41486253 0.41491042 0.41498902 0.41501741 0.41514094 0.41528965\n",
      " 0.41529915 0.41532721 0.41532941 0.41538304 0.41540912 0.41542396\n",
      " 0.41542759 0.41564396 0.41577296 0.4160076  0.41605691 0.41611995\n",
      " 0.41617708 0.41625403 0.41632724 0.41642211 0.41654218 0.41659391\n",
      " 0.41673061 0.41713057 0.41721241 0.41746239 0.41746816 0.41760075\n",
      " 0.41762828 0.41806435 0.41812496 0.418187   0.4185692  0.41859283\n",
      " 0.41861648 0.41862302 0.41874053 0.41884805 0.41890742 0.41892445\n",
      " 0.41897288 0.41898341 0.41921049 0.41923757 0.41926635 0.41948457\n",
      " 0.41963709 0.41969531 0.41972544 0.41991925 0.4200174  0.42001788\n",
      " 0.42006498 0.42011115 0.42024891 0.42039423 0.42044575 0.42046061\n",
      " 0.42050749 0.42053876 0.42054729 0.42065831 0.42076229 0.42079455\n",
      " 0.42095272 0.42096039 0.42107913 0.42108661 0.42113856 0.42117919\n",
      " 0.42142845 0.42145195 0.4215151  0.42163045 0.42164785 0.42167367\n",
      " 0.42173052 0.42178061 0.42186756 0.42187077 0.42189009 0.42212182\n",
      " 0.42239568 0.42241165 0.42247925 0.42255409 0.4226903  0.42271473\n",
      " 0.42276832 0.42277486 0.42306263 0.42321038 0.42321193 0.42334575\n",
      " 0.42351654 0.42356835 0.42364801 0.42364892 0.42374644 0.42392418\n",
      " 0.42394233 0.42400216 0.42408297 0.42411061 0.4241141  0.42424479\n",
      " 0.4244278  0.42445698 0.42450074 0.42453387 0.42454171 0.4245439\n",
      " 0.42455021 0.42462159 0.42464981 0.42480931 0.42481278 0.42481701\n",
      " 0.42498477 0.42500494 0.42502862 0.42510874 0.42520526 0.42521195\n",
      " 0.42531586 0.42545045 0.42549186 0.42555367 0.42564657 0.42565814\n",
      " 0.4257517  0.42579087 0.42583527 0.42585956 0.42589743 0.42594321\n",
      " 0.42607202 0.42609756 0.42613176 0.42617815 0.42621222 0.42623257\n",
      " 0.42627541 0.42647494 0.42652985 0.42655296 0.42667497 0.42680724\n",
      " 0.42682359 0.42693176 0.42697662 0.42732327 0.42742207 0.42747513\n",
      " 0.4274833  0.42749906 0.42781903 0.42782329 0.42783076 0.42799026\n",
      " 0.42799864 0.42813089 0.42825462 0.42827346 0.42831432 0.42842547\n",
      " 0.42851348 0.42856787 0.42868621 0.42870516 0.42872888 0.42882359\n",
      " 0.42884293 0.42884387 0.42903188 0.42920974 0.42938902 0.42942519\n",
      " 0.42947804 0.42961817 0.42979467 0.4298075  0.4302228  0.43023264\n",
      " 0.43025155 0.43027695 0.43033596 0.43034495 0.43044594 0.43053202\n",
      " 0.4306241  0.43064871 0.43072551 0.4307962  0.43089898 0.43096227\n",
      " 0.43098619 0.43141086 0.43144036 0.43145047 0.43145585 0.43160435\n",
      " 0.43176614 0.43205761 0.43209507 0.43211016 0.43219563 0.43241198\n",
      " 0.43248579 0.4324971  0.43265277 0.43277295 0.43290946 0.43296666\n",
      " 0.43305554 0.43328102 0.43343412 0.43357369 0.43361269 0.433648\n",
      " 0.4337158  0.43384822 0.43412875 0.4343197  0.43436458 0.43447368\n",
      " 0.4344797  0.43462304 0.43467069 0.43485396 0.43490258 0.43494906\n",
      " 0.43505894 0.4351585  0.43517041 0.4352679  0.43559169 0.43565635\n",
      " 0.43609194 0.43641804 0.43643613 0.43650624 0.43679561 0.43681509\n",
      " 0.43699094 0.43715906 0.43737554 0.4374131  0.43744073 0.4375061\n",
      " 0.4379025  0.43817166 0.4382476  0.43855515 0.43855662 0.43858017\n",
      " 0.43861211 0.43864683 0.43872636 0.43874065 0.4388705  0.43896899\n",
      " 0.438971   0.43900111 0.43901072 0.43916987 0.43917674 0.43924445\n",
      " 0.43936408 0.43948284 0.4394913  0.4394964  0.43966032 0.43970632\n",
      " 0.43999743 0.4400041  0.44016719 0.44048154 0.44054622 0.44075933\n",
      " 0.44083737 0.44084789 0.44087495 0.44100751 0.44106715 0.44109508\n",
      " 0.44118849 0.44122238 0.44123802 0.44139165 0.44143263 0.44163961\n",
      " 0.44181491 0.44184652 0.44200356 0.44203797 0.44260298 0.44270399\n",
      " 0.44272093 0.4429474  0.44301437 0.4431668  0.44330828 0.44332161\n",
      " 0.44352976 0.44378356 0.44401458 0.44412834 0.44441055 0.44445792\n",
      " 0.44454267 0.4446061  0.44470647 0.44470894 0.44491578 0.44496788\n",
      " 0.44499768 0.44510454 0.44522089 0.44522925 0.44543046 0.44543298\n",
      " 0.44575698 0.44590463 0.44590724 0.44603617 0.44642295 0.44643912\n",
      " 0.44665756 0.44665962 0.44668493 0.44669123 0.44674313 0.44677422\n",
      " 0.44682338 0.44686785 0.44696529 0.44700737 0.44715204 0.44725372\n",
      " 0.44728722 0.44730354 0.4474524  0.44750009 0.44762865 0.4479055\n",
      " 0.44812685 0.44815964 0.4482068  0.44823123 0.44828038 0.44830707\n",
      " 0.44831221 0.44833616 0.4485134  0.44884545 0.44916196 0.4492233\n",
      " 0.44947676 0.44957896 0.44971459 0.4497566  0.44987002 0.45014438\n",
      " 0.45016275 0.45024603 0.4503254  0.45036705 0.45040523 0.4506477\n",
      " 0.45080484 0.45090846 0.45134398 0.45147929 0.45157474 0.45162592\n",
      " 0.45168568 0.45182091 0.45259166 0.45286155 0.4528973  0.45297301\n",
      " 0.4529766  0.45307791 0.45313842 0.45323976 0.45330802 0.45331644\n",
      " 0.45336415 0.45359978 0.45386064 0.4538885  0.45424912 0.45434612\n",
      " 0.45472689 0.45477263 0.45486303 0.45494951 0.45497208 0.45498846\n",
      " 0.45502667 0.45503288 0.45548679 0.45568696 0.45581967 0.455966\n",
      " 0.45603955 0.45632909 0.45645836 0.4565556  0.45671534 0.45697392\n",
      " 0.45698077 0.45699681 0.45739603 0.45740576 0.45764204 0.45764885\n",
      " 0.45788993 0.45803534 0.45831699 0.45862054 0.4587387  0.45888943\n",
      " 0.45908463 0.45936968 0.45954749 0.46018293 0.46032103 0.46042685\n",
      " 0.46072836 0.46076043 0.46082508 0.46086803 0.46091782 0.46100309\n",
      " 0.46103754 0.46113495 0.46148474 0.46194256 0.46196507 0.46200274\n",
      " 0.46240362 0.46263558 0.46289247 0.46337958 0.46352908 0.46427845\n",
      " 0.46445055 0.46445432 0.46448386 0.46466869 0.46542912 0.46581082\n",
      " 0.46597163 0.46647347 0.46664985 0.46674969 0.4668609  0.46743973\n",
      " 0.46794916 0.46849949 0.4685274  0.46873933 0.46898889 0.46928833\n",
      " 0.469445   0.46980201 0.46984236 0.47047546 0.47173145 0.47236619\n",
      " 0.47269624 0.47290339 0.4729362  0.4729507  0.47311834 0.47321413\n",
      " 0.47367498 0.47379033 0.47393128 0.47401225 0.47448222 0.4746746\n",
      " 0.47475599 0.4747917  0.47498059 0.47527714 0.4755627  0.47562887\n",
      " 0.47570183 0.47624771 0.47630776 0.47639677 0.47643303 0.47672277\n",
      " 0.47712046 0.4773432  0.47742625 0.47752398 0.47766605 0.47770464\n",
      " 0.4781155  0.47925943 0.47933782 0.47973099 0.48058408 0.48195416\n",
      " 0.48214311 0.4825958  0.48330929 0.48332707 0.48340631 0.4836901\n",
      " 0.4838775  0.48409362 0.48474143 0.48496457 0.48518553 0.48536756\n",
      " 0.48575532 0.48615391 0.48738948 0.48750091 0.48785705 0.4879955\n",
      " 0.48845171 0.48920864 0.49000941 0.49015234 0.49079598 0.49086095\n",
      " 0.4910984  0.49110067 0.49126914 0.49192426 0.49225109 0.4924732\n",
      " 0.49257799 0.49276997 0.49329266 0.49400284 0.49482085 0.49483707\n",
      " 0.49620391 0.49644106 0.49645157 0.49660975 0.49713649 0.49729488\n",
      " 0.4975526  0.49767477 0.49794565 0.49802991 0.4992398  0.49948677\n",
      " 0.49952907 0.501408   0.50247314 0.5028077  0.50358839 0.50425787\n",
      " 0.50508135 0.505671   0.50606759 0.50621735 0.50666645 0.50706574\n",
      " 0.50777981 0.50898828 0.50974236 0.51058287 0.51098857 0.51126981\n",
      " 0.51177873 0.51269656 0.51273964 0.51281442 0.51311668 0.51369376\n",
      " 0.51420286 0.51450534 0.51479755 0.5151109  0.51518364 0.51543988\n",
      " 0.51658264 0.51660334 0.51687939 0.51725916 0.51822123 0.51850994\n",
      " 0.51870152 0.51989532 0.52096175 0.52195521 0.52220154 0.52226256\n",
      " 0.5228301  0.52297359 0.52311238 0.52362712 0.52488823 0.52512668\n",
      " 0.52554633 0.52579567 0.52609251 0.52705376 0.52727183 0.52850024\n",
      " 0.52925154 0.53090761 0.53388576 0.53468091 0.53595829 0.53654309\n",
      " 0.53686695 0.5379217  0.53812277 0.54031124 0.54058208 0.54069894\n",
      " 0.54091553 0.54112911 0.54138041 0.54216305 0.54427872 0.54560719\n",
      " 0.5469497  0.5484489  0.54884757 0.549206   0.54968764 0.54980919\n",
      " 0.55121452 0.55172166 0.55181605 0.55589387 0.55594202 0.556219\n",
      " 0.55791299 0.55821445 0.55822956 0.5584521  0.55866344 0.55892179\n",
      " 0.55967956 0.56026767 0.56040969 0.5612945  0.56181651 0.5618275\n",
      " 0.56217872 0.56381845 0.56410269 0.5647952  0.56638959 0.5686217\n",
      " 0.56937829 0.57100066 0.57279465 0.57638821 0.57694241 0.57705108\n",
      " 0.57709984 0.57768551 0.57870224 0.58074063 0.58988128 0.59233403\n",
      " 0.59685351 0.60422585 0.60786173 0.60813393 0.60849469 0.60888461\n",
      " 0.63817095 0.63838238 0.65175825 0.69396988]\n",
      "Error Mean:  0.0662062536509052\n",
      "Error Std 0.07805020617975536\n",
      "[0.36385915 0.36400002 0.36481478 0.36580992 0.36678662 0.36701109\n",
      " 0.3687089  0.36926892 0.36965846 0.37153292 0.3715448  0.37159693\n",
      " 0.37297087 0.37414373 0.37429495 0.37470635 0.37593624 0.37637029\n",
      " 0.37747954 0.37819147 0.37999744 0.38120899 0.38386165 0.38467512\n",
      " 0.38655168 0.38710309 0.38825222 0.38891443 0.39145389 0.39242399\n",
      " 0.39301663 0.39427646 0.3944786  0.39461596 0.39625033 0.39739774\n",
      " 0.39868162 0.39904778 0.40017122 0.40195212 0.40446063 0.4045475\n",
      " 0.40762673 0.40768838 0.40770446 0.40809633 0.40811669 0.4084844\n",
      " 0.41301168 0.41314381 0.41963709 0.42039423 0.42113856 0.42117919\n",
      " 0.42374644 0.4257517  0.42609756 0.42682359 0.42697662 0.42782329\n",
      " 0.42884387 0.43033596 0.43160435 0.43296666 0.43467069 0.43641804\n",
      " 0.43855662 0.43861211 0.44048154 0.44106715 0.44750009 0.44833616\n",
      " 0.44884545 0.45764204 0.46082508 0.46200274 0.4668609  0.47269624\n",
      " 0.47311834 0.47321413 0.47527714 0.47562887 0.47752398 0.48195416\n",
      " 0.4838775  0.48409362 0.49079598 0.49329266 0.49660975 0.50898828\n",
      " 0.51058287 0.51311668 0.51687939 0.52226256 0.52512668 0.54138041\n",
      " 0.55121452 0.55821445 0.56040969 0.56381845]\n",
      "[10 16  6  1 10  3  1  3 15 10 14  3  8 17  2 20  8 17 14  6 22 19  9  7\n",
      "  9  9  6 20 12  4 26  3 19 22 13 16 10  7 15 21 21  1 21  8  3 28  0 26\n",
      "  1 23 21 14 25  0  4  4 14 21  2 16 14 10 25 20 12  8 26  3 15 13  2 14\n",
      " 20 26 21 19 26  2  9 14  1 20 26  4  3 26 19 22  1 13  2  0 26  4 25  8\n",
      "  8  1 26 27]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3., 7., 5., 7., 5., 0., 3., 2., 6., 4., 5., 0., 2., 3., 0., 7., 3.,\n",
       "        3., 2., 0., 4., 5., 6., 3., 1., 0., 3., 9., 1., 1.]),\n",
       " array([ 0.        ,  0.93333333,  1.86666667,  2.8       ,  3.73333333,\n",
       "         4.66666667,  5.6       ,  6.53333333,  7.46666667,  8.4       ,\n",
       "         9.33333333, 10.26666667, 11.2       , 12.13333333, 13.06666667,\n",
       "        14.        , 14.93333333, 15.86666667, 16.8       , 17.73333333,\n",
       "        18.66666667, 19.6       , 20.53333333, 21.46666667, 22.4       ,\n",
       "        23.33333333, 24.26666667, 25.2       , 26.13333333, 27.06666667,\n",
       "        28.        ]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Training Set\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred_train = model.predict(x_train)\n",
    "\n",
    "if trainingset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_train_originalform = y_pred_train/trainingset[\"VectorScaleFactor\"]\n",
    "    y_true_train_originalform = y_train/trainingset[\"VectorScaleFactor\"]\n",
    "elif trainingset[\"PreProcessMode\"] == \"Standarization\" or trainingset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", trainingset[\"PreProcessMode\"])\n",
    "    y_pred_train_originalform = trainingset[\"Scaler_Y\"].inverse_transform(y_pred_train)\n",
    "    y_true_train_originalform = trainingset[\"Scaler_Y\"].inverse_transform(y_train)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "#err = np.linalg.norm(y_true_train_originalform[:,-3:]-y_pred_train_originalform[:,-3:], axis=1)\n",
    "err = np.linalg.norm(y_true_train_originalform[:,-3:]-y_pred_train_originalform[:,-3:], axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted[-1000:])  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())\n",
    "\n",
    "##Plot prediction on the initial dataset\n",
    "err_initdata=err[0:12000+1]\n",
    "\n",
    "err_initdata_sorted = np.sort(err_initdata)\n",
    "print(err_initdata_sorted[-100:])  # print the 100 biggest error\n",
    "\n",
    "err_initdata_idx_sorted = np.argsort(err_initdata)\n",
    "print(err_initdata_idx_sorted[-100:]%30)\n",
    "selected_err=err_initdata_idx_sorted[-100:]%30\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(selected_err, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.0152655  0.01924194 0.01993528 ... 1.37106785 1.56014468 1.57593845]\n",
      "Error Mean:  0.1182508332592285\n",
      "Error Std 0.10569500468976349\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Validation Set\n",
    "\n",
    "y_pred_valid = model.predict(x_valid)\n",
    "\n",
    "\n",
    "if validationset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_valid_originalform = y_pred_valid/validationset[\"VectorScaleFactor\"]\n",
    "    y_true_valid_originalform = y_valid/validationset[\"VectorScaleFactor\"]\n",
    "elif validationset[\"PreProcessMode\"] == \"Standarization\" or validationset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", validationset[\"PreProcessMode\"])\n",
    "    y_pred_valid_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_pred_valid)\n",
    "    y_true_valid_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_valid)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_true_valid_originalform-y_pred_valid_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted)  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.25184772e-02 -1.47172540e-01  1.24203298e-01 ... -2.53796893e-02\n",
      "   9.51831367e-04  2.11604073e-01]\n",
      " [-5.57569303e-04  8.52804604e-02  3.60013521e-02 ... -2.89573721e-01\n",
      "  -7.58581505e-01  1.36294719e-01]\n",
      " [ 9.30173466e-02 -2.70568120e-02  1.18602778e-01 ... -1.10852825e-02\n",
      "  -1.84420381e-02 -5.19030573e-02]\n",
      " ...\n",
      " [ 1.23190557e-01 -6.72411612e-02  4.32211888e-02 ...  2.71126215e-03\n",
      "  -6.25478368e-03  7.58849170e-02]\n",
      " [ 1.81226783e-01  9.48602851e-02  1.54669764e-01 ... -2.16014314e-01\n",
      "  -8.20502761e-01 -4.03573832e-01]\n",
      " [ 8.62224370e-02 -8.37325195e-02  2.49926418e-02 ... -5.07909210e-03\n",
      "  -5.48972966e-03  7.41409614e-02]]\n",
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.03858617 0.0447075  0.04813154 ... 2.2925241  2.32051504 2.39680686]\n",
      "Error Mean:  0.8388263259099799\n",
      "Error Std 0.3953971995084884\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Test Set\n",
    "\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "if testset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_test_originalform = y_pred_test/testset[\"VectorScaleFactor\"]\n",
    "    y_true_test_originalform = y_test/testset[\"VectorScaleFactor\"]\n",
    "elif testset[\"PreProcessMode\"] == \"Standarization\" or testset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", validationset[\"PreProcessMode\"])\n",
    "    y_pred_test_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_pred_test)\n",
    "    y_true_test_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_test)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_pred_test_originalform-y_true_test_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted)  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a501000252d127e7c27d83f75df0a57bca228f59033b739034a7cde4260d0152"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
