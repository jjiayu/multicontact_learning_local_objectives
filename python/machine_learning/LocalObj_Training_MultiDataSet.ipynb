{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double check the Path for storing trajectories is correct\n"
     ]
    }
   ],
   "source": [
    "#Import Packages\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from multicontact_learning_local_objectives.python.machine_learning.ml_utils import *\n",
    "import matplotlib.pyplot as plt #Matplotlib\n",
    "import shutil\n",
    "\n",
    "print(\"Double check the Path for storing trajectories is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Check we provide the Correct Traj Path: \n",
      " /home/jiayu/Desktop/MLP_DataSet/LargeSlope_TimeTrack_Angle_17_26/\n"
     ]
    }
   ],
   "source": [
    "#Define Path for Storing Trajectories\n",
    "#Collect Data Points Path\n",
    "#workingDirectory = \"/home/jiayu/Desktop/multicontact_learning_local_objectives/data/large_slope_flat_patches/\"\n",
    "#workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_DaggerExact/\"\n",
    "#workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_Add2Steps\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_Add2Step_KeepOutlier\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_AddVarSteps_1to2StepbeforeFail_RemovebyClip/\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_Add2Steps_1StepbeforeFail_RemovebyClip/\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/LargeSlope_Angle_17_26/\"\n",
    "workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/LargeSlope_TimeTrack_Angle_17_26/\"\n",
    "\n",
    "#NOTE: need to have \"/\" at the end\n",
    "print(\"Double Check we provide the Correct Traj Path: \\n\", workingDirectory)\n",
    "\n",
    "#Define dataset folder\n",
    "TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingInit\"]\n",
    "\n",
    "# TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingSet_Initial\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"TrainingAug2Steps_1StepbeforeFail_1Time_RemovebyClip\",]\n",
    "\n",
    "# TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingSet\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_1Time\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_2Time\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_3Time\"]\n",
    "\n",
    "ValidationSetPath = workingDirectory + \"/DataSet/\"+\"ValidationSet\"\n",
    "TestSetPath = workingDirectory + \"/DataSet/\"+\"TestSet\"\n",
    "\n",
    "#Path to store ML Model, create one if we dont have\n",
    "ML_Model_Path = workingDirectory + \"/ML_Models/\"\n",
    "if not (os.path.isdir(ML_Model_Path)):\n",
    "    os.mkdir(ML_Model_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Code\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset:  0\n",
      "DataSet Sizes: \n",
      "(38880, 85)\n",
      "(38880, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "Final Data Set Size\n",
      "(38880, 85)\n",
      "(38880, 14)\n",
      " \n",
      "Set Up for Validation Set\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      "Validation Set Size\n",
      "(4860, 85)\n",
      "(4860, 14)\n",
      " \n",
      " \n",
      "Set Up for Test Set\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      "Test Set Size\n",
      "(4860, 85)\n",
      "(4860, 14)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Load DataSet File\n",
    "\n",
    "#For training set\n",
    "for trainingset_idx in range(len(TrainingSetPath)):\n",
    "    trainingset_file = TrainingSetPath[trainingset_idx] + \"/data\"+'.p'\n",
    "    trainingset = pickle.load(open(trainingset_file,\"rb\"))\n",
    "    \n",
    "    print(\"For dataset: \", trainingset_idx)\n",
    "    print(\"DataSet Sizes: \")\n",
    "    \n",
    "    if trainingset_idx == 0:\n",
    "        x_train = trainingset[\"input\"]\n",
    "        y_train = trainingset[\"output\"]\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train,trainingset[\"input\"]),axis=0)\n",
    "        y_train = np.concatenate((y_train,trainingset[\"output\"]),axis=0)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    print(\"World Frame Shift: \", trainingset[\"Shift_World_Frame_Type\"])\n",
    "    print(\"Contact Location Representation Type: \",trainingset[\"Contact_Representation_Type\"])\n",
    "    print(\"Scaling Factor of Variables: \",trainingset[\"VectorScaleFactor\"])\n",
    "    print(\"Number of Preview Steps: \", trainingset[\"NumPreviewSteps\"])\n",
    "    print(\"Pre Process Mode: \",trainingset[\"PreProcessMode\"])\n",
    "    print(\" \")\n",
    "\n",
    "print(\"Final Data Set Size\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\" \")\n",
    "\n",
    "#For validation and Test\n",
    "\n",
    "#Load Validation Set and Test Set\n",
    "validationset_file = ValidationSetPath + \"/data\"+'.p'\n",
    "validationset = pickle.load(open(validationset_file,\"rb\"))\n",
    "\n",
    "testset_file = TestSetPath + \"/data\"+'.p'\n",
    "testset = pickle.load(open(testset_file,\"rb\"))\n",
    "\n",
    "x_valid = validationset[\"input\"]\n",
    "y_valid = validationset[\"output\"]\n",
    "\n",
    "x_test = testset[\"input\"]\n",
    "y_test = testset[\"output\"]\n",
    "\n",
    "print(\"Set Up for Validation Set\")\n",
    "print(\"World Frame Shift: \", validationset[\"Shift_World_Frame_Type\"])\n",
    "print(\"Contact Location Representation Type: \",validationset[\"Contact_Representation_Type\"])\n",
    "print(\"Scaling Factor of Variables: \",validationset[\"VectorScaleFactor\"])\n",
    "print(\"Number of Preview Steps: \", validationset[\"NumPreviewSteps\"])\n",
    "print(\"Pre Process Mode: \",validationset[\"PreProcessMode\"])\n",
    "print(\"Validation Set Size\")\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(\" \")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Set Up for Test Set\")\n",
    "print(\"World Frame Shift: \", testset[\"Shift_World_Frame_Type\"])\n",
    "print(\"Contact Location Representation Type: \",testset[\"Contact_Representation_Type\"])\n",
    "print(\"Scaling Factor of Variables: \",testset[\"VectorScaleFactor\"])\n",
    "print(\"Number of Preview Steps: \", testset[\"NumPreviewSteps\"])\n",
    "print(\"Pre Process Mode: \",testset[\"PreProcessMode\"])\n",
    "print(\"Test Set Size\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim:  85\n",
      "output dim: 14\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Decide input and outpu dimensionality\n",
    "d_in = x_train[0].shape[0]\n",
    "print(\"input dim: \", d_in)\n",
    "d_out = y_train[0].shape[0]\n",
    "print(\"output dim:\", d_out)\n",
    "print(\" \")\n",
    "\n",
    "# #Double check with mean and std\n",
    "# print(\"Inputs: \")\n",
    "# print(\"Input Mean: \", x_train.mean(axis=0))\n",
    "# print(\"Input Std: \", x_train.std(axis=0))\n",
    "# print(\"Input Max: \", x_train.max(axis=0))\n",
    "# print(\"Input Min: \", x_train.min(axis=0))\n",
    "# print(\" \")\n",
    "\n",
    "\n",
    "# print(\"Output Mean: \", y_train.mean(axis=0))\n",
    "# print(\"Output Std: \", y_train.std(axis=0))\n",
    "# print(\"Output Max: \", y_train.max(axis=0))\n",
    "# print(\"Output Min: \", y_train.min(axis=0))\n",
    "\n",
    "# print(\"Final Data Set Size\")\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define learning model\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,)),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(d_out)\n",
    "# ])\n",
    "# loss: 4.6886e-04 - val_loss: 5.4786e-04\n",
    "\n",
    "# #True code\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,)), #tanh\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(d_out, activation='linear')\n",
    "# ])\n",
    "\n",
    "# #True code\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,), kernel_regularizer='l1'), #tanh\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(d_out, activation='linear')\n",
    "# ])\n",
    "\n",
    "#True code\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(d_in,), ), #tanh\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(d_out, activation='linear')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 8.1352e-04 - val_loss: 0.0017\n",
      "Epoch 2/300\n",
      "31/31 [==============================] - ETA: 0s - loss: 7.6291e-0 - 0s 2ms/step - loss: 7.6174e-04 - val_loss: 0.0018\n",
      "Epoch 3/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5891e-04 - val_loss: 0.0017\n",
      "Epoch 4/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6319e-04 - val_loss: 0.0018\n",
      "Epoch 5/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6262e-04 - val_loss: 0.0017\n",
      "Epoch 6/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6096e-04 - val_loss: 0.0017\n",
      "Epoch 7/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5920e-04 - val_loss: 0.0017\n",
      "Epoch 8/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6284e-04 - val_loss: 0.0017\n",
      "Epoch 9/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5629e-04 - val_loss: 0.0017\n",
      "Epoch 10/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6292e-04 - val_loss: 0.0018\n",
      "Epoch 11/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6798e-04 - val_loss: 0.0018\n",
      "Epoch 12/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6511e-04 - val_loss: 0.0018\n",
      "Epoch 13/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6768e-04 - val_loss: 0.0018\n",
      "Epoch 14/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.7724e-04 - val_loss: 0.0017\n",
      "Epoch 15/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6284e-04 - val_loss: 0.0017\n",
      "Epoch 16/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6891e-04 - val_loss: 0.0017\n",
      "Epoch 17/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6093e-04 - val_loss: 0.0017\n",
      "Epoch 18/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6020e-04 - val_loss: 0.0017\n",
      "Epoch 19/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6069e-04 - val_loss: 0.0018\n",
      "Epoch 20/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6041e-04 - val_loss: 0.0018\n",
      "Epoch 21/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5940e-04 - val_loss: 0.0017\n",
      "Epoch 22/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6098e-04 - val_loss: 0.0017\n",
      "Epoch 23/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5768e-04 - val_loss: 0.0017\n",
      "Epoch 24/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6595e-04 - val_loss: 0.0017\n",
      "Epoch 25/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6647e-04 - val_loss: 0.0017\n",
      "Epoch 26/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6283e-04 - val_loss: 0.0017\n",
      "Epoch 27/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6968e-04 - val_loss: 0.0018\n",
      "Epoch 28/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6267e-04 - val_loss: 0.0017\n",
      "Epoch 29/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6239e-04 - val_loss: 0.0018\n",
      "Epoch 30/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6986e-04 - val_loss: 0.0017\n",
      "Epoch 31/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6592e-04 - val_loss: 0.0018\n",
      "Epoch 32/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6122e-04 - val_loss: 0.0017\n",
      "Epoch 33/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6354e-04 - val_loss: 0.0018\n",
      "Epoch 34/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6815e-04 - val_loss: 0.0017\n",
      "Epoch 35/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6554e-04 - val_loss: 0.0018\n",
      "Epoch 36/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.7136e-04 - val_loss: 0.0018\n",
      "Epoch 37/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6325e-04 - val_loss: 0.0017\n",
      "Epoch 38/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6590e-04 - val_loss: 0.0018\n",
      "Epoch 39/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6152e-04 - val_loss: 0.0018\n",
      "Epoch 40/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6087e-04 - val_loss: 0.0017\n",
      "Epoch 41/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6885e-04 - val_loss: 0.0018\n",
      "Epoch 42/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.7105e-04 - val_loss: 0.0018\n",
      "Epoch 43/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6181e-04 - val_loss: 0.0017\n",
      "Epoch 44/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5767e-04 - val_loss: 0.0017\n",
      "Epoch 45/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6127e-04 - val_loss: 0.0018\n",
      "Epoch 46/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5852e-04 - val_loss: 0.0017\n",
      "Epoch 47/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6070e-04 - val_loss: 0.0018\n",
      "Epoch 48/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6580e-04 - val_loss: 0.0017\n",
      "Epoch 49/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5917e-04 - val_loss: 0.0018\n",
      "Epoch 50/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5966e-04 - val_loss: 0.0017\n",
      "Epoch 51/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6931e-04 - val_loss: 0.0018\n",
      "Epoch 52/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5897e-04 - val_loss: 0.0017\n",
      "Epoch 53/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6161e-04 - val_loss: 0.0018\n",
      "Epoch 54/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6361e-04 - val_loss: 0.0018\n",
      "Epoch 55/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6266e-04 - val_loss: 0.0017\n",
      "Epoch 56/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5919e-04 - val_loss: 0.0017\n",
      "Epoch 57/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5891e-04 - val_loss: 0.0017\n",
      "Epoch 58/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6231e-04 - val_loss: 0.0018\n",
      "Epoch 59/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6170e-04 - val_loss: 0.0017\n",
      "Epoch 60/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5887e-04 - val_loss: 0.0017\n",
      "Epoch 61/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5505e-04 - val_loss: 0.0017\n",
      "Epoch 62/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6112e-04 - val_loss: 0.0017\n",
      "Epoch 63/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6451e-04 - val_loss: 0.0017\n",
      "Epoch 64/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6470e-04 - val_loss: 0.0017\n",
      "Epoch 65/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6152e-04 - val_loss: 0.0018\n",
      "Epoch 66/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6091e-04 - val_loss: 0.0018\n",
      "Epoch 67/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.7315e-04 - val_loss: 0.0017\n",
      "Epoch 68/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5788e-04 - val_loss: 0.0017\n",
      "Epoch 69/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5945e-04 - val_loss: 0.0017\n",
      "Epoch 70/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6424e-04 - val_loss: 0.0018\n",
      "Epoch 71/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6161e-04 - val_loss: 0.0017\n",
      "Epoch 72/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6459e-04 - val_loss: 0.0017\n",
      "Epoch 73/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6556e-04 - val_loss: 0.0018\n",
      "Epoch 74/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6349e-04 - val_loss: 0.0018\n",
      "Epoch 75/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6283e-04 - val_loss: 0.0018\n",
      "Epoch 76/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6678e-04 - val_loss: 0.0017\n",
      "Epoch 77/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6100e-04 - val_loss: 0.0018\n",
      "Epoch 78/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5911e-04 - val_loss: 0.0017\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5575e-04 - val_loss: 0.0017\n",
      "Epoch 80/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5826e-04 - val_loss: 0.0017\n",
      "Epoch 81/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5396e-04 - val_loss: 0.0017\n",
      "Epoch 82/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5872e-04 - val_loss: 0.0018\n",
      "Epoch 83/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5602e-04 - val_loss: 0.0018\n",
      "Epoch 84/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5773e-04 - val_loss: 0.0017\n",
      "Epoch 85/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5754e-04 - val_loss: 0.0017\n",
      "Epoch 86/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6027e-04 - val_loss: 0.0018\n",
      "Epoch 87/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6003e-04 - val_loss: 0.0018\n",
      "Epoch 88/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6009e-04 - val_loss: 0.0018\n",
      "Epoch 89/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6368e-04 - val_loss: 0.0018\n",
      "Epoch 90/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6159e-04 - val_loss: 0.0017\n",
      "Epoch 91/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6147e-04 - val_loss: 0.0017\n",
      "Epoch 92/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6415e-04 - val_loss: 0.0018\n",
      "Epoch 93/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6725e-04 - val_loss: 0.0017\n",
      "Epoch 94/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6249e-04 - val_loss: 0.0017\n",
      "Epoch 95/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5836e-04 - val_loss: 0.0017\n",
      "Epoch 96/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5738e-04 - val_loss: 0.0017\n",
      "Epoch 97/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6261e-04 - val_loss: 0.0018\n",
      "Epoch 98/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6141e-04 - val_loss: 0.0018\n",
      "Epoch 99/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6807e-04 - val_loss: 0.0018\n",
      "Epoch 100/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6048e-04 - val_loss: 0.0018\n",
      "Epoch 101/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5666e-04 - val_loss: 0.0017\n",
      "Epoch 102/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5498e-04 - val_loss: 0.0018\n",
      "Epoch 103/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5816e-04 - val_loss: 0.0017\n",
      "Epoch 104/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5226e-04 - val_loss: 0.0018\n",
      "Epoch 105/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6329e-04 - val_loss: 0.0017\n",
      "Epoch 106/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6143e-04 - val_loss: 0.0018\n",
      "Epoch 107/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5646e-04 - val_loss: 0.0018\n",
      "Epoch 108/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5483e-04 - val_loss: 0.0018\n",
      "Epoch 109/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5807e-04 - val_loss: 0.0017\n",
      "Epoch 110/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6256e-04 - val_loss: 0.0017\n",
      "Epoch 111/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5721e-04 - val_loss: 0.0018\n",
      "Epoch 112/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5664e-04 - val_loss: 0.0017\n",
      "Epoch 113/300\n",
      "31/31 [==============================] - ETA: 0s - loss: 7.5451e-0 - 0s 3ms/step - loss: 7.5289e-04 - val_loss: 0.0018\n",
      "Epoch 114/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5598e-04 - val_loss: 0.0017\n",
      "Epoch 115/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5601e-04 - val_loss: 0.0018\n",
      "Epoch 116/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6166e-04 - val_loss: 0.0018\n",
      "Epoch 117/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6362e-04 - val_loss: 0.0018\n",
      "Epoch 118/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6034e-04 - val_loss: 0.0018\n",
      "Epoch 119/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6004e-04 - val_loss: 0.0017\n",
      "Epoch 120/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5996e-04 - val_loss: 0.0018\n",
      "Epoch 121/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5979e-04 - val_loss: 0.0018\n",
      "Epoch 122/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5475e-04 - val_loss: 0.0018\n",
      "Epoch 123/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6009e-04 - val_loss: 0.0018\n",
      "Epoch 124/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5549e-04 - val_loss: 0.0017\n",
      "Epoch 125/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5871e-04 - val_loss: 0.0017\n",
      "Epoch 126/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5889e-04 - val_loss: 0.0018\n",
      "Epoch 127/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6741e-04 - val_loss: 0.0017\n",
      "Epoch 128/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5926e-04 - val_loss: 0.0018\n",
      "Epoch 129/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6266e-04 - val_loss: 0.0018\n",
      "Epoch 130/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6845e-04 - val_loss: 0.0018\n",
      "Epoch 131/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5329e-04 - val_loss: 0.0017\n",
      "Epoch 132/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5565e-04 - val_loss: 0.0017\n",
      "Epoch 133/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5470e-04 - val_loss: 0.0017\n",
      "Epoch 134/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5362e-04 - val_loss: 0.0018\n",
      "Epoch 135/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6218e-04 - val_loss: 0.0017\n",
      "Epoch 136/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5713e-04 - val_loss: 0.0017\n",
      "Epoch 137/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5379e-04 - val_loss: 0.0018\n",
      "Epoch 138/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6124e-04 - val_loss: 0.0018\n",
      "Epoch 139/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5389e-04 - val_loss: 0.0017\n",
      "Epoch 140/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6020e-04 - val_loss: 0.0017\n",
      "Epoch 141/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5727e-04 - val_loss: 0.0017\n",
      "Epoch 142/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5456e-04 - val_loss: 0.0017\n",
      "Epoch 143/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5860e-04 - val_loss: 0.0018\n",
      "Epoch 144/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5435e-04 - val_loss: 0.0018\n",
      "Epoch 145/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5857e-04 - val_loss: 0.0018\n",
      "Epoch 146/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5440e-04 - val_loss: 0.0017\n",
      "Epoch 147/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5669e-04 - val_loss: 0.0017\n",
      "Epoch 148/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5333e-04 - val_loss: 0.0017\n",
      "Epoch 149/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5641e-04 - val_loss: 0.0018\n",
      "Epoch 150/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5733e-04 - val_loss: 0.0018\n",
      "Epoch 151/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5808e-04 - val_loss: 0.0018\n",
      "Epoch 152/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6039e-04 - val_loss: 0.0018\n",
      "Epoch 153/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5783e-04 - val_loss: 0.0017\n",
      "Epoch 154/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5716e-04 - val_loss: 0.0018\n",
      "Epoch 155/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5418e-04 - val_loss: 0.0017\n",
      "Epoch 156/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6142e-04 - val_loss: 0.0018\n",
      "Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6189e-04 - val_loss: 0.0017\n",
      "Epoch 158/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5465e-04 - val_loss: 0.0017\n",
      "Epoch 159/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5417e-04 - val_loss: 0.0017\n",
      "Epoch 160/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5687e-04 - val_loss: 0.0017\n",
      "Epoch 161/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5318e-04 - val_loss: 0.0017\n",
      "Epoch 162/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4899e-04 - val_loss: 0.0017\n",
      "Epoch 163/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5627e-04 - val_loss: 0.0017\n",
      "Epoch 164/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5434e-04 - val_loss: 0.0018\n",
      "Epoch 165/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5568e-04 - val_loss: 0.0017\n",
      "Epoch 166/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5527e-04 - val_loss: 0.0017\n",
      "Epoch 167/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5494e-04 - val_loss: 0.0018\n",
      "Epoch 168/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5804e-04 - val_loss: 0.0017\n",
      "Epoch 169/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5345e-04 - val_loss: 0.0017\n",
      "Epoch 170/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5085e-04 - val_loss: 0.0018\n",
      "Epoch 171/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5544e-04 - val_loss: 0.0018\n",
      "Epoch 172/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5270e-04 - val_loss: 0.0017\n",
      "Epoch 173/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5766e-04 - val_loss: 0.0018\n",
      "Epoch 174/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5820e-04 - val_loss: 0.0018\n",
      "Epoch 175/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6132e-04 - val_loss: 0.0017\n",
      "Epoch 176/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5750e-04 - val_loss: 0.0018\n",
      "Epoch 177/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5494e-04 - val_loss: 0.0017\n",
      "Epoch 178/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5040e-04 - val_loss: 0.0017\n",
      "Epoch 179/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5409e-04 - val_loss: 0.0018\n",
      "Epoch 180/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5651e-04 - val_loss: 0.0017\n",
      "Epoch 181/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5245e-04 - val_loss: 0.0017\n",
      "Epoch 182/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5937e-04 - val_loss: 0.0017\n",
      "Epoch 183/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5538e-04 - val_loss: 0.0018\n",
      "Epoch 184/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5472e-04 - val_loss: 0.0017\n",
      "Epoch 185/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5248e-04 - val_loss: 0.0017\n",
      "Epoch 186/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5246e-04 - val_loss: 0.0018\n",
      "Epoch 187/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5504e-04 - val_loss: 0.0018\n",
      "Epoch 188/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5572e-04 - val_loss: 0.0018\n",
      "Epoch 189/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5469e-04 - val_loss: 0.0018\n",
      "Epoch 190/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5318e-04 - val_loss: 0.0018\n",
      "Epoch 191/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5028e-04 - val_loss: 0.0017\n",
      "Epoch 192/300\n",
      "31/31 [==============================] - ETA: 0s - loss: 7.7446e-0 - 0s 2ms/step - loss: 7.5661e-04 - val_loss: 0.0018\n",
      "Epoch 193/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5347e-04 - val_loss: 0.0017\n",
      "Epoch 194/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5392e-04 - val_loss: 0.0018\n",
      "Epoch 195/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5056e-04 - val_loss: 0.0017\n",
      "Epoch 196/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4889e-04 - val_loss: 0.0017\n",
      "Epoch 197/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5330e-04 - val_loss: 0.0018\n",
      "Epoch 198/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4934e-04 - val_loss: 0.0017\n",
      "Epoch 199/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5139e-04 - val_loss: 0.0018\n",
      "Epoch 200/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6225e-04 - val_loss: 0.0018\n",
      "Epoch 201/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5437e-04 - val_loss: 0.0018\n",
      "Epoch 202/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4904e-04 - val_loss: 0.0018\n",
      "Epoch 203/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5137e-04 - val_loss: 0.0018\n",
      "Epoch 204/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4915e-04 - val_loss: 0.0017\n",
      "Epoch 205/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5130e-04 - val_loss: 0.0018\n",
      "Epoch 206/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5061e-04 - val_loss: 0.0018\n",
      "Epoch 207/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4953e-04 - val_loss: 0.0018\n",
      "Epoch 208/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5019e-04 - val_loss: 0.0018\n",
      "Epoch 209/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5089e-04 - val_loss: 0.0018\n",
      "Epoch 210/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5801e-04 - val_loss: 0.0017\n",
      "Epoch 211/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5442e-04 - val_loss: 0.0018\n",
      "Epoch 212/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5450e-04 - val_loss: 0.0018\n",
      "Epoch 213/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4791e-04 - val_loss: 0.0018\n",
      "Epoch 214/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5022e-04 - val_loss: 0.0018\n",
      "Epoch 215/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5235e-04 - val_loss: 0.0018\n",
      "Epoch 216/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5025e-04 - val_loss: 0.0017\n",
      "Epoch 217/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4892e-04 - val_loss: 0.0017\n",
      "Epoch 218/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4531e-04 - val_loss: 0.0017\n",
      "Epoch 219/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5190e-04 - val_loss: 0.0018\n",
      "Epoch 220/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.6365e-04 - val_loss: 0.0018\n",
      "Epoch 221/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5651e-04 - val_loss: 0.0018\n",
      "Epoch 222/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5845e-04 - val_loss: 0.0018\n",
      "Epoch 223/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5745e-04 - val_loss: 0.0017\n",
      "Epoch 224/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5319e-04 - val_loss: 0.0018\n",
      "Epoch 225/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5435e-04 - val_loss: 0.0018\n",
      "Epoch 226/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5032e-04 - val_loss: 0.0018\n",
      "Epoch 227/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4847e-04 - val_loss: 0.0018\n",
      "Epoch 228/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4791e-04 - val_loss: 0.0017\n",
      "Epoch 229/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4656e-04 - val_loss: 0.0018\n",
      "Epoch 230/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4894e-04 - val_loss: 0.0018\n",
      "Epoch 231/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5095e-04 - val_loss: 0.0018\n",
      "Epoch 232/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5549e-04 - val_loss: 0.0018\n",
      "Epoch 233/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5876e-04 - val_loss: 0.0018\n",
      "Epoch 234/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5188e-04 - val_loss: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5196e-04 - val_loss: 0.0018\n",
      "Epoch 236/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5434e-04 - val_loss: 0.0018\n",
      "Epoch 237/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5315e-04 - val_loss: 0.0018\n",
      "Epoch 238/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4854e-04 - val_loss: 0.0017\n",
      "Epoch 239/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5115e-04 - val_loss: 0.0017\n",
      "Epoch 240/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5905e-04 - val_loss: 0.0017\n",
      "Epoch 241/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4940e-04 - val_loss: 0.0018\n",
      "Epoch 242/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4726e-04 - val_loss: 0.0018\n",
      "Epoch 243/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5060e-04 - val_loss: 0.0017\n",
      "Epoch 244/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4599e-04 - val_loss: 0.0017\n",
      "Epoch 245/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4945e-04 - val_loss: 0.0017\n",
      "Epoch 246/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4805e-04 - val_loss: 0.0018\n",
      "Epoch 247/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4440e-04 - val_loss: 0.0018\n",
      "Epoch 248/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4844e-04 - val_loss: 0.0018\n",
      "Epoch 249/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5693e-04 - val_loss: 0.0018\n",
      "Epoch 250/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5490e-04 - val_loss: 0.0018\n",
      "Epoch 251/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4829e-04 - val_loss: 0.0017\n",
      "Epoch 252/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4724e-04 - val_loss: 0.0017\n",
      "Epoch 253/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4663e-04 - val_loss: 0.0018\n",
      "Epoch 254/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4416e-04 - val_loss: 0.0018\n",
      "Epoch 255/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5124e-04 - val_loss: 0.0018\n",
      "Epoch 256/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5653e-04 - val_loss: 0.0018\n",
      "Epoch 257/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5456e-04 - val_loss: 0.0017\n",
      "Epoch 258/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4736e-04 - val_loss: 0.0017\n",
      "Epoch 259/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4996e-04 - val_loss: 0.0017\n",
      "Epoch 260/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4807e-04 - val_loss: 0.0017\n",
      "Epoch 261/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4452e-04 - val_loss: 0.0018\n",
      "Epoch 262/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4156e-04 - val_loss: 0.0018\n",
      "Epoch 263/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5201e-04 - val_loss: 0.0017\n",
      "Epoch 264/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5454e-04 - val_loss: 0.0018\n",
      "Epoch 265/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5011e-04 - val_loss: 0.0017\n",
      "Epoch 266/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4775e-04 - val_loss: 0.0018\n",
      "Epoch 267/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5358e-04 - val_loss: 0.0018\n",
      "Epoch 268/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4890e-04 - val_loss: 0.0018\n",
      "Epoch 269/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4836e-04 - val_loss: 0.0017\n",
      "Epoch 270/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4360e-04 - val_loss: 0.0017\n",
      "Epoch 271/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4708e-04 - val_loss: 0.0017\n",
      "Epoch 272/300\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 7.5084e-04 - val_loss: 0.0018\n",
      "Epoch 273/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5010e-04 - val_loss: 0.0018\n",
      "Epoch 274/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4881e-04 - val_loss: 0.0017\n",
      "Epoch 275/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4487e-04 - val_loss: 0.0018\n",
      "Epoch 276/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4623e-04 - val_loss: 0.0018\n",
      "Epoch 277/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4384e-04 - val_loss: 0.0017\n",
      "Epoch 278/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4442e-04 - val_loss: 0.0018\n",
      "Epoch 279/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4689e-04 - val_loss: 0.0018\n",
      "Epoch 280/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4807e-04 - val_loss: 0.0018\n",
      "Epoch 281/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4259e-04 - val_loss: 0.0018\n",
      "Epoch 282/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4685e-04 - val_loss: 0.0018\n",
      "Epoch 283/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4774e-04 - val_loss: 0.0017\n",
      "Epoch 284/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4854e-04 - val_loss: 0.0018\n",
      "Epoch 285/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4469e-04 - val_loss: 0.0017\n",
      "Epoch 286/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4637e-04 - val_loss: 0.0018\n",
      "Epoch 287/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4958e-04 - val_loss: 0.0018\n",
      "Epoch 288/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.6419e-04 - val_loss: 0.0018\n",
      "Epoch 289/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.5562e-04 - val_loss: 0.0017\n",
      "Epoch 290/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4416e-04 - val_loss: 0.0017\n",
      "Epoch 291/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4697e-04 - val_loss: 0.0018\n",
      "Epoch 292/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.5090e-04 - val_loss: 0.0018\n",
      "Epoch 293/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4944e-04 - val_loss: 0.0017\n",
      "Epoch 294/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4507e-04 - val_loss: 0.0017\n",
      "Epoch 295/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4471e-04 - val_loss: 0.0018\n",
      "Epoch 296/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4900e-04 - val_loss: 0.0018\n",
      "Epoch 297/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4916e-04 - val_loss: 0.0018\n",
      "Epoch 298/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4555e-04 - val_loss: 0.0018\n",
      "Epoch 299/300\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 7.4485e-04 - val_loss: 0.0018\n",
      "Epoch 300/300\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 7.4302e-04 - val_loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "#Train Learning Model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00005), loss='mse') #0.0001\n",
    "\n",
    "#history = model.fit(x_train, y_train, epochs = 50000, validation_split=0.0, batch_size = x_train.shape[0])\n",
    "#history = model.fit(x_train, y_train, epochs = 3000, validation_split=0.0, batch_size = 1280) #1280\n",
    "#Batch size = 1280 for remove outlier, 2560 for keep outlier\n",
    "history = model.fit(x = x_train, y = y_train, epochs =300, batch_size = 1280, validation_data = (x_valid, y_valid),shuffle=True) #1280, 1000 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot Training Progree\n",
    "plt.plot(history.history['loss'], 'r', label='loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(history.history['val_loss'], 'b', label='val_loss') if 'val_loss' in history.history else None\n",
    "plt.legend()\n",
    "plt.axhline(y=0.0017, xmin=0, xmax=5, linewidth=2, color = 'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jiayu/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/jiayu/Desktop/MLP_DataSet/LargeSlope_TimeTrack_Angle_17_26//ML_Models/NN_Model_InitSet/assets\n"
     ]
    }
   ],
   "source": [
    "#Save Trained Model\n",
    "#MLmodel_name = \"NN_Model_Valid_\" + trainingset[\"PreProcessMode\"] + \"_Dagger_InitSet_2Iter\"\n",
    "#MLmodel_name = \"NN_Model\" + \"_\" + \"AugVarStep_1to2StepbeforeFail_3Time_RemovebyClip_SmallThre\"\n",
    "MLmodel_name = \"NN_Model\" + \"_\" + \"InitSet\"\n",
    "model.save(ML_Model_Path + MLmodel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DataSet Setttings\n",
    "datasetSettings = {\"Shift_World_Frame_Type\":trainingset[\"Shift_World_Frame_Type\"],\n",
    "                   \"VectorScaleFactor\":trainingset[\"VectorScaleFactor\"],\n",
    "                   \"NumPreviewSteps\":trainingset[\"NumPreviewSteps\"],\n",
    "                   \"Contact_Representation_Type\":trainingset[\"Contact_Representation_Type\"],\n",
    "                   \"TrainingLoss\":history.history['loss']}\n",
    "#Validation loss\n",
    "datasetSettings[\"ValidationLoss\"] = history.history['val_loss'] if 'val_loss' in history.history else None\n",
    "\n",
    "#ProProcess\n",
    "datasetSettings[\"PreProcessMode\"] = trainingset[\"PreProcessMode\"]\n",
    "datasetSettings[\"Scaler_X\"] = trainingset[\"Scaler_X\"]\n",
    "datasetSettings[\"Scaler_Y\"] = trainingset[\"Scaler_Y\"]\n",
    "\n",
    "#Dump File\n",
    "pickle.dump(datasetSettings, open(ML_Model_Path + MLmodel_name+ '/datasetSettings' +'.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.16898236e-01  1.36893514e-02  7.78614581e-01  1.60702856e-01\n",
      " -1.00131502e-01 -2.82221670e-02 -4.23586116e-08 -4.66310963e-08\n",
      "  3.62160760e-09 -3.04074891e-01  1.84197779e-01  8.60246897e-02\n",
      "  1.00000000e+00  1.10000022e-01  1.12409997e+00  1.44698797e-01\n",
      " -4.64999978e-01  1.11437414e+00  2.23371473e-01 -4.64999978e-01\n",
      "  1.21929106e-01  1.00681383e-01  1.10000022e-01  1.31654942e-01\n",
      "  2.20087066e-02  1.10000022e-01  1.34375748e-01  1.02889919e-13\n",
      " -4.64999978e-01  1.34375748e-01  1.02889919e-13 -4.64999978e-01\n",
      " -8.73236732e-01  1.02876041e-13  1.10000022e-01 -8.73236732e-01\n",
      "  1.02876041e-13  6.85000022e-01  1.11519838e+00  2.16704166e-01\n",
      "  1.10000022e-01  1.12327573e+00  1.51366104e-01  1.10000022e-01\n",
      "  1.30830702e-01  2.86760141e-02  6.85000022e-01  1.22753345e-01\n",
      "  9.40140759e-02  6.85000022e-01  1.22180474e-01  9.86480555e-02\n",
      "  1.10000022e-01  1.31403573e-01  2.40420345e-02  1.10000022e-01\n",
      " -8.61041459e-01 -9.86480555e-02  6.85000022e-01 -8.70264558e-01\n",
      " -2.40420345e-02  1.26000002e+00  1.10686565e+00  2.84107910e-01\n",
      "  6.85000022e-01  1.10686565e+00  2.84107910e-01  6.85000022e-01\n",
      "  1.39163427e-01 -3.87277304e-02  1.26000002e+00  1.39163427e-01\n",
      " -3.87277304e-02  1.26000002e+00  1.54408070e-01 -1.62042256e-01\n",
      "  6.85000022e-01  1.54408070e-01 -1.62042256e-01  6.85000022e-01\n",
      " -8.93269055e-01  1.62042256e-01  1.26000002e+00 -8.93269055e-01\n",
      "  1.62042256e-01]\n",
      "Data Kept Original Form, But need to scale back to meters\n",
      "predicted result: \n",
      " [[ 1.2885885e-01  4.0373083e-02  7.6551437e-01  1.6321053e-01\n",
      "   9.6512333e-02  1.3789146e-02 -1.9841897e-03  1.3051196e-02\n",
      "   2.5184872e-04  3.0741218e-01  6.6075809e-02  4.8606935e-01\n",
      "   7.0598108e-01  4.9124765e-01]]\n",
      "true value: \n",
      " [ 1.12492340e-01  3.59975140e-02  7.32270816e-01  1.37151240e-01\n",
      "  1.02645252e-01 -1.49915180e-02  2.79827864e-04 -8.92999454e-05\n",
      "  4.78495838e-05  2.66982340e-01  6.00000357e-02  4.99998848e-01\n",
      "  7.00000290e-01  4.99999393e-01]\n",
      "diff: \n",
      " [[0.01636651 0.00437557 0.03324356 0.02605929 0.00613292 0.02878066\n",
      "  0.00226402 0.0131405  0.000204   0.04042984 0.00607577 0.0139295\n",
      "  0.00598079 0.00875174]]\n"
     ]
    }
   ],
   "source": [
    "#Show Prediction Result for Training\n",
    "from sklearn import preprocessing\n",
    "\n",
    "datapoint_num = 0\n",
    "y_pred_temp = model.predict(np.array([x_train[datapoint_num]]))\n",
    "\n",
    "print(x_train[datapoint_num])\n",
    "\n",
    "#Recover to original format\n",
    "if trainingset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_originalform = y_pred_temp/trainingset[\"VectorScaleFactor\"]\n",
    "    y_true_originalform = y_train[datapoint_num]/trainingset[\"VectorScaleFactor\"]\n",
    "elif trainingset[\"PreProcessMode\"] == \"Standarization\" or trainingset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    y_pred_originalform = dataset[\"Scaler_Y\"].inverse_transform(y_pred_temp)\n",
    "    y_true_originalform = dataset[\"Scaler_Y\"].inverse_transform(np.array([y_train[datapoint_num]]))\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "\n",
    "print(\"predicted result: \\n\",y_pred_originalform)\n",
    "print(\"true value: \\n\",y_true_originalform)\n",
    "print(\"diff: \\n\", np.absolute(y_pred_originalform - y_true_originalform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.18487482 0.18489251 0.18491974 0.18496106 0.18512547 0.18522893\n",
      " 0.18523779 0.18533384 0.18541724 0.1854538  0.18573358 0.18574588\n",
      " 0.18575258 0.18579154 0.1858462  0.1858484  0.18605768 0.18622198\n",
      " 0.18629363 0.18635248 0.18643978 0.1865446  0.18664918 0.18669163\n",
      " 0.18675124 0.18681238 0.18691617 0.18702336 0.18703063 0.18704239\n",
      " 0.187151   0.18721773 0.18722384 0.18732003 0.1873235  0.18753011\n",
      " 0.18755671 0.18762068 0.18763551 0.18776871 0.1877757  0.18779189\n",
      " 0.18782501 0.18787116 0.18789719 0.18804445 0.188197   0.1882293\n",
      " 0.18825698 0.18834272 0.18834583 0.18857446 0.18866834 0.18877749\n",
      " 0.18879915 0.18890442 0.18894511 0.18910228 0.18911051 0.1891377\n",
      " 0.18915452 0.18919312 0.18943091 0.18948597 0.18956333 0.1897649\n",
      " 0.18976634 0.19002276 0.19003837 0.19025815 0.19050776 0.19071339\n",
      " 0.19073906 0.19104462 0.1911704  0.19120238 0.19140164 0.19150787\n",
      " 0.19171706 0.19199838 0.19201492 0.19202963 0.19221083 0.19229164\n",
      " 0.19230058 0.19243912 0.19253988 0.1925968  0.19263735 0.19273391\n",
      " 0.19274117 0.19299413 0.19311812 0.19319462 0.19330011 0.19331064\n",
      " 0.19337766 0.19338041 0.19345753 0.19358135 0.19365753 0.19387293\n",
      " 0.19392443 0.19399226 0.19418312 0.19444596 0.19444621 0.19451207\n",
      " 0.19455375 0.19466588 0.19494852 0.19504464 0.19505458 0.19505593\n",
      " 0.19507455 0.19509308 0.1951128  0.19520805 0.19521077 0.19533277\n",
      " 0.19541566 0.19546801 0.1955929  0.19567544 0.19587905 0.1961508\n",
      " 0.19617161 0.196194   0.19622234 0.19644046 0.1965109  0.19655917\n",
      " 0.19659305 0.19662715 0.19665174 0.19669898 0.19670191 0.19692754\n",
      " 0.19695203 0.19708923 0.19709724 0.19738866 0.19743619 0.19744557\n",
      " 0.19746089 0.1975034  0.19764708 0.19776254 0.1979187  0.19794107\n",
      " 0.19795918 0.1979593  0.19795956 0.19809668 0.19812848 0.19827503\n",
      " 0.19845629 0.19874754 0.19877982 0.19882265 0.19890672 0.19899073\n",
      " 0.19900414 0.19915423 0.19957493 0.19968709 0.19978575 0.19991816\n",
      " 0.20000906 0.20006497 0.20055357 0.20056093 0.20062335 0.20066837\n",
      " 0.20080302 0.20094244 0.2010295  0.20108223 0.20113742 0.20115924\n",
      " 0.20119115 0.20125868 0.20131098 0.20150724 0.20153723 0.2015826\n",
      " 0.20161194 0.20165956 0.20170102 0.20246006 0.20246348 0.20255558\n",
      " 0.20256441 0.20261265 0.20261285 0.20264378 0.20282245 0.2028549\n",
      " 0.20305292 0.2030741  0.20318415 0.20325211 0.20331428 0.2033797\n",
      " 0.20340384 0.20369178 0.20372337 0.20373973 0.20375371 0.20376302\n",
      " 0.20378588 0.20388414 0.20399771 0.2042073  0.20421199 0.2043884\n",
      " 0.20440506 0.20443948 0.20447309 0.20454934 0.20459329 0.20483588\n",
      " 0.20533856 0.20548724 0.20561191 0.20608749 0.20611518 0.20629767\n",
      " 0.20633509 0.20633957 0.2064006  0.20652827 0.20653301 0.20653729\n",
      " 0.20661839 0.20671524 0.20691778 0.20696461 0.20710148 0.20736666\n",
      " 0.20745328 0.20746004 0.20746964 0.20754814 0.20759752 0.2078029\n",
      " 0.20798564 0.2080752  0.20814662 0.20819646 0.20829412 0.20843907\n",
      " 0.20846206 0.2084646  0.20874288 0.20881861 0.20916552 0.2093173\n",
      " 0.20961942 0.20963225 0.21013305 0.21020262 0.21053787 0.21061593\n",
      " 0.21062537 0.21067697 0.2107602  0.21088054 0.21096075 0.2111376\n",
      " 0.21115318 0.21117068 0.2112211  0.21123275 0.21135405 0.21140233\n",
      " 0.21140687 0.21147793 0.21163629 0.21166908 0.21176135 0.21200988\n",
      " 0.21204617 0.21212644 0.21227813 0.21230033 0.21232109 0.21235963\n",
      " 0.21243601 0.21244032 0.21265015 0.21266298 0.21283793 0.21299766\n",
      " 0.21311206 0.21313308 0.21334286 0.21358037 0.21362126 0.21400748\n",
      " 0.21404211 0.21408945 0.21410744 0.21417775 0.2141853  0.21432908\n",
      " 0.21440486 0.21484724 0.21492731 0.21500268 0.21516273 0.21527441\n",
      " 0.21528367 0.21537924 0.21552603 0.21558466 0.2160218  0.2161161\n",
      " 0.21614303 0.21625745 0.21644665 0.21646259 0.21661091 0.21663635\n",
      " 0.21666577 0.21673124 0.21690729 0.21697412 0.21699008 0.2175901\n",
      " 0.21777877 0.21786542 0.21800427 0.21818325 0.2189386  0.21924162\n",
      " 0.219419   0.21951628 0.21964189 0.21980977 0.21986085 0.21994259\n",
      " 0.22008474 0.2201222  0.22015705 0.22026075 0.22033148 0.22049185\n",
      " 0.22058949 0.22065197 0.22071281 0.22106682 0.22117896 0.22128833\n",
      " 0.22134506 0.22140163 0.22142222 0.22144023 0.22162607 0.22168307\n",
      " 0.22172072 0.22174384 0.22183405 0.22186407 0.22188196 0.2220667\n",
      " 0.22223807 0.22229089 0.22249196 0.22254306 0.22284606 0.22297817\n",
      " 0.22344449 0.22363034 0.22383222 0.22417496 0.22430473 0.22445522\n",
      " 0.22450326 0.22503305 0.22535494 0.22537044 0.22540853 0.225795\n",
      " 0.22584746 0.22587492 0.22589333 0.22593312 0.22605223 0.22629763\n",
      " 0.22630062 0.2263677  0.22649428 0.22676993 0.22688265 0.22703825\n",
      " 0.22743596 0.22754982 0.22770683 0.22781534 0.22796655 0.22802698\n",
      " 0.2280343  0.22874946 0.22894331 0.22907182 0.22915281 0.22917863\n",
      " 0.2294256  0.22945707 0.22945943 0.22952041 0.22956514 0.22968094\n",
      " 0.22971132 0.22973043 0.22976197 0.2298415  0.22989199 0.22991797\n",
      " 0.23000368 0.23018515 0.23066742 0.23088186 0.23088947 0.23104076\n",
      " 0.23107277 0.23146646 0.23157137 0.23167034 0.23175769 0.23188212\n",
      " 0.23201745 0.23203152 0.23221492 0.2324961  0.23256316 0.23314976\n",
      " 0.23334088 0.23363576 0.23403536 0.23411126 0.23412138 0.23412501\n",
      " 0.23427447 0.23471183 0.23512156 0.23543895 0.23566174 0.23616259\n",
      " 0.2364837  0.2364931  0.23654324 0.2366367  0.23664866 0.23669132\n",
      " 0.23696286 0.236996   0.23715431 0.23747206 0.23758879 0.23765005\n",
      " 0.23783489 0.23812552 0.23828867 0.23848495 0.23867247 0.23874644\n",
      " 0.23881707 0.23885287 0.2392228  0.23928893 0.23939688 0.2395307\n",
      " 0.23963152 0.23971153 0.2399757  0.24001687 0.24006439 0.24006949\n",
      " 0.24023784 0.24054504 0.24095538 0.24127181 0.24142448 0.24190634\n",
      " 0.24238442 0.2426747  0.24311098 0.2431392  0.24325149 0.24331179\n",
      " 0.24340593 0.24357139 0.24374462 0.24375425 0.24417383 0.24434677\n",
      " 0.24451227 0.2446194  0.24476559 0.24485962 0.24488738 0.24496983\n",
      " 0.24498407 0.24523816 0.24556667 0.24590672 0.24600191 0.24600527\n",
      " 0.2463771  0.24644534 0.24661953 0.24714488 0.24718836 0.24734561\n",
      " 0.24782238 0.24782974 0.24796791 0.24823656 0.24833913 0.2483839\n",
      " 0.24859076 0.24863889 0.24885803 0.24909795 0.2491664  0.24934901\n",
      " 0.24950055 0.24971442 0.25015505 0.25016926 0.25022084 0.25025192\n",
      " 0.25038055 0.25060364 0.25165404 0.25174174 0.25177937 0.25181483\n",
      " 0.25197261 0.25203127 0.2521378  0.25241387 0.25241436 0.25248058\n",
      " 0.25284116 0.25340366 0.25365938 0.25432145 0.25442177 0.25484459\n",
      " 0.2553515  0.25537523 0.25562214 0.25630805 0.25647449 0.25688028\n",
      " 0.25694616 0.25718408 0.25764122 0.2576612  0.25767781 0.25773728\n",
      " 0.2577653  0.25785782 0.25810972 0.25835652 0.25875559 0.25912363\n",
      " 0.25924741 0.25966008 0.2598413  0.25985048 0.25994736 0.25996517\n",
      " 0.26003708 0.26031549 0.26100093 0.26129464 0.26133615 0.26133964\n",
      " 0.26158899 0.26217011 0.26292157 0.26303306 0.26309069 0.26309578\n",
      " 0.26346329 0.26444701 0.26452493 0.26465558 0.26506947 0.26515531\n",
      " 0.26523853 0.26591456 0.26602575 0.26642333 0.26671125 0.2667226\n",
      " 0.26675581 0.26688782 0.2669137  0.26692221 0.26698151 0.26702786\n",
      " 0.26720868 0.26746157 0.26766063 0.26770281 0.26780951 0.2679495\n",
      " 0.26797902 0.26819998 0.26859325 0.26871307 0.26882619 0.27015019\n",
      " 0.27022924 0.27041042 0.27042725 0.27062238 0.27068226 0.27092912\n",
      " 0.27100564 0.27115283 0.27143744 0.27146033 0.27155341 0.27165321\n",
      " 0.27166903 0.27172284 0.27193254 0.27215495 0.27218778 0.27232217\n",
      " 0.27234192 0.27276355 0.27349966 0.27399963 0.27450922 0.27451133\n",
      " 0.27467118 0.27493025 0.27523659 0.27578381 0.27586365 0.27657875\n",
      " 0.27720828 0.2773429  0.27747908 0.27765463 0.27830803 0.27831001\n",
      " 0.27831118 0.27852199 0.27872667 0.2797187  0.27979612 0.27982999\n",
      " 0.27998466 0.28043983 0.28051563 0.28069656 0.28130884 0.28195563\n",
      " 0.28281617 0.28288152 0.28288555 0.28334738 0.28341821 0.2834292\n",
      " 0.28367178 0.28466227 0.28600623 0.28602946 0.28615147 0.28621892\n",
      " 0.28623291 0.28636532 0.28641519 0.28663291 0.28756957 0.28769994\n",
      " 0.28778611 0.28822814 0.28836104 0.28859807 0.28898164 0.2894002\n",
      " 0.28973283 0.28973977 0.28977799 0.29013947 0.29054321 0.29096972\n",
      " 0.29097867 0.29171617 0.29229512 0.29233285 0.29242714 0.29244956\n",
      " 0.2927121  0.29271329 0.29284967 0.29318973 0.29333168 0.29344829\n",
      " 0.2939769  0.29431365 0.29435978 0.29444754 0.2947117  0.29483312\n",
      " 0.29491196 0.29560181 0.29560803 0.29580934 0.296069   0.29620279\n",
      " 0.29636336 0.29640667 0.29667736 0.29677898 0.29703061 0.29704147\n",
      " 0.29730204 0.29759143 0.29814732 0.29825633 0.29827674 0.2984714\n",
      " 0.29862283 0.2986284  0.29866534 0.29965813 0.30009294 0.30028169\n",
      " 0.30033161 0.30044487 0.3012394  0.30235378 0.30320931 0.30323973\n",
      " 0.30352857 0.30372664 0.30411651 0.30418026 0.3043471  0.30456921\n",
      " 0.30496714 0.30520615 0.30540126 0.305582   0.30606837 0.30629353\n",
      " 0.30660345 0.30662917 0.30669649 0.30726729 0.30754353 0.30771996\n",
      " 0.30787774 0.30855506 0.30964021 0.30999053 0.31106362 0.31118256\n",
      " 0.31151757 0.3121236  0.31323899 0.31448938 0.31463756 0.31481901\n",
      " 0.31581081 0.31599812 0.31661983 0.3168125  0.31706371 0.31773239\n",
      " 0.31874839 0.31874884 0.3193214  0.32027548 0.32035788 0.32052032\n",
      " 0.32173093 0.32252162 0.32341123 0.32363177 0.32372659 0.32440799\n",
      " 0.32441067 0.32443601 0.32578037 0.32604305 0.32827521 0.32836332\n",
      " 0.32858634 0.32881866 0.32944765 0.32961792 0.32968494 0.33067401\n",
      " 0.33079368 0.33148381 0.33149395 0.33208104 0.33246251 0.33299849\n",
      " 0.33376428 0.33443597 0.33588154 0.33613606 0.33638068 0.33641613\n",
      " 0.33696935 0.33802693 0.33855759 0.33911648 0.33917531 0.34042179\n",
      " 0.34135097 0.34145002 0.34148471 0.34192851 0.34203608 0.34312983\n",
      " 0.34347288 0.34360057 0.3440211  0.34403295 0.34412231 0.34489803\n",
      " 0.34570611 0.3462797  0.34629389 0.34680081 0.34704972 0.34730328\n",
      " 0.34745861 0.34783402 0.34953933 0.34967371 0.35086012 0.35166691\n",
      " 0.35227602 0.35417458 0.35532827 0.35640476 0.35685249 0.3575345\n",
      " 0.35781195 0.35849447 0.35897491 0.36127117 0.36258294 0.3632521\n",
      " 0.36397947 0.36405258 0.3642428  0.36500019 0.36513264 0.36536892\n",
      " 0.36583929 0.36669851 0.36800568 0.36860199 0.36884113 0.36888231\n",
      " 0.36926879 0.36933251 0.36988678 0.37001428 0.37075795 0.37233479\n",
      " 0.37275772 0.37289611 0.37300458 0.37332139 0.37591355 0.37617543\n",
      " 0.37627916 0.37811593 0.37859015 0.37991156 0.38013314 0.38081227\n",
      " 0.38213544 0.3837924  0.38411555 0.38634277 0.38650487 0.38675626\n",
      " 0.38686479 0.38716553 0.38891499 0.38927321 0.3896957  0.39006642\n",
      " 0.39036797 0.3927211  0.39298406 0.39349064 0.39363973 0.39425662\n",
      " 0.394464   0.39469648 0.3958609  0.39631159 0.39676088 0.39707255\n",
      " 0.39817496 0.39832654 0.39942753 0.39980505 0.39986323 0.39987133\n",
      " 0.40027738 0.40273386 0.40338272 0.40749638 0.40917646 0.40951861\n",
      " 0.40957411 0.40971128 0.41085016 0.41205091 0.41252165 0.41440796\n",
      " 0.41548554 0.41637372 0.41649051 0.41713658 0.41722751 0.41888623\n",
      " 0.41924009 0.41970661 0.41994677 0.42003551 0.4208036  0.42130845\n",
      " 0.42139758 0.42177591 0.42246862 0.42303182 0.42393862 0.42410931\n",
      " 0.42574379 0.42723978 0.42727337 0.42784666 0.42846859 0.42880622\n",
      " 0.42921191 0.43024276 0.43073788 0.43136881 0.43212479 0.43223112\n",
      " 0.43379599 0.43400881 0.43409124 0.43458642 0.43739804 0.43951796\n",
      " 0.44061264 0.44138564 0.44190716 0.44260488 0.44304158 0.4432385\n",
      " 0.44334041 0.44453213 0.44651355 0.44714895 0.44746757 0.45183977\n",
      " 0.45187946 0.45195304 0.45196697 0.45394933 0.45559375 0.45587211\n",
      " 0.45748021 0.45853389 0.4610775  0.46347263 0.46577316 0.46954149\n",
      " 0.47480639 0.48056358 0.48111977 0.48209396 0.48365875 0.49195247\n",
      " 0.50535597 0.5140243  0.5155767  0.57677427]\n",
      "Error Mean:  0.03991305796183689\n",
      "Error Std 0.050698431181542936\n",
      "[0.28769994 0.28859807 0.28898164 0.2894002  0.28973283 0.29229512\n",
      " 0.29233285 0.2927121  0.29271329 0.29431365 0.2947117  0.29560181\n",
      " 0.296069   0.29620279 0.29677898 0.29703061 0.29704147 0.29730204\n",
      " 0.2984714  0.30044487 0.30352857 0.30372664 0.3043471  0.30456921\n",
      " 0.30726729 0.30754353 0.30855506 0.30999053 0.31106362 0.3121236\n",
      " 0.31448938 0.3193214  0.32173093 0.32252162 0.32440799 0.32578037\n",
      " 0.32604305 0.32827521 0.32836332 0.32961792 0.33079368 0.33148381\n",
      " 0.33588154 0.34192851 0.34347288 0.34360057 0.3440211  0.34403295\n",
      " 0.34745861 0.34967371 0.35227602 0.35532827 0.35685249 0.35849447\n",
      " 0.36127117 0.36258294 0.36405258 0.36500019 0.36583929 0.36800568\n",
      " 0.36860199 0.36888231 0.36988678 0.37001428 0.37859015 0.3837924\n",
      " 0.3896957  0.39006642 0.39298406 0.39363973 0.39425662 0.39817496\n",
      " 0.40971128 0.41637372 0.41649051 0.41713658 0.41924009 0.42139758\n",
      " 0.42393862 0.42723978 0.42784666 0.42846859 0.43073788 0.43136881\n",
      " 0.43458642 0.43739804 0.43951796 0.44138564 0.44190716 0.44260488\n",
      " 0.44304158 0.44651355 0.44746757 0.45196697 0.45394933 0.45748021\n",
      " 0.45853389 0.47480639 0.49195247 0.5155767 ]\n",
      "[ 7 16  4  8 20 10  3  3  9 20  6 27  8  8  7 20 16  4 21  9  2 22 15 29\n",
      " 28 14 23 26  8  2 25 21  6 26 22 29 22 24 21 20  3  1  8 20  2  9 14 20\n",
      "  4  6 15 19  2 20  1  0 22  1 25 14  9  6  0  3 19  5  8 15 21  7 14 21\n",
      " 13 13  8  2  1 21  4 26 20 23  3 25 26  2  1 14 12 25  0 19  0 13 26  7\n",
      " 19 26  1  1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([4., 7., 6., 5., 4., 1., 4., 4., 7., 4., 1., 0., 1., 3., 5., 3., 2.,\n",
       "        0., 0., 4., 8., 6., 4., 2., 1., 4., 6., 1., 1., 2.]),\n",
       " array([ 0.        ,  0.96666667,  1.93333333,  2.9       ,  3.86666667,\n",
       "         4.83333333,  5.8       ,  6.76666667,  7.73333333,  8.7       ,\n",
       "         9.66666667, 10.63333333, 11.6       , 12.56666667, 13.53333333,\n",
       "        14.5       , 15.46666667, 16.43333333, 17.4       , 18.36666667,\n",
       "        19.33333333, 20.3       , 21.26666667, 22.23333333, 23.2       ,\n",
       "        24.16666667, 25.13333333, 26.1       , 27.06666667, 28.03333333,\n",
       "        29.        ]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Training Set\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred_train = model.predict(x_train)\n",
    "\n",
    "if trainingset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_train_originalform = y_pred_train/trainingset[\"VectorScaleFactor\"]\n",
    "    y_true_train_originalform = y_train/trainingset[\"VectorScaleFactor\"]\n",
    "elif trainingset[\"PreProcessMode\"] == \"Standarization\" or trainingset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", trainingset[\"PreProcessMode\"])\n",
    "    y_pred_train_originalform = trainingset[\"Scaler_Y\"].inverse_transform(y_pred_train)\n",
    "    y_true_train_originalform = trainingset[\"Scaler_Y\"].inverse_transform(y_train)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "#err = np.linalg.norm(y_true_train_originalform[:,-3:]-y_pred_train_originalform[:,-3:], axis=1)\n",
    "err = np.linalg.norm(y_true_train_originalform[:,-3:]-y_pred_train_originalform[:,-3:], axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted[-1000:])  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())\n",
    "\n",
    "##Plot prediction on the initial dataset\n",
    "err_initdata=err[0:12000+1]\n",
    "\n",
    "err_initdata_sorted = np.sort(err_initdata)\n",
    "print(err_initdata_sorted[-100:])  # print the 100 biggest error\n",
    "\n",
    "err_initdata_idx_sorted = np.argsort(err_initdata)\n",
    "print(err_initdata_idx_sorted[-100:]%30)\n",
    "selected_err=err_initdata_idx_sorted[-100:]%30\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(selected_err, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.01057922 0.01453451 0.01498797 ... 1.21578115 1.21641191 1.34611813]\n",
      "Error Mean:  0.10931306078711447\n",
      "Error Std 0.11126840454081423\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Validation Set\n",
    "\n",
    "y_pred_valid = model.predict(x_valid)\n",
    "\n",
    "\n",
    "if validationset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_valid_originalform = y_pred_valid/validationset[\"VectorScaleFactor\"]\n",
    "    y_true_valid_originalform = y_valid/validationset[\"VectorScaleFactor\"]\n",
    "elif validationset[\"PreProcessMode\"] == \"Standarization\" or validationset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", validationset[\"PreProcessMode\"])\n",
    "    y_pred_valid_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_pred_valid)\n",
    "    y_true_valid_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_valid)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_true_valid_originalform-y_pred_valid_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted)  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.01057922 0.01453451 0.01498797 ... 1.21578115 1.21641191 1.34611813]\n",
      "Error Mean:  0.10931306078711447\n",
      "Error Std 0.11126840454081423\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Test Set\n",
    "\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "if testset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_test_originalform = y_pred_test/testset[\"VectorScaleFactor\"]\n",
    "    y_true_test_originalform = y_test/testset[\"VectorScaleFactor\"]\n",
    "elif testset[\"PreProcessMode\"] == \"Standarization\" or testset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", validationset[\"PreProcessMode\"])\n",
    "    y_pred_test_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_pred_test)\n",
    "    y_true_test_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_test)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_pred_test_originalform-y_true_test_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted)  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a501000252d127e7c27d83f75df0a57bca228f59033b739034a7cde4260d0152"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
