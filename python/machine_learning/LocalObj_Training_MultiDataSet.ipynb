{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double check the Path for storing trajectories is correct\n"
     ]
    }
   ],
   "source": [
    "#Import Packages\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from multicontact_learning_local_objectives.python.machine_learning.ml_utils import *\n",
    "import matplotlib.pyplot as plt #Matplotlib\n",
    "import shutil\n",
    "\n",
    "print(\"Double check the Path for storing trajectories is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Check we provide the Correct Traj Path: \n",
      " /media/jiayu/Seagate/LargeSlope_Angle_23_X_negative/\n"
     ]
    }
   ],
   "source": [
    "#Define Path for Storing Trajectories\n",
    "#Collect Data Points Path\n",
    "#workingDirectory = \"/home/jiayu/Desktop/multicontact_learning_local_objectives/data/large_slope_flat_patches/\"\n",
    "#workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_DaggerExact/\"\n",
    "#workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_Add2Steps\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_Add2Step_KeepOutlier\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_AddVarSteps_1to2StepbeforeFail_RemovebyClip/\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_Add2Steps_1StepbeforeFail_RemovebyClip/\"\n",
    "workingDirectory = \"/media/jiayu/Seagate/LargeSlope_Angle_23_X_negative/\"\n",
    "\n",
    "#NOTE: need to have \"/\" at the end\n",
    "print(\"Double Check we provide the Correct Traj Path: \\n\", workingDirectory)\n",
    "\n",
    "#Define dataset folder\n",
    "TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"Training_Init\"]\n",
    "\n",
    "# TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingSet_Initial\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"TrainingAug2Steps_1StepbeforeFail_1Time_RemovebyClip\",]\n",
    "\n",
    "# TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingSet\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_1Time\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_2Time\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_3Time\"]\n",
    "\n",
    "ValidationSetPath = workingDirectory + \"/DataSet/\"+\"ValidationSet\"\n",
    "TestSetPath = workingDirectory + \"/DataSet/\"+\"TestSet\"\n",
    "\n",
    "#Path to store ML Model, create one if we dont have\n",
    "ML_Model_Path = workingDirectory + \"/ML_Models/\"\n",
    "if not (os.path.isdir(ML_Model_Path)):\n",
    "    os.mkdir(ML_Model_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Code\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset:  0\n",
      "DataSet Sizes: \n",
      "(21370, 85)\n",
      "(21370, 11)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "Final Data Set Size\n",
      "(21370, 85)\n",
      "(21370, 11)\n",
      " \n",
      "Set Up for Validation Set\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      "Validation Set Size\n",
      "(5000, 85)\n",
      "(5000, 11)\n",
      " \n",
      " \n",
      "Set Up for Test Set\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      "Test Set Size\n",
      "(5000, 85)\n",
      "(5000, 11)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Load DataSet File\n",
    "\n",
    "#For training set\n",
    "for trainingset_idx in range(len(TrainingSetPath)):\n",
    "    trainingset_file = TrainingSetPath[trainingset_idx] + \"/data\"+'.p'\n",
    "    trainingset = pickle.load(open(trainingset_file,\"rb\"))\n",
    "    \n",
    "    print(\"For dataset: \", trainingset_idx)\n",
    "    print(\"DataSet Sizes: \")\n",
    "    \n",
    "    if trainingset_idx == 0:\n",
    "        x_train = trainingset[\"input\"]\n",
    "        y_train = trainingset[\"output\"]\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train,trainingset[\"input\"]),axis=0)\n",
    "        y_train = np.concatenate((y_train,trainingset[\"output\"]),axis=0)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    print(\"World Frame Shift: \", trainingset[\"Shift_World_Frame_Type\"])\n",
    "    print(\"Contact Location Representation Type: \",trainingset[\"Contact_Representation_Type\"])\n",
    "    print(\"Scaling Factor of Variables: \",trainingset[\"VectorScaleFactor\"])\n",
    "    print(\"Number of Preview Steps: \", trainingset[\"NumPreviewSteps\"])\n",
    "    print(\"Pre Process Mode: \",trainingset[\"PreProcessMode\"])\n",
    "    print(\" \")\n",
    "\n",
    "print(\"Final Data Set Size\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\" \")\n",
    "\n",
    "#For validation and Test\n",
    "\n",
    "#Load Validation Set and Test Set\n",
    "validationset_file = ValidationSetPath + \"/data\"+'.p'\n",
    "validationset = pickle.load(open(validationset_file,\"rb\"))\n",
    "\n",
    "testset_file = TestSetPath + \"/data\"+'.p'\n",
    "testset = pickle.load(open(testset_file,\"rb\"))\n",
    "\n",
    "x_valid = validationset[\"input\"]\n",
    "y_valid = validationset[\"output\"]\n",
    "\n",
    "x_test = testset[\"input\"]\n",
    "y_test = testset[\"output\"]\n",
    "\n",
    "print(\"Set Up for Validation Set\")\n",
    "print(\"World Frame Shift: \", validationset[\"Shift_World_Frame_Type\"])\n",
    "print(\"Contact Location Representation Type: \",validationset[\"Contact_Representation_Type\"])\n",
    "print(\"Scaling Factor of Variables: \",validationset[\"VectorScaleFactor\"])\n",
    "print(\"Number of Preview Steps: \", validationset[\"NumPreviewSteps\"])\n",
    "print(\"Pre Process Mode: \",validationset[\"PreProcessMode\"])\n",
    "print(\"Validation Set Size\")\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(\" \")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Set Up for Test Set\")\n",
    "print(\"World Frame Shift: \", testset[\"Shift_World_Frame_Type\"])\n",
    "print(\"Contact Location Representation Type: \",testset[\"Contact_Representation_Type\"])\n",
    "print(\"Scaling Factor of Variables: \",testset[\"VectorScaleFactor\"])\n",
    "print(\"Number of Preview Steps: \", testset[\"NumPreviewSteps\"])\n",
    "print(\"Pre Process Mode: \",testset[\"PreProcessMode\"])\n",
    "print(\"Test Set Size\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim:  85\n",
      "output dim: 11\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Decide input and outpu dimensionality\n",
    "d_in = x_train[0].shape[0]\n",
    "print(\"input dim: \", d_in)\n",
    "d_out = y_train[0].shape[0]\n",
    "print(\"output dim:\", d_out)\n",
    "print(\" \")\n",
    "\n",
    "# #Double check with mean and std\n",
    "# print(\"Inputs: \")\n",
    "# print(\"Input Mean: \", x_train.mean(axis=0))\n",
    "# print(\"Input Std: \", x_train.std(axis=0))\n",
    "# print(\"Input Max: \", x_train.max(axis=0))\n",
    "# print(\"Input Min: \", x_train.min(axis=0))\n",
    "# print(\" \")\n",
    "\n",
    "\n",
    "# print(\"Output Mean: \", y_train.mean(axis=0))\n",
    "# print(\"Output Std: \", y_train.std(axis=0))\n",
    "# print(\"Output Max: \", y_train.max(axis=0))\n",
    "# print(\"Output Min: \", y_train.min(axis=0))\n",
    "\n",
    "# print(\"Final Data Set Size\")\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define learning model\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,)),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(d_out)\n",
    "# ])\n",
    "# loss: 4.6886e-04 - val_loss: 5.4786e-04\n",
    "\n",
    "# #True code\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,)), #tanh\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(d_out, activation='linear')\n",
    "# ])\n",
    "\n",
    "# #True code\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,), kernel_regularizer='l1'), #tanh\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(d_out, activation='linear')\n",
    "# ])\n",
    "\n",
    "#True code\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(d_in,), ), #tanh\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(d_out, activation='linear')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 3.0915e-04 - val_loss: 9.2230e-04\n",
      "Epoch 2/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.6468e-04 - val_loss: 9.4241e-04\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3526e-04 - val_loss: 9.2448e-04\n",
      "Epoch 4/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2990e-04 - val_loss: 9.2797e-04\n",
      "Epoch 5/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2788e-04 - val_loss: 9.2430e-04\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2773e-04 - val_loss: 9.2754e-04\n",
      "Epoch 7/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2844e-04 - val_loss: 9.2099e-04\n",
      "Epoch 8/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2850e-04 - val_loss: 9.3163e-04\n",
      "Epoch 9/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2847e-04 - val_loss: 9.3967e-04\n",
      "Epoch 10/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2978e-04 - val_loss: 9.2528e-04\n",
      "Epoch 11/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2991e-04 - val_loss: 9.3990e-04\n",
      "Epoch 12/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3007e-04 - val_loss: 9.1469e-04\n",
      "Epoch 13/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3023e-04 - val_loss: 9.1514e-04\n",
      "Epoch 14/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3007e-04 - val_loss: 9.2627e-04\n",
      "Epoch 15/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2980e-04 - val_loss: 9.2469e-04\n",
      "Epoch 16/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3108e-04 - val_loss: 9.2834e-04\n",
      "Epoch 17/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3236e-04 - val_loss: 9.0464e-04\n",
      "Epoch 18/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3574e-04 - val_loss: 9.2521e-04\n",
      "Epoch 19/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3227e-04 - val_loss: 9.1267e-04\n",
      "Epoch 20/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3254e-04 - val_loss: 9.3243e-04\n",
      "Epoch 21/500\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 1.3347e-04 - val_loss: 9.3389e-04\n",
      "Epoch 22/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4116e-04 - val_loss: 8.9897e-04\n",
      "Epoch 23/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3954e-04 - val_loss: 9.7020e-04\n",
      "Epoch 24/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4147e-04 - val_loss: 9.6167e-04\n",
      "Epoch 25/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3995e-04 - val_loss: 9.1718e-04\n",
      "Epoch 26/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3603e-04 - val_loss: 9.3099e-04\n",
      "Epoch 27/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3527e-04 - val_loss: 9.3667e-04\n",
      "Epoch 28/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3432e-04 - val_loss: 9.2701e-04\n",
      "Epoch 29/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3324e-04 - val_loss: 9.3524e-04\n",
      "Epoch 30/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3295e-04 - val_loss: 9.3405e-04\n",
      "Epoch 31/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3352e-04 - val_loss: 9.3371e-04\n",
      "Epoch 32/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3239e-04 - val_loss: 9.3460e-04\n",
      "Epoch 33/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3561e-04 - val_loss: 9.3238e-04\n",
      "Epoch 34/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3379e-04 - val_loss: 9.1848e-04\n",
      "Epoch 35/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3422e-04 - val_loss: 9.2000e-04\n",
      "Epoch 36/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3781e-04 - val_loss: 9.2830e-04\n",
      "Epoch 37/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3720e-04 - val_loss: 9.3419e-04\n",
      "Epoch 38/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4067e-04 - val_loss: 9.7077e-04\n",
      "Epoch 39/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3867e-04 - val_loss: 9.3744e-04\n",
      "Epoch 40/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3459e-04 - val_loss: 9.3750e-04\n",
      "Epoch 41/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3141e-04 - val_loss: 9.3364e-04\n",
      "Epoch 42/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2912e-04 - val_loss: 9.1790e-04\n",
      "Epoch 43/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3271e-04 - val_loss: 9.2355e-04\n",
      "Epoch 44/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3163e-04 - val_loss: 9.4059e-04\n",
      "Epoch 45/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3005e-04 - val_loss: 9.2430e-04\n",
      "Epoch 46/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3202e-04 - val_loss: 9.6246e-04\n",
      "Epoch 47/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3568e-04 - val_loss: 9.2748e-04\n",
      "Epoch 48/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3355e-04 - val_loss: 9.3101e-04\n",
      "Epoch 49/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3429e-04 - val_loss: 9.3373e-04\n",
      "Epoch 50/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3480e-04 - val_loss: 9.3451e-04\n",
      "Epoch 51/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3859e-04 - val_loss: 9.3323e-04\n",
      "Epoch 52/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3403e-04 - val_loss: 9.3047e-04\n",
      "Epoch 53/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3653e-04 - val_loss: 9.5153e-04\n",
      "Epoch 54/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3487e-04 - val_loss: 9.2596e-04\n",
      "Epoch 55/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3326e-04 - val_loss: 9.6202e-04\n",
      "Epoch 56/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3238e-04 - val_loss: 9.2785e-04\n",
      "Epoch 57/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3126e-04 - val_loss: 9.4627e-04\n",
      "Epoch 58/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3685e-04 - val_loss: 9.0371e-04\n",
      "Epoch 59/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4045e-04 - val_loss: 9.3137e-04\n",
      "Epoch 60/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3667e-04 - val_loss: 9.4103e-04\n",
      "Epoch 61/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3964e-04 - val_loss: 9.4058e-04\n",
      "Epoch 62/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3686e-04 - val_loss: 9.4828e-04\n",
      "Epoch 63/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3788e-04 - val_loss: 9.6901e-04\n",
      "Epoch 64/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3585e-04 - val_loss: 9.3347e-04\n",
      "Epoch 65/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3487e-04 - val_loss: 9.3234e-04\n",
      "Epoch 66/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3223e-04 - val_loss: 9.5841e-04\n",
      "Epoch 67/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3346e-04 - val_loss: 9.3708e-04\n",
      "Epoch 68/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3546e-04 - val_loss: 9.3683e-04\n",
      "Epoch 69/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3558e-04 - val_loss: 9.4641e-04\n",
      "Epoch 70/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3608e-04 - val_loss: 9.2964e-04\n",
      "Epoch 71/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3609e-04 - val_loss: 9.6123e-04\n",
      "Epoch 72/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3417e-04 - val_loss: 9.3522e-04\n",
      "Epoch 73/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3366e-04 - val_loss: 9.2642e-04\n",
      "Epoch 74/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3339e-04 - val_loss: 9.3229e-04\n",
      "Epoch 75/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3799e-04 - val_loss: 9.3793e-04\n",
      "Epoch 76/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.4212e-04 - val_loss: 9.5454e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4621e-04 - val_loss: 9.4319e-04\n",
      "Epoch 78/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.6023e-04 - val_loss: 9.4575e-04\n",
      "Epoch 79/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4503e-04 - val_loss: 9.3189e-04\n",
      "Epoch 80/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3847e-04 - val_loss: 9.6074e-04\n",
      "Epoch 81/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3715e-04 - val_loss: 9.3507e-04\n",
      "Epoch 82/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3431e-04 - val_loss: 9.2885e-04\n",
      "Epoch 83/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3615e-04 - val_loss: 9.2820e-04\n",
      "Epoch 84/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3294e-04 - val_loss: 9.2583e-04\n",
      "Epoch 85/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3282e-04 - val_loss: 9.5304e-04\n",
      "Epoch 86/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3321e-04 - val_loss: 9.1964e-04\n",
      "Epoch 87/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3071e-04 - val_loss: 9.0853e-04\n",
      "Epoch 88/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3385e-04 - val_loss: 9.3520e-04\n",
      "Epoch 89/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3365e-04 - val_loss: 9.3719e-04\n",
      "Epoch 90/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3064e-04 - val_loss: 9.4313e-04\n",
      "Epoch 91/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3563e-04 - val_loss: 9.3823e-04\n",
      "Epoch 92/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3778e-04 - val_loss: 9.4092e-04\n",
      "Epoch 93/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3953e-04 - val_loss: 9.2191e-04\n",
      "Epoch 94/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3757e-04 - val_loss: 9.3223e-04\n",
      "Epoch 95/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3482e-04 - val_loss: 9.3766e-04\n",
      "Epoch 96/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3130e-04 - val_loss: 9.3898e-04\n",
      "Epoch 97/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3017e-04 - val_loss: 9.2426e-04\n",
      "Epoch 98/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3327e-04 - val_loss: 9.4147e-04\n",
      "Epoch 99/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3878e-04 - val_loss: 9.1386e-04\n",
      "Epoch 100/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3954e-04 - val_loss: 9.7561e-04\n",
      "Epoch 101/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3347e-04 - val_loss: 9.5535e-04\n",
      "Epoch 102/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3257e-04 - val_loss: 9.3406e-04\n",
      "Epoch 103/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3467e-04 - val_loss: 9.4543e-04\n",
      "Epoch 104/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3521e-04 - val_loss: 9.2699e-04\n",
      "Epoch 105/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3875e-04 - val_loss: 9.4224e-04\n",
      "Epoch 106/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3386e-04 - val_loss: 9.5920e-04\n",
      "Epoch 107/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3829e-04 - val_loss: 9.2816e-04\n",
      "Epoch 108/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3462e-04 - val_loss: 9.6075e-04\n",
      "Epoch 109/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3569e-04 - val_loss: 9.2890e-04\n",
      "Epoch 110/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3293e-04 - val_loss: 9.2856e-04\n",
      "Epoch 111/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3416e-04 - val_loss: 9.2856e-04\n",
      "Epoch 112/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3577e-04 - val_loss: 9.6143e-04\n",
      "Epoch 113/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3900e-04 - val_loss: 9.2705e-04\n",
      "Epoch 114/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4211e-04 - val_loss: 9.5521e-04\n",
      "Epoch 115/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3562e-04 - val_loss: 9.2591e-04\n",
      "Epoch 116/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3522e-04 - val_loss: 9.7653e-04\n",
      "Epoch 117/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3462e-04 - val_loss: 9.5594e-04\n",
      "Epoch 118/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3506e-04 - val_loss: 9.3665e-04\n",
      "Epoch 119/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3600e-04 - val_loss: 9.3546e-04\n",
      "Epoch 120/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3840e-04 - val_loss: 9.2336e-04\n",
      "Epoch 121/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3904e-04 - val_loss: 9.2873e-04\n",
      "Epoch 122/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3550e-04 - val_loss: 9.3679e-04\n",
      "Epoch 123/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3212e-04 - val_loss: 9.4037e-04\n",
      "Epoch 124/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3212e-04 - val_loss: 9.1991e-04\n",
      "Epoch 125/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3123e-04 - val_loss: 9.3916e-04\n",
      "Epoch 126/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3148e-04 - val_loss: 9.3716e-04\n",
      "Epoch 127/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2933e-04 - val_loss: 9.5345e-04\n",
      "Epoch 128/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2890e-04 - val_loss: 9.4564e-04\n",
      "Epoch 129/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3025e-04 - val_loss: 9.3252e-04\n",
      "Epoch 130/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3124e-04 - val_loss: 9.4874e-04\n",
      "Epoch 131/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3273e-04 - val_loss: 9.1927e-04\n",
      "Epoch 132/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3072e-04 - val_loss: 9.2713e-04\n",
      "Epoch 133/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3265e-04 - val_loss: 9.2942e-04\n",
      "Epoch 134/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3406e-04 - val_loss: 9.3537e-04\n",
      "Epoch 135/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3553e-04 - val_loss: 9.3423e-04\n",
      "Epoch 136/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3290e-04 - val_loss: 9.4034e-04\n",
      "Epoch 137/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3543e-04 - val_loss: 9.5926e-04\n",
      "Epoch 138/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3454e-04 - val_loss: 9.8078e-04\n",
      "Epoch 139/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.4568e-04 - val_loss: 9.5847e-04\n",
      "Epoch 140/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4883e-04 - val_loss: 9.7795e-04\n",
      "Epoch 141/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4085e-04 - val_loss: 9.8662e-04\n",
      "Epoch 142/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4652e-04 - val_loss: 9.3654e-04\n",
      "Epoch 143/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3456e-04 - val_loss: 9.6730e-04\n",
      "Epoch 144/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3798e-04 - val_loss: 8.9789e-04\n",
      "Epoch 145/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3527e-04 - val_loss: 9.2982e-04\n",
      "Epoch 146/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3879e-04 - val_loss: 9.5064e-04\n",
      "Epoch 147/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4187e-04 - val_loss: 0.0010\n",
      "Epoch 148/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4599e-04 - val_loss: 9.2193e-04\n",
      "Epoch 149/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4392e-04 - val_loss: 9.5650e-04\n",
      "Epoch 150/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3741e-04 - val_loss: 9.3127e-04\n",
      "Epoch 151/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4206e-04 - val_loss: 9.5580e-04\n",
      "Epoch 152/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4038e-04 - val_loss: 9.3483e-04\n",
      "Epoch 153/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3543e-04 - val_loss: 9.5329e-04\n",
      "Epoch 154/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3784e-04 - val_loss: 9.4055e-04\n",
      "Epoch 155/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3357e-04 - val_loss: 9.3014e-04\n",
      "Epoch 156/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4296e-04 - val_loss: 9.4036e-04\n",
      "Epoch 157/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3788e-04 - val_loss: 9.4639e-04\n",
      "Epoch 158/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3138e-04 - val_loss: 9.4607e-04\n",
      "Epoch 159/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3483e-04 - val_loss: 9.2584e-04\n",
      "Epoch 160/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3555e-04 - val_loss: 9.2119e-04\n",
      "Epoch 161/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3466e-04 - val_loss: 9.6069e-04\n",
      "Epoch 162/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3421e-04 - val_loss: 9.4430e-04\n",
      "Epoch 163/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3182e-04 - val_loss: 9.4658e-04\n",
      "Epoch 164/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3368e-04 - val_loss: 9.5568e-04\n",
      "Epoch 165/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3196e-04 - val_loss: 9.5030e-04\n",
      "Epoch 166/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3201e-04 - val_loss: 9.3511e-04\n",
      "Epoch 167/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3428e-04 - val_loss: 9.3485e-04\n",
      "Epoch 168/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3766e-04 - val_loss: 9.2322e-04\n",
      "Epoch 169/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3707e-04 - val_loss: 9.3157e-04\n",
      "Epoch 170/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3494e-04 - val_loss: 9.2337e-04\n",
      "Epoch 171/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3547e-04 - val_loss: 9.8614e-04\n",
      "Epoch 172/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.5140e-04 - val_loss: 9.4072e-04\n",
      "Epoch 173/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3726e-04 - val_loss: 9.2908e-04\n",
      "Epoch 174/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3442e-04 - val_loss: 9.3986e-04\n",
      "Epoch 175/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3123e-04 - val_loss: 9.4007e-04\n",
      "Epoch 176/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3331e-04 - val_loss: 8.9020e-04\n",
      "Epoch 177/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3870e-04 - val_loss: 9.3085e-04\n",
      "Epoch 178/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3281e-04 - val_loss: 9.3135e-04\n",
      "Epoch 179/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3404e-04 - val_loss: 9.5178e-04\n",
      "Epoch 180/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3624e-04 - val_loss: 9.5849e-04\n",
      "Epoch 181/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3874e-04 - val_loss: 9.4575e-04\n",
      "Epoch 182/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3663e-04 - val_loss: 9.6330e-04\n",
      "Epoch 183/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4025e-04 - val_loss: 9.3489e-04\n",
      "Epoch 184/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3344e-04 - val_loss: 9.4827e-04\n",
      "Epoch 185/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3043e-04 - val_loss: 9.2136e-04\n",
      "Epoch 186/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3184e-04 - val_loss: 9.2616e-04\n",
      "Epoch 187/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3499e-04 - val_loss: 9.5203e-04\n",
      "Epoch 188/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3779e-04 - val_loss: 9.3251e-04\n",
      "Epoch 189/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3600e-04 - val_loss: 9.3440e-04\n",
      "Epoch 190/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3239e-04 - val_loss: 9.3897e-04\n",
      "Epoch 191/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2876e-04 - val_loss: 9.3679e-04\n",
      "Epoch 192/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2858e-04 - val_loss: 9.3496e-04\n",
      "Epoch 193/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2906e-04 - val_loss: 9.3989e-04\n",
      "Epoch 194/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3243e-04 - val_loss: 9.5130e-04\n",
      "Epoch 195/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2976e-04 - val_loss: 9.1134e-04\n",
      "Epoch 196/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3061e-04 - val_loss: 9.3266e-04\n",
      "Epoch 197/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3190e-04 - val_loss: 9.3594e-04\n",
      "Epoch 198/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3632e-04 - val_loss: 9.1978e-04\n",
      "Epoch 199/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3762e-04 - val_loss: 9.4841e-04\n",
      "Epoch 200/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3410e-04 - val_loss: 9.2682e-04\n",
      "Epoch 201/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3547e-04 - val_loss: 9.2776e-04\n",
      "Epoch 202/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3230e-04 - val_loss: 9.3343e-04\n",
      "Epoch 203/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3774e-04 - val_loss: 9.1645e-04\n",
      "Epoch 204/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4356e-04 - val_loss: 9.5598e-04\n",
      "Epoch 205/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4679e-04 - val_loss: 9.5125e-04\n",
      "Epoch 206/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.5014e-04 - val_loss: 9.5505e-04\n",
      "Epoch 207/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4126e-04 - val_loss: 9.3470e-04\n",
      "Epoch 208/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3591e-04 - val_loss: 9.1112e-04\n",
      "Epoch 209/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3219e-04 - val_loss: 9.3922e-04\n",
      "Epoch 210/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2933e-04 - val_loss: 9.4055e-04\n",
      "Epoch 211/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3209e-04 - val_loss: 9.3769e-04\n",
      "Epoch 212/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3303e-04 - val_loss: 9.3855e-04\n",
      "Epoch 213/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3472e-04 - val_loss: 9.5516e-04\n",
      "Epoch 214/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3124e-04 - val_loss: 9.4030e-04\n",
      "Epoch 215/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3169e-04 - val_loss: 9.7137e-04\n",
      "Epoch 216/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3477e-04 - val_loss: 9.9184e-04\n",
      "Epoch 217/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3524e-04 - val_loss: 9.3891e-04\n",
      "Epoch 218/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3155e-04 - val_loss: 9.4300e-04\n",
      "Epoch 219/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2894e-04 - val_loss: 9.3610e-04\n",
      "Epoch 220/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2845e-04 - val_loss: 9.2981e-04\n",
      "Epoch 221/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2951e-04 - val_loss: 9.3134e-04\n",
      "Epoch 222/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3175e-04 - val_loss: 9.2846e-04\n",
      "Epoch 223/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3086e-04 - val_loss: 9.3872e-04\n",
      "Epoch 224/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2995e-04 - val_loss: 9.3966e-04\n",
      "Epoch 225/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2835e-04 - val_loss: 9.5847e-04\n",
      "Epoch 226/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2981e-04 - val_loss: 9.3074e-04\n",
      "Epoch 227/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2919e-04 - val_loss: 9.2344e-04\n",
      "Epoch 228/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3923e-04 - val_loss: 9.4481e-04\n",
      "Epoch 229/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3481e-04 - val_loss: 9.3745e-04\n",
      "Epoch 230/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3253e-04 - val_loss: 9.7985e-04\n",
      "Epoch 231/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3162e-04 - val_loss: 9.3476e-04\n",
      "Epoch 232/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2857e-04 - val_loss: 9.4853e-04\n",
      "Epoch 233/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3295e-04 - val_loss: 9.1447e-04\n",
      "Epoch 234/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3600e-04 - val_loss: 9.5081e-04\n",
      "Epoch 235/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3407e-04 - val_loss: 9.5321e-04\n",
      "Epoch 236/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3065e-04 - val_loss: 9.4667e-04\n",
      "Epoch 237/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2999e-04 - val_loss: 9.1390e-04\n",
      "Epoch 238/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3175e-04 - val_loss: 9.2508e-04\n",
      "Epoch 239/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2801e-04 - val_loss: 9.2320e-04\n",
      "Epoch 240/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2828e-04 - val_loss: 9.3454e-04\n",
      "Epoch 241/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3101e-04 - val_loss: 9.2788e-04\n",
      "Epoch 242/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2989e-04 - val_loss: 9.3163e-04\n",
      "Epoch 243/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3127e-04 - val_loss: 9.7594e-04\n",
      "Epoch 244/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3077e-04 - val_loss: 9.5051e-04\n",
      "Epoch 245/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3256e-04 - val_loss: 9.3422e-04\n",
      "Epoch 246/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4484e-04 - val_loss: 9.2340e-04\n",
      "Epoch 247/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4662e-04 - val_loss: 9.5316e-04\n",
      "Epoch 248/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3753e-04 - val_loss: 9.1853e-04\n",
      "Epoch 249/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3348e-04 - val_loss: 9.7492e-04\n",
      "Epoch 250/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3266e-04 - val_loss: 9.4438e-04\n",
      "Epoch 251/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3259e-04 - val_loss: 9.3166e-04\n",
      "Epoch 252/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2923e-04 - val_loss: 9.3224e-04\n",
      "Epoch 253/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2956e-04 - val_loss: 9.1097e-04\n",
      "Epoch 254/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2994e-04 - val_loss: 9.4417e-04\n",
      "Epoch 255/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2928e-04 - val_loss: 9.4148e-04\n",
      "Epoch 256/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3236e-04 - val_loss: 9.1994e-04\n",
      "Epoch 257/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3079e-04 - val_loss: 9.4940e-04\n",
      "Epoch 258/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3478e-04 - val_loss: 9.2785e-04\n",
      "Epoch 259/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3002e-04 - val_loss: 9.2770e-04\n",
      "Epoch 260/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3219e-04 - val_loss: 9.4528e-04\n",
      "Epoch 261/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3680e-04 - val_loss: 9.5382e-04\n",
      "Epoch 262/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3427e-04 - val_loss: 9.2568e-04\n",
      "Epoch 263/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3357e-04 - val_loss: 9.1741e-04\n",
      "Epoch 264/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3012e-04 - val_loss: 9.4658e-04\n",
      "Epoch 265/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3009e-04 - val_loss: 9.2737e-04\n",
      "Epoch 266/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2980e-04 - val_loss: 9.2849e-04\n",
      "Epoch 267/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3208e-04 - val_loss: 9.3139e-04\n",
      "Epoch 268/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3409e-04 - val_loss: 9.3327e-04\n",
      "Epoch 269/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3102e-04 - val_loss: 9.3951e-04\n",
      "Epoch 270/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3092e-04 - val_loss: 9.2062e-04\n",
      "Epoch 271/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3325e-04 - val_loss: 9.4770e-04\n",
      "Epoch 272/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3005e-04 - val_loss: 9.2107e-04\n",
      "Epoch 273/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3296e-04 - val_loss: 9.3897e-04\n",
      "Epoch 274/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3649e-04 - val_loss: 9.3318e-04\n",
      "Epoch 275/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3132e-04 - val_loss: 9.5999e-04\n",
      "Epoch 276/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2800e-04 - val_loss: 9.4270e-04\n",
      "Epoch 277/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3041e-04 - val_loss: 9.5770e-04\n",
      "Epoch 278/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2932e-04 - val_loss: 9.4842e-04\n",
      "Epoch 279/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3116e-04 - val_loss: 9.2903e-04\n",
      "Epoch 280/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3265e-04 - val_loss: 9.0252e-04\n",
      "Epoch 281/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4070e-04 - val_loss: 9.6674e-04\n",
      "Epoch 282/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3428e-04 - val_loss: 9.5117e-04\n",
      "Epoch 283/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3451e-04 - val_loss: 9.2829e-04\n",
      "Epoch 284/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3012e-04 - val_loss: 9.3118e-04\n",
      "Epoch 285/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3386e-04 - val_loss: 9.5416e-04\n",
      "Epoch 286/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3772e-04 - val_loss: 9.1763e-04\n",
      "Epoch 287/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3907e-04 - val_loss: 9.3499e-04\n",
      "Epoch 288/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3975e-04 - val_loss: 9.5083e-04\n",
      "Epoch 289/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3420e-04 - val_loss: 9.6775e-04\n",
      "Epoch 290/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3666e-04 - val_loss: 9.4918e-04\n",
      "Epoch 291/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3392e-04 - val_loss: 9.4176e-04\n",
      "Epoch 292/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3418e-04 - val_loss: 9.6209e-04\n",
      "Epoch 293/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3471e-04 - val_loss: 9.4104e-04\n",
      "Epoch 294/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3105e-04 - val_loss: 9.3732e-04\n",
      "Epoch 295/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3236e-04 - val_loss: 9.6560e-04\n",
      "Epoch 296/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3033e-04 - val_loss: 9.3415e-04\n",
      "Epoch 297/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3311e-04 - val_loss: 9.2440e-04\n",
      "Epoch 298/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3459e-04 - val_loss: 9.4345e-04\n",
      "Epoch 299/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3323e-04 - val_loss: 0.0010\n",
      "Epoch 300/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3350e-04 - val_loss: 9.3060e-04\n",
      "Epoch 301/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3223e-04 - val_loss: 9.4141e-04\n",
      "Epoch 302/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2929e-04 - val_loss: 9.4017e-04\n",
      "Epoch 303/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2948e-04 - val_loss: 9.5261e-04\n",
      "Epoch 304/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3230e-04 - val_loss: 9.4267e-04\n",
      "Epoch 305/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3233e-04 - val_loss: 9.5042e-04\n",
      "Epoch 306/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3446e-04 - val_loss: 9.6361e-04\n",
      "Epoch 307/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3011e-04 - val_loss: 9.1959e-04\n",
      "Epoch 308/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3004e-04 - val_loss: 9.3090e-04\n",
      "Epoch 309/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2850e-04 - val_loss: 9.6081e-04\n",
      "Epoch 310/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3150e-04 - val_loss: 9.1535e-04\n",
      "Epoch 311/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3089e-04 - val_loss: 9.3525e-04\n",
      "Epoch 312/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3017e-04 - val_loss: 9.3761e-04\n",
      "Epoch 313/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3185e-04 - val_loss: 9.3415e-04\n",
      "Epoch 314/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3072e-04 - val_loss: 9.5338e-04\n",
      "Epoch 315/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3049e-04 - val_loss: 9.2097e-04\n",
      "Epoch 316/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3011e-04 - val_loss: 9.4520e-04\n",
      "Epoch 317/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.2693e-04 - val_loss: 9.3361e-04\n",
      "Epoch 318/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.2863e-04 - val_loss: 9.5481e-04\n",
      "Epoch 319/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3032e-04 - val_loss: 9.2684e-04\n",
      "Epoch 320/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.2975e-04 - val_loss: 9.2706e-04\n",
      "Epoch 321/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3118e-04 - val_loss: 9.4641e-04\n",
      "Epoch 322/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3344e-04 - val_loss: 9.3750e-04\n",
      "Epoch 323/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3385e-04 - val_loss: 9.5540e-04\n",
      "Epoch 324/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3340e-04 - val_loss: 9.4062e-04\n",
      "Epoch 325/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3360e-04 - val_loss: 9.7753e-04\n",
      "Epoch 326/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3758e-04 - val_loss: 9.1582e-04\n",
      "Epoch 327/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3196e-04 - val_loss: 9.4890e-04\n",
      "Epoch 328/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3293e-04 - val_loss: 9.3756e-04\n",
      "Epoch 329/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3114e-04 - val_loss: 9.8442e-04\n",
      "Epoch 330/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3233e-04 - val_loss: 9.4788e-04\n",
      "Epoch 331/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3033e-04 - val_loss: 9.7230e-04\n",
      "Epoch 332/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3270e-04 - val_loss: 9.2427e-04\n",
      "Epoch 333/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3180e-04 - val_loss: 9.3262e-04\n",
      "Epoch 334/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2721e-04 - val_loss: 9.4242e-04\n",
      "Epoch 335/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2802e-04 - val_loss: 9.1854e-04\n",
      "Epoch 336/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3085e-04 - val_loss: 9.2158e-04\n",
      "Epoch 337/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3022e-04 - val_loss: 9.3946e-04\n",
      "Epoch 338/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3250e-04 - val_loss: 9.5354e-04\n",
      "Epoch 339/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3134e-04 - val_loss: 9.3542e-04\n",
      "Epoch 340/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2895e-04 - val_loss: 9.7064e-04\n",
      "Epoch 341/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2895e-04 - val_loss: 9.2176e-04\n",
      "Epoch 342/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3074e-04 - val_loss: 9.2887e-04\n",
      "Epoch 343/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2734e-04 - val_loss: 9.5781e-04\n",
      "Epoch 344/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2711e-04 - val_loss: 9.2524e-04\n",
      "Epoch 345/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2976e-04 - val_loss: 9.5416e-04\n",
      "Epoch 346/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3045e-04 - val_loss: 9.9228e-04\n",
      "Epoch 347/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3169e-04 - val_loss: 9.3421e-04\n",
      "Epoch 348/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2631e-04 - val_loss: 9.3612e-04\n",
      "Epoch 349/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2958e-04 - val_loss: 9.6199e-04\n",
      "Epoch 350/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4039e-04 - val_loss: 9.2985e-04\n",
      "Epoch 351/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.5618e-04 - val_loss: 9.4979e-04\n",
      "Epoch 352/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3894e-04 - val_loss: 9.3831e-04\n",
      "Epoch 353/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3157e-04 - val_loss: 9.5610e-04\n",
      "Epoch 354/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3166e-04 - val_loss: 9.4945e-04\n",
      "Epoch 355/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3395e-04 - val_loss: 9.5551e-04\n",
      "Epoch 356/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3579e-04 - val_loss: 9.2918e-04\n",
      "Epoch 357/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3800e-04 - val_loss: 9.6087e-04\n",
      "Epoch 358/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3362e-04 - val_loss: 9.2853e-04\n",
      "Epoch 359/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3057e-04 - val_loss: 9.5852e-04\n",
      "Epoch 360/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3333e-04 - val_loss: 8.9187e-04\n",
      "Epoch 361/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3564e-04 - val_loss: 9.9413e-04\n",
      "Epoch 362/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3784e-04 - val_loss: 9.7515e-04\n",
      "Epoch 363/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3804e-04 - val_loss: 9.2281e-04\n",
      "Epoch 364/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4425e-04 - val_loss: 9.3437e-04\n",
      "Epoch 365/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4622e-04 - val_loss: 9.6316e-04\n",
      "Epoch 366/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.7893e-04 - val_loss: 9.8471e-04\n",
      "Epoch 367/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.7834e-04 - val_loss: 9.6522e-04\n",
      "Epoch 368/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.4861e-04 - val_loss: 9.3007e-04\n",
      "Epoch 369/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3572e-04 - val_loss: 9.4730e-04\n",
      "Epoch 370/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3043e-04 - val_loss: 9.3554e-04\n",
      "Epoch 371/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2984e-04 - val_loss: 9.3355e-04\n",
      "Epoch 372/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2646e-04 - val_loss: 9.2955e-04\n",
      "Epoch 373/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2846e-04 - val_loss: 9.3700e-04\n",
      "Epoch 374/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2763e-04 - val_loss: 9.2083e-04\n",
      "Epoch 375/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2657e-04 - val_loss: 9.4227e-04\n",
      "Epoch 376/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2699e-04 - val_loss: 9.3271e-04\n",
      "Epoch 377/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2586e-04 - val_loss: 9.4733e-04\n",
      "Epoch 378/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3014e-04 - val_loss: 9.4695e-04\n",
      "Epoch 379/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2889e-04 - val_loss: 9.4255e-04\n",
      "Epoch 380/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2740e-04 - val_loss: 9.5458e-04\n",
      "Epoch 381/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2638e-04 - val_loss: 9.3694e-04\n",
      "Epoch 382/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2571e-04 - val_loss: 9.2657e-04\n",
      "Epoch 383/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2643e-04 - val_loss: 9.2956e-04\n",
      "Epoch 384/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3142e-04 - val_loss: 9.3639e-04\n",
      "Epoch 385/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3046e-04 - val_loss: 9.4807e-04\n",
      "Epoch 386/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2818e-04 - val_loss: 9.2632e-04\n",
      "Epoch 387/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2521e-04 - val_loss: 9.4411e-04\n",
      "Epoch 388/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2499e-04 - val_loss: 9.2631e-04\n",
      "Epoch 389/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3175e-04 - val_loss: 9.4804e-04\n",
      "Epoch 390/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3098e-04 - val_loss: 9.3884e-04\n",
      "Epoch 391/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2671e-04 - val_loss: 9.3423e-04\n",
      "Epoch 392/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2568e-04 - val_loss: 9.3620e-04\n",
      "Epoch 393/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2573e-04 - val_loss: 9.3686e-04\n",
      "Epoch 394/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2657e-04 - val_loss: 9.1261e-04\n",
      "Epoch 395/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2950e-04 - val_loss: 9.6100e-04\n",
      "Epoch 396/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2925e-04 - val_loss: 9.3592e-04\n",
      "Epoch 397/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2876e-04 - val_loss: 9.3326e-04\n",
      "Epoch 398/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3072e-04 - val_loss: 9.2989e-04\n",
      "Epoch 399/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3122e-04 - val_loss: 9.5034e-04\n",
      "Epoch 400/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3887e-04 - val_loss: 9.5094e-04\n",
      "Epoch 401/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2993e-04 - val_loss: 9.2975e-04\n",
      "Epoch 402/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2893e-04 - val_loss: 9.3071e-04\n",
      "Epoch 403/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2637e-04 - val_loss: 9.4833e-04\n",
      "Epoch 404/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2895e-04 - val_loss: 9.5469e-04\n",
      "Epoch 405/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3602e-04 - val_loss: 9.3986e-04\n",
      "Epoch 406/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3392e-04 - val_loss: 9.4688e-04\n",
      "Epoch 407/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3117e-04 - val_loss: 9.5126e-04\n",
      "Epoch 408/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3097e-04 - val_loss: 9.2703e-04\n",
      "Epoch 409/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2978e-04 - val_loss: 9.7263e-04\n",
      "Epoch 410/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3376e-04 - val_loss: 9.0592e-04\n",
      "Epoch 411/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2984e-04 - val_loss: 9.3651e-04\n",
      "Epoch 412/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2708e-04 - val_loss: 9.1309e-04\n",
      "Epoch 413/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3070e-04 - val_loss: 9.7924e-04\n",
      "Epoch 414/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3448e-04 - val_loss: 9.3915e-04\n",
      "Epoch 415/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2727e-04 - val_loss: 9.4864e-04\n",
      "Epoch 416/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2819e-04 - val_loss: 9.6424e-04\n",
      "Epoch 417/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3053e-04 - val_loss: 9.5439e-04\n",
      "Epoch 418/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2979e-04 - val_loss: 9.7908e-04\n",
      "Epoch 419/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3390e-04 - val_loss: 9.2198e-04\n",
      "Epoch 420/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3350e-04 - val_loss: 9.4339e-04\n",
      "Epoch 421/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2864e-04 - val_loss: 9.3484e-04\n",
      "Epoch 422/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2728e-04 - val_loss: 9.2199e-04\n",
      "Epoch 423/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3295e-04 - val_loss: 9.4318e-04\n",
      "Epoch 424/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2920e-04 - val_loss: 9.4053e-04\n",
      "Epoch 425/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3259e-04 - val_loss: 9.4495e-04\n",
      "Epoch 426/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2870e-04 - val_loss: 9.3197e-04\n",
      "Epoch 427/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2742e-04 - val_loss: 9.3148e-04\n",
      "Epoch 428/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2815e-04 - val_loss: 9.3142e-04\n",
      "Epoch 429/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2840e-04 - val_loss: 9.3087e-04\n",
      "Epoch 430/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2912e-04 - val_loss: 9.4054e-04\n",
      "Epoch 431/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3115e-04 - val_loss: 9.3893e-04\n",
      "Epoch 432/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2793e-04 - val_loss: 9.3831e-04\n",
      "Epoch 433/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2934e-04 - val_loss: 9.6452e-04\n",
      "Epoch 434/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3071e-04 - val_loss: 9.3699e-04\n",
      "Epoch 435/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3426e-04 - val_loss: 9.1644e-04\n",
      "Epoch 436/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3733e-04 - val_loss: 9.2388e-04\n",
      "Epoch 437/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2992e-04 - val_loss: 9.5047e-04\n",
      "Epoch 438/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2854e-04 - val_loss: 9.4957e-04\n",
      "Epoch 439/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2800e-04 - val_loss: 9.1675e-04\n",
      "Epoch 440/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2748e-04 - val_loss: 9.3829e-04\n",
      "Epoch 441/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2828e-04 - val_loss: 9.4249e-04\n",
      "Epoch 442/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2910e-04 - val_loss: 9.4128e-04\n",
      "Epoch 443/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2866e-04 - val_loss: 9.3259e-04\n",
      "Epoch 444/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2824e-04 - val_loss: 9.5863e-04\n",
      "Epoch 445/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3029e-04 - val_loss: 9.2832e-04\n",
      "Epoch 446/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2963e-04 - val_loss: 9.4977e-04\n",
      "Epoch 447/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2726e-04 - val_loss: 9.3986e-04\n",
      "Epoch 448/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2812e-04 - val_loss: 9.3432e-04\n",
      "Epoch 449/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3190e-04 - val_loss: 9.3023e-04\n",
      "Epoch 450/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3263e-04 - val_loss: 9.4005e-04\n",
      "Epoch 451/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3225e-04 - val_loss: 9.3979e-04\n",
      "Epoch 452/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2682e-04 - val_loss: 9.4290e-04\n",
      "Epoch 453/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2709e-04 - val_loss: 9.5845e-04\n",
      "Epoch 454/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2669e-04 - val_loss: 9.4501e-04\n",
      "Epoch 455/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2780e-04 - val_loss: 9.2556e-04\n",
      "Epoch 456/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2569e-04 - val_loss: 9.3649e-04\n",
      "Epoch 457/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3087e-04 - val_loss: 9.4521e-04\n",
      "Epoch 458/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4034e-04 - val_loss: 9.2553e-04\n",
      "Epoch 459/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3259e-04 - val_loss: 9.3642e-04\n",
      "Epoch 460/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3139e-04 - val_loss: 9.4229e-04\n",
      "Epoch 461/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3316e-04 - val_loss: 9.2448e-04\n",
      "Epoch 462/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3092e-04 - val_loss: 9.4472e-04\n",
      "Epoch 463/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3380e-04 - val_loss: 9.2354e-04\n",
      "Epoch 464/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3313e-04 - val_loss: 9.8628e-04\n",
      "Epoch 465/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3082e-04 - val_loss: 9.6130e-04\n",
      "Epoch 466/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3109e-04 - val_loss: 9.6008e-04\n",
      "Epoch 467/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2748e-04 - val_loss: 9.3512e-04\n",
      "Epoch 468/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2850e-04 - val_loss: 9.2976e-04\n",
      "Epoch 469/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2727e-04 - val_loss: 9.3861e-04\n",
      "Epoch 470/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2758e-04 - val_loss: 9.3154e-04\n",
      "Epoch 471/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2764e-04 - val_loss: 9.2182e-04\n",
      "Epoch 472/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3129e-04 - val_loss: 9.8418e-04\n",
      "Epoch 473/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3046e-04 - val_loss: 9.5818e-04\n",
      "Epoch 474/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2891e-04 - val_loss: 9.5464e-04\n",
      "Epoch 475/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2567e-04 - val_loss: 9.5224e-04\n",
      "Epoch 476/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3048e-04 - val_loss: 9.3803e-04\n",
      "Epoch 477/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3174e-04 - val_loss: 9.6290e-04\n",
      "Epoch 478/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2850e-04 - val_loss: 9.4914e-04\n",
      "Epoch 479/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2501e-04 - val_loss: 9.5697e-04\n",
      "Epoch 480/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2707e-04 - val_loss: 9.2209e-04\n",
      "Epoch 481/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2394e-04 - val_loss: 9.3001e-04\n",
      "Epoch 482/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2301e-04 - val_loss: 9.4307e-04\n",
      "Epoch 483/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2466e-04 - val_loss: 9.3248e-04\n",
      "Epoch 484/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2785e-04 - val_loss: 9.5935e-04\n",
      "Epoch 485/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2724e-04 - val_loss: 9.7396e-04\n",
      "Epoch 486/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3186e-04 - val_loss: 9.6210e-04\n",
      "Epoch 487/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3372e-04 - val_loss: 9.2685e-04\n",
      "Epoch 488/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3969e-04 - val_loss: 9.4540e-04\n",
      "Epoch 489/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3531e-04 - val_loss: 9.4971e-04\n",
      "Epoch 490/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2988e-04 - val_loss: 9.5314e-04\n",
      "Epoch 491/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2724e-04 - val_loss: 9.2880e-04\n",
      "Epoch 492/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2820e-04 - val_loss: 9.4481e-04\n",
      "Epoch 493/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3027e-04 - val_loss: 9.1881e-04\n",
      "Epoch 494/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2860e-04 - val_loss: 9.2869e-04\n",
      "Epoch 495/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2773e-04 - val_loss: 9.3093e-04\n",
      "Epoch 496/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2637e-04 - val_loss: 9.3126e-04\n",
      "Epoch 497/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2857e-04 - val_loss: 9.3678e-04\n",
      "Epoch 498/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2859e-04 - val_loss: 9.4450e-04\n",
      "Epoch 499/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3106e-04 - val_loss: 9.3846e-04\n",
      "Epoch 500/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2739e-04 - val_loss: 9.4200e-04\n"
     ]
    }
   ],
   "source": [
    "#Train Learning Model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='mse') #0.0001\n",
    "\n",
    "#history = model.fit(x_train, y_train, epochs = 50000, validation_split=0.0, batch_size = x_train.shape[0])\n",
    "#history = model.fit(x_train, y_train, epochs = 3000, validation_split=0.0, batch_size = 1280) #1280\n",
    "#Batch size = 1280 for remove outlier, 2560 for keep outlier\n",
    "history = model.fit(x = x_train, y = y_train, epochs = 500, batch_size = 1280, validation_data = (x_valid, y_valid),shuffle=True) #1280, 1000 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot Training Progree\n",
    "plt.plot(history.history['loss'], 'r', label='loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(history.history['val_loss'], 'b', label='val_loss') if 'val_loss' in history.history else None\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/jiayu/Seagate/LargeSlope_Angle_23_X_negative//ML_Models/NN_Model_InitialSet/assets\n"
     ]
    }
   ],
   "source": [
    "#Save Trained Model\n",
    "#MLmodel_name = \"NN_Model_Valid_\" + trainingset[\"PreProcessMode\"] + \"_Dagger_InitSet_2Iter\"\n",
    "#MLmodel_name = \"NN_Model\" + \"_\" + \"AugVarStep_1to2StepbeforeFail_3Time_RemovebyClip_SmallThre\"\n",
    "MLmodel_name = \"NN_Model\" + \"_\" + \"InitialSet\"\n",
    "model.save(ML_Model_Path + MLmodel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DataSet Setttings\n",
    "datasetSettings = {\"Shift_World_Frame_Type\":trainingset[\"Shift_World_Frame_Type\"],\n",
    "                   \"VectorScaleFactor\":trainingset[\"VectorScaleFactor\"],\n",
    "                   \"NumPreviewSteps\":trainingset[\"NumPreviewSteps\"],\n",
    "                   \"Contact_Representation_Type\":trainingset[\"Contact_Representation_Type\"],\n",
    "                   \"TrainingLoss\":history.history['loss']}\n",
    "#Validation loss\n",
    "datasetSettings[\"ValidationLoss\"] = history.history['val_loss'] if 'val_loss' in history.history else None\n",
    "\n",
    "#ProProcess\n",
    "datasetSettings[\"PreProcessMode\"] = trainingset[\"PreProcessMode\"]\n",
    "datasetSettings[\"Scaler_X\"] = trainingset[\"Scaler_X\"]\n",
    "datasetSettings[\"Scaler_Y\"] = trainingset[\"Scaler_Y\"]\n",
    "\n",
    "#Dump File\n",
    "pickle.dump(datasetSettings, open(ML_Model_Path + MLmodel_name+ '/datasetSettings' +'.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.07821036e-02 -3.31458895e-02  7.74266390e-01  1.25860383e-01\n",
      "  9.44217431e-02 -4.26340612e-03 -1.17954869e-03 -7.03362466e-04\n",
      "  3.27587628e-04 -2.52477771e-01 -1.97323335e-01  1.63386488e-01\n",
      "  0.00000000e+00  4.34687402e-01  8.63058304e-01 -1.03553277e-12\n",
      " -1.40312598e-01  8.63058304e-01 -1.03553277e-12 -1.40312598e-01\n",
      " -1.50710342e-01 -1.03547726e-12  4.34687402e-01 -1.50710342e-01\n",
      " -1.03547726e-12 -1.40312598e-01 -1.23528802e-01  1.63238898e-01\n",
      " -7.15312598e-01 -1.23528802e-01  1.63238898e-01 -7.15312598e-01\n",
      " -1.13695995e+00  1.65265763e-01 -1.40312598e-01 -1.13695995e+00\n",
      "  1.65265763e-01  4.34687402e-01 -1.44655945e-01  3.63597126e-02\n",
      " -1.40312598e-01 -1.29414448e-01  1.27892618e-01 -1.40312598e-01\n",
      " -1.11583280e+00  2.92144949e-01  4.34687402e-01 -1.13107430e+00\n",
      "  2.00612044e-01  1.00968740e+00  8.40056243e-01 -1.38139014e-01\n",
      "  4.34687402e-01  8.58710075e-01 -2.61133173e-02  4.34687402e-01\n",
      " -1.27708281e-01  1.38139014e-01  1.00968740e+00 -1.46362112e-01\n",
      "  2.61133173e-02  1.00968740e+00 -1.71854430e-01 -1.26980949e-01\n",
      "  4.34687402e-01 -1.71854430e-01 -1.26980949e-01  4.34687402e-01\n",
      " -1.08863432e+00  4.55485611e-01  1.00968740e+00 -1.08863432e+00\n",
      "  4.55485611e-01  1.58468740e+00  8.45216815e-01 -1.07147171e-01\n",
      "  1.00968740e+00  8.53549503e-01 -5.71051602e-02  1.00968740e+00\n",
      " -1.32868852e-01  1.07147171e-01  1.58468740e+00 -1.41201541e-01\n",
      "  5.71051602e-02]\n",
      "Data Kept Original Form, But need to scale back to meters\n",
      "predicted result: \n",
      " [[ 0.15049039 -0.14373763  0.7270921   0.17958589 -0.16706315 -0.01963924\n",
      "  -0.00262958 -0.00214338 -0.00205797  0.8098822   0.8248515 ]]\n",
      "true value: \n",
      " [ 1.51031156e-01 -1.38540662e-01  7.26007803e-01  1.94814576e-01\n",
      " -1.49737024e-01  8.44262578e-03 -3.45339628e-03  2.00287304e-03\n",
      " -8.27120809e-04  8.11139111e-01  8.36389438e-01]\n",
      "diff: \n",
      " [[0.00054077 0.00519697 0.00108428 0.01522869 0.01732612 0.02808187\n",
      "  0.00082382 0.00414625 0.00123085 0.00125689 0.01153793]]\n"
     ]
    }
   ],
   "source": [
    "#Show Prediction Result for Training\n",
    "from sklearn import preprocessing\n",
    "\n",
    "datapoint_num = 33\n",
    "y_pred_temp = model.predict(np.array([x_train[datapoint_num]]))\n",
    "\n",
    "print(x_train[datapoint_num])\n",
    "\n",
    "#Recover to original format\n",
    "if trainingset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_originalform = y_pred_temp/trainingset[\"VectorScaleFactor\"]\n",
    "    y_true_originalform = y_train[datapoint_num]/trainingset[\"VectorScaleFactor\"]\n",
    "elif trainingset[\"PreProcessMode\"] == \"Standarization\" or trainingset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    y_pred_originalform = dataset[\"Scaler_Y\"].inverse_transform(y_pred_temp)\n",
    "    y_true_originalform = dataset[\"Scaler_Y\"].inverse_transform(np.array([y_train[datapoint_num]]))\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "\n",
    "print(\"predicted result: \\n\",y_pred_originalform)\n",
    "print(\"true value: \\n\",y_true_originalform)\n",
    "print(\"diff: \\n\", np.absolute(y_pred_originalform - y_true_originalform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.06155035 0.06156018 0.06157594 0.06158588 0.06158747 0.06159318\n",
      " 0.06168313 0.06174151 0.0617531  0.06177521 0.06177932 0.0618281\n",
      " 0.06182899 0.06183657 0.06185253 0.06189248 0.06190004 0.06191104\n",
      " 0.06191486 0.06192473 0.06193342 0.06195932 0.06196308 0.06196735\n",
      " 0.06200968 0.06202425 0.06202608 0.06204023 0.06204479 0.06205481\n",
      " 0.06206199 0.06207386 0.06207659 0.06209848 0.06214875 0.06215154\n",
      " 0.06215632 0.06221035 0.06221355 0.06222449 0.06222487 0.06223612\n",
      " 0.06236135 0.06239984 0.06240358 0.06240586 0.06241059 0.06242532\n",
      " 0.06244328 0.06245637 0.06245742 0.06246603 0.06249284 0.0624938\n",
      " 0.06251245 0.06256309 0.06261047 0.06263583 0.06264703 0.06265009\n",
      " 0.0626721  0.06268895 0.06272497 0.06272601 0.06274676 0.06276476\n",
      " 0.06277305 0.06277553 0.06283232 0.06283807 0.06287783 0.06293017\n",
      " 0.06293465 0.06293647 0.06294615 0.06295037 0.06299393 0.06299938\n",
      " 0.06304461 0.06304746 0.06310309 0.06313656 0.06314606 0.06316942\n",
      " 0.06320882 0.06322387 0.06323851 0.06324805 0.06325356 0.0632661\n",
      " 0.06327936 0.06330388 0.06330589 0.0633537  0.0633603  0.06338406\n",
      " 0.0633966  0.06340261 0.06342126 0.06350717 0.06353964 0.06355922\n",
      " 0.06359694 0.06360981 0.06362531 0.06362996 0.06363316 0.06366448\n",
      " 0.06367616 0.06367785 0.06368667 0.06368724 0.06369167 0.06373177\n",
      " 0.06374211 0.06377283 0.0637875  0.06379077 0.063807   0.06386024\n",
      " 0.06388763 0.06405489 0.06406561 0.0640812  0.06409088 0.06411457\n",
      " 0.06412705 0.06415212 0.06416178 0.06416483 0.0641654  0.06420315\n",
      " 0.06421159 0.06421812 0.06422962 0.06427842 0.0642838  0.06430788\n",
      " 0.06438689 0.06442687 0.0644404  0.06445754 0.06445786 0.06446707\n",
      " 0.0644726  0.06448113 0.06453346 0.06455201 0.06456127 0.06458153\n",
      " 0.06459201 0.06461747 0.06461879 0.0646305  0.06464264 0.06466118\n",
      " 0.06469218 0.06471988 0.06472039 0.0647598  0.06479368 0.06480069\n",
      " 0.06481096 0.06482396 0.06483154 0.06483692 0.06485066 0.06485159\n",
      " 0.06487933 0.06491701 0.06492693 0.06495591 0.06495908 0.06498217\n",
      " 0.06498268 0.0649975  0.06499885 0.06500659 0.06503586 0.06505863\n",
      " 0.06506752 0.06507553 0.06509499 0.06511447 0.06511586 0.06511895\n",
      " 0.06514886 0.06517492 0.06520359 0.06520362 0.06521571 0.06522497\n",
      " 0.06533151 0.06534625 0.06536042 0.06540127 0.06541713 0.06546279\n",
      " 0.0654633  0.06547236 0.06550632 0.0655079  0.06551295 0.0655213\n",
      " 0.0655617  0.0656073  0.06561725 0.06562401 0.06568371 0.06568495\n",
      " 0.0656868  0.06569756 0.06570785 0.06573846 0.06575461 0.06578455\n",
      " 0.06580035 0.06580399 0.06586024 0.06587488 0.0658823  0.06589755\n",
      " 0.06591841 0.06593258 0.06594591 0.06598347 0.06598535 0.06599648\n",
      " 0.06601808 0.06612002 0.06616464 0.06616967 0.06617803 0.06618169\n",
      " 0.06618341 0.0662311  0.06630263 0.06630713 0.06636256 0.06640907\n",
      " 0.06646361 0.06648341 0.06648479 0.06650534 0.06650613 0.06652743\n",
      " 0.06653985 0.06654904 0.0665992  0.06660324 0.06667839 0.06671141\n",
      " 0.06672149 0.06679166 0.0667961  0.06685399 0.06685541 0.06690335\n",
      " 0.06713776 0.06713779 0.06715633 0.0671616  0.06719447 0.06719689\n",
      " 0.06724695 0.06726345 0.06730704 0.06732954 0.06733728 0.06735639\n",
      " 0.06739611 0.06740191 0.06740928 0.06744033 0.06744759 0.06745951\n",
      " 0.06750008 0.06750043 0.06756476 0.06764136 0.06765468 0.0676551\n",
      " 0.06768089 0.06772948 0.06774906 0.06776174 0.06777004 0.06778101\n",
      " 0.06780678 0.06789224 0.06791148 0.06794587 0.06795521 0.06796132\n",
      " 0.06799026 0.06802005 0.0680571  0.06806515 0.0681271  0.06816704\n",
      " 0.06818516 0.06819606 0.06821936 0.06822063 0.06824457 0.06826289\n",
      " 0.06835433 0.06839202 0.06842498 0.06846201 0.06849873 0.06851732\n",
      " 0.06851994 0.06856278 0.06859568 0.06862511 0.06863412 0.06864529\n",
      " 0.06865988 0.06866795 0.06868078 0.06871819 0.06872925 0.06880428\n",
      " 0.0688288  0.06885713 0.06890646 0.06899896 0.06904719 0.06908257\n",
      " 0.0691608  0.06919599 0.06920074 0.06922042 0.06924146 0.06927704\n",
      " 0.06928355 0.06931113 0.06933427 0.06937659 0.069379   0.06946121\n",
      " 0.0694781  0.06955422 0.06955462 0.06957261 0.06958348 0.06961349\n",
      " 0.06962107 0.06962934 0.06966016 0.06966291 0.06967024 0.06967054\n",
      " 0.06971124 0.06974327 0.06978886 0.06981561 0.06986088 0.06987407\n",
      " 0.06988616 0.06991586 0.06993311 0.06993676 0.06997966 0.07001465\n",
      " 0.07003559 0.07005849 0.07006384 0.07009544 0.0701315  0.07016172\n",
      " 0.07019056 0.07024069 0.07025972 0.07034307 0.07037725 0.07042154\n",
      " 0.07043762 0.07052941 0.07055693 0.07057637 0.07063192 0.07065214\n",
      " 0.0706745  0.07080655 0.07091572 0.07094332 0.07097972 0.07104016\n",
      " 0.07108658 0.0710887  0.07109019 0.07114236 0.07119697 0.07122568\n",
      " 0.07127015 0.07127594 0.07129031 0.07131488 0.07132354 0.07133017\n",
      " 0.07135834 0.07136189 0.07144645 0.07144653 0.07146451 0.07161384\n",
      " 0.0716178  0.07162681 0.07163053 0.07175439 0.07177457 0.07178066\n",
      " 0.0717812  0.07180148 0.07180651 0.07188691 0.07189055 0.07189969\n",
      " 0.07205383 0.07207031 0.07209509 0.07226078 0.07227892 0.07231318\n",
      " 0.07234818 0.07238544 0.07240569 0.07241319 0.07242313 0.07244565\n",
      " 0.07251074 0.07252293 0.07252384 0.07257581 0.07263806 0.07264825\n",
      " 0.07267981 0.07269876 0.07281194 0.072832   0.07284638 0.07284744\n",
      " 0.07285858 0.07291263 0.07296639 0.07297694 0.07302204 0.07313626\n",
      " 0.07318624 0.07326679 0.07327573 0.07328968 0.07337315 0.07337417\n",
      " 0.07337726 0.07341929 0.07343642 0.07345923 0.07367879 0.07373354\n",
      " 0.07374867 0.07377987 0.07386268 0.07391057 0.07392918 0.07408996\n",
      " 0.07410594 0.07412965 0.07413986 0.07415944 0.07423019 0.07425816\n",
      " 0.07427122 0.07430028 0.0743028  0.07431765 0.07438495 0.07443\n",
      " 0.07446489 0.07448862 0.07451291 0.07456767 0.07462098 0.07467006\n",
      " 0.07468998 0.07470307 0.07473251 0.07488164 0.07489223 0.07493596\n",
      " 0.07499588 0.07510813 0.07513316 0.07518614 0.07519035 0.07529917\n",
      " 0.07533212 0.07535478 0.07540233 0.07540568 0.07540862 0.07547264\n",
      " 0.07550561 0.07555085 0.07556314 0.0755695  0.07560348 0.07568121\n",
      " 0.07570427 0.07572109 0.07572963 0.07573649 0.07576586 0.07577415\n",
      " 0.07586498 0.07591058 0.07592922 0.07595503 0.07603191 0.07605437\n",
      " 0.0761672  0.07618302 0.07620378 0.0762691  0.07638396 0.07644018\n",
      " 0.07645754 0.07651001 0.07652701 0.07653326 0.07655593 0.07665089\n",
      " 0.07665498 0.07674694 0.07687715 0.07692342 0.07692443 0.07692616\n",
      " 0.07693213 0.07700557 0.0770164  0.07702829 0.07709713 0.07713539\n",
      " 0.0771569  0.07721496 0.07733461 0.07738657 0.07743853 0.0774786\n",
      " 0.07758505 0.07760161 0.07763353 0.07766305 0.07771731 0.0778919\n",
      " 0.07792065 0.0779457  0.0779885  0.07812514 0.07838774 0.07843162\n",
      " 0.07844482 0.07847656 0.07861565 0.07869206 0.07869931 0.07880109\n",
      " 0.07884653 0.07900423 0.07901466 0.07903181 0.07904242 0.07906414\n",
      " 0.07912011 0.07914363 0.07928907 0.07929102 0.07968293 0.07974994\n",
      " 0.07977055 0.07991421 0.07996975 0.08010072 0.08016866 0.0802248\n",
      " 0.08024056 0.08035912 0.08036521 0.08046609 0.08046944 0.0806183\n",
      " 0.08068499 0.08070622 0.08070688 0.08091969 0.08095439 0.0810032\n",
      " 0.08135582 0.08144395 0.08144466 0.08151384 0.08154759 0.08163208\n",
      " 0.08163997 0.08167974 0.08168492 0.08172187 0.0817907  0.08185072\n",
      " 0.08199625 0.08202491 0.08202938 0.08212783 0.08228247 0.08228394\n",
      " 0.08240921 0.0824411  0.08249845 0.08250039 0.08255722 0.08258138\n",
      " 0.08274311 0.08278935 0.08279345 0.08282852 0.08286532 0.08304908\n",
      " 0.08313804 0.08317327 0.08319303 0.08324482 0.08328812 0.08333819\n",
      " 0.08336508 0.08343781 0.0835771  0.08363298 0.08363854 0.08364379\n",
      " 0.08364991 0.0840312  0.08408549 0.08409499 0.08413311 0.08414397\n",
      " 0.08414495 0.08425567 0.08434936 0.08435803 0.08480765 0.08494515\n",
      " 0.08514604 0.08522963 0.08530674 0.08542104 0.08545826 0.08548743\n",
      " 0.08569956 0.08576467 0.08596358 0.08607364 0.08607413 0.08618208\n",
      " 0.08619132 0.08619781 0.08621084 0.08626409 0.08644062 0.08662899\n",
      " 0.08667178 0.08678703 0.0868171  0.08688226 0.08690013 0.08692288\n",
      " 0.08704101 0.08710137 0.08723802 0.08732333 0.08740386 0.08752979\n",
      " 0.08758582 0.08770851 0.08773869 0.08783789 0.08803565 0.08819811\n",
      " 0.08828012 0.08857372 0.08869127 0.08881051 0.08882304 0.0889694\n",
      " 0.08898051 0.08903279 0.08912233 0.08944407 0.08944969 0.08948829\n",
      " 0.08970893 0.08978754 0.08981476 0.08992929 0.0899525  0.09007831\n",
      " 0.0901684  0.09022268 0.09026445 0.09080389 0.09086246 0.09094378\n",
      " 0.09128655 0.09150504 0.09151343 0.09159337 0.09164343 0.09167694\n",
      " 0.09184079 0.09187579 0.09198852 0.09204093 0.09207152 0.09215766\n",
      " 0.09222984 0.09238543 0.09250805 0.09279163 0.09289058 0.09312417\n",
      " 0.09336835 0.09354165 0.09361719 0.09371615 0.0938274  0.09385899\n",
      " 0.09390403 0.09393202 0.09400172 0.09414738 0.09460953 0.09471562\n",
      " 0.09488758 0.09545326 0.09549394 0.09598208 0.09601112 0.09601661\n",
      " 0.09634017 0.09640843 0.09644974 0.0970435  0.09704851 0.09705956\n",
      " 0.09712606 0.09719378 0.09754947 0.097607   0.09780687 0.09799076\n",
      " 0.09800602 0.09815112 0.09822327 0.09837254 0.09888108 0.09899572\n",
      " 0.09901787 0.0991372  0.09921538 0.09937971 0.09975741 0.09986928\n",
      " 0.10036493 0.10036712 0.10048818 0.10052551 0.10060323 0.10085806\n",
      " 0.10102741 0.10106446 0.10112389 0.1014136  0.10180734 0.10226684\n",
      " 0.1024329  0.1026237  0.10293154 0.10294688 0.10300142 0.10323302\n",
      " 0.10351806 0.1036094  0.10361226 0.1037343  0.10387019 0.10411637\n",
      " 0.10423936 0.10456283 0.10492888 0.10522424 0.10535304 0.10557545\n",
      " 0.10577508 0.10633919 0.10666255 0.10683382 0.1068578  0.10689688\n",
      " 0.10704887 0.1070952  0.10731964 0.10745511 0.10781302 0.10838983\n",
      " 0.1087858  0.10918829 0.10944445 0.10959014 0.11022713 0.11023402\n",
      " 0.11032491 0.11086319 0.11174532 0.11221969 0.1122727  0.11243429\n",
      " 0.11249827 0.11256598 0.11290919 0.11316486 0.11327094 0.1139766\n",
      " 0.1139793  0.1140562  0.1141741  0.11449012 0.11466449 0.11477955\n",
      " 0.11491049 0.11524864 0.11544942 0.11545982 0.11649891 0.11715251\n",
      " 0.11761551 0.11767521 0.11768858 0.11787882 0.11797134 0.11810226\n",
      " 0.11859993 0.11944287 0.11952288 0.1198212  0.11990865 0.12031031\n",
      " 0.12064219 0.12097863 0.12099959 0.12171282 0.12179079 0.12213327\n",
      " 0.12273559 0.12298191 0.12306153 0.12368422 0.12421457 0.12442853\n",
      " 0.12468603 0.12536097 0.12573384 0.12617888 0.12747411 0.12751912\n",
      " 0.12789092 0.12817413 0.12844971 0.12961822 0.12970563 0.13017043\n",
      " 0.1302724  0.13186217 0.13270015 0.13335655 0.13412657 0.13561131\n",
      " 0.13598375 0.13669328 0.13739516 0.13772422 0.13785792 0.13889164\n",
      " 0.13924385 0.13966251 0.13980464 0.14033191 0.14040665 0.14041835\n",
      " 0.14111099 0.14111681 0.14160064 0.14253113 0.14355159 0.14379124\n",
      " 0.144014   0.14458975 0.14520257 0.14637132 0.14691534 0.14760939\n",
      " 0.14778976 0.14880751 0.1491967  0.15005594 0.15152362 0.15199336\n",
      " 0.15206376 0.15257013 0.15347852 0.15352647 0.15446429 0.1544685\n",
      " 0.15469853 0.15499589 0.15515961 0.15584949 0.15600462 0.15723813\n",
      " 0.15758612 0.15868197 0.15921031 0.16030187 0.16123211 0.16301513\n",
      " 0.16393747 0.16637154 0.16700438 0.1672523  0.16741788 0.16830878\n",
      " 0.17028371 0.17152835 0.17153035 0.17395962 0.17446403 0.17499202\n",
      " 0.17959537 0.18103685 0.18298095 0.18435444 0.1846201  0.18489485\n",
      " 0.18548927 0.1858     0.18657872 0.18660196 0.1890441  0.189078\n",
      " 0.19185731 0.19228375 0.19314082 0.19382709 0.19433097 0.19481443\n",
      " 0.19532387 0.20254544 0.20285394 0.20621379 0.2088286  0.21133589\n",
      " 0.21712654 0.21858999 0.22006021 0.2323453  0.23606004 0.23649699\n",
      " 0.23778245 0.24110248 0.24638176 0.24747301 0.24853975 0.249422\n",
      " 0.25123524 0.25771846 0.25828734 0.2592134  0.25981752 0.26236565\n",
      " 0.26938441 0.27417931 0.28299612 0.28397805 0.2894254  0.30820124\n",
      " 0.31860163 0.44056809 0.48809148 0.60268004]\n",
      "Error Mean:  0.03172786316704872\n",
      "Error Std 0.019705680178406862\n",
      "[0.11032491 0.11086319 0.11174532 0.11221969 0.1122727  0.11243429\n",
      " 0.11256598 0.11316486 0.1139766  0.1141741  0.11466449 0.11477955\n",
      " 0.11524864 0.11545982 0.11715251 0.11761551 0.11767521 0.11787882\n",
      " 0.11797134 0.11810226 0.11952288 0.1198212  0.11990865 0.12031031\n",
      " 0.12064219 0.12097863 0.12099959 0.12171282 0.12213327 0.12306153\n",
      " 0.12468603 0.12573384 0.12751912 0.12844971 0.12961822 0.13335655\n",
      " 0.13412657 0.13561131 0.13772422 0.13785792 0.13889164 0.13924385\n",
      " 0.13980464 0.14041835 0.14111099 0.14111681 0.14355159 0.14379124\n",
      " 0.144014   0.14458975 0.14637132 0.14691534 0.14760939 0.1491967\n",
      " 0.15206376 0.15257013 0.15347852 0.15352647 0.15446429 0.15584949\n",
      " 0.15723813 0.15758612 0.15868197 0.15921031 0.16030187 0.16301513\n",
      " 0.16393747 0.16637154 0.1672523  0.16741788 0.17028371 0.17152835\n",
      " 0.17153035 0.17395962 0.17446403 0.18103685 0.18298095 0.18489485\n",
      " 0.18548927 0.1858     0.18657872 0.1890441  0.19481443 0.2088286\n",
      " 0.21133589 0.22006021 0.23606004 0.23649699 0.23778245 0.24638176\n",
      " 0.24853975 0.2592134  0.25981752 0.26236565 0.27417931 0.28397805\n",
      " 0.2894254  0.31860163 0.44056809 0.60268004]\n",
      "[26 25  2 23 20 14  5 16 19 15 15 25 26  9 23 15 15  5 25  5  5  5 18 13\n",
      "  5 13 26 23  6 23  5 18  0 10 25 15 15 19  3 28  6  5 22  5 28 11 20 26\n",
      " 12 26 15 23 25 28 26  3 29  0 25 15  5 16 25 12  9  7  9 12 22 29 27 12\n",
      "  7 15 19 27  5  0 12 12 10 20  9 10 29 25 23 22 19 20  5 29  3  0  9  6\n",
      " 13  7  5 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  0.,  1.,  3.,  0., 13.,  3.,  3.,  0.,  5.,  3.,  1.,  6.,\n",
       "         3.,  1.,  9.,  2.,  0.,  2.,  4.,  4.,  0.,  3.,  6.,  0.,  9.,\n",
       "         6.,  2.,  3.,  4.]),\n",
       " array([ 0.        ,  0.96666667,  1.93333333,  2.9       ,  3.86666667,\n",
       "         4.83333333,  5.8       ,  6.76666667,  7.73333333,  8.7       ,\n",
       "         9.66666667, 10.63333333, 11.6       , 12.56666667, 13.53333333,\n",
       "        14.5       , 15.46666667, 16.43333333, 17.4       , 18.36666667,\n",
       "        19.33333333, 20.3       , 21.26666667, 22.23333333, 23.2       ,\n",
       "        24.16666667, 25.13333333, 26.1       , 27.06666667, 28.03333333,\n",
       "        29.        ]),\n",
       " <a list of 30 Patch objects>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Training Set\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred_train = model.predict(x_train)\n",
    "\n",
    "if trainingset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_train_originalform = y_pred_train/trainingset[\"VectorScaleFactor\"]\n",
    "    y_true_train_originalform = y_train/trainingset[\"VectorScaleFactor\"]\n",
    "elif trainingset[\"PreProcessMode\"] == \"Standarization\" or trainingset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", trainingset[\"PreProcessMode\"])\n",
    "    y_pred_train_originalform = trainingset[\"Scaler_Y\"].inverse_transform(y_pred_train)\n",
    "    y_true_train_originalform = trainingset[\"Scaler_Y\"].inverse_transform(y_train)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_true_train_originalform-y_pred_train_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted[-1000:])  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())\n",
    "\n",
    "##Plot prediction on the initial dataset\n",
    "err_initdata=err[0:12000+1]\n",
    "\n",
    "err_initdata_sorted = np.sort(err_initdata)\n",
    "print(err_initdata_sorted[-100:])  # print the 100 biggest error\n",
    "\n",
    "err_initdata_idx_sorted = np.argsort(err_initdata)\n",
    "print(err_initdata_idx_sorted[-100:]%30)\n",
    "selected_err=err_initdata_idx_sorted[-100:]%30\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(selected_err, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.00708737 0.00713982 0.00793114 ... 1.79281164 2.172831   2.56889567]\n",
      "Error Mean:  0.05292238031871567\n",
      "Error Std 0.08575294017319963\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Validation Set\n",
    "\n",
    "y_pred_valid = model.predict(x_valid)\n",
    "\n",
    "\n",
    "if validationset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_valid_originalform = y_pred_valid/validationset[\"VectorScaleFactor\"]\n",
    "    y_true_valid_originalform = y_valid/validationset[\"VectorScaleFactor\"]\n",
    "elif validationset[\"PreProcessMode\"] == \"Standarization\" or validationset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", validationset[\"PreProcessMode\"])\n",
    "    y_pred_valid_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_pred_valid)\n",
    "    y_true_valid_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_valid)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_true_valid_originalform-y_pred_valid_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted)  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.00708737 0.00713982 0.00793114 ... 1.79281164 2.172831   2.56889567]\n",
      "Error Mean:  0.05292238031871567\n",
      "Error Std 0.08575294017319963\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Test Set\n",
    "\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "if testset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_test_originalform = y_pred_test/testset[\"VectorScaleFactor\"]\n",
    "    y_true_test_originalform = y_test/testset[\"VectorScaleFactor\"]\n",
    "elif testset[\"PreProcessMode\"] == \"Standarization\" or testset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", validationset[\"PreProcessMode\"])\n",
    "    y_pred_test_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_pred_test)\n",
    "    y_true_test_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_test)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_pred_test_originalform-y_true_test_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted)  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a501000252d127e7c27d83f75df0a57bca228f59033b739034a7cde4260d0152"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
