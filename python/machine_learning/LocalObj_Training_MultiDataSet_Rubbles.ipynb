{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double check the Path for storing trajectories is correct\n"
     ]
    }
   ],
   "source": [
    "#Import Packages\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from multicontact_learning_local_objectives.python.machine_learning.ml_utils import *\n",
    "import matplotlib.pyplot as plt #Matplotlib\n",
    "import shutil\n",
    "\n",
    "print(\"Double check the Path for storing trajectories is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Check we provide the Correct Traj Path: \n",
      " /home/jiayu/Desktop/MLP_DataSet/Rubbles_TimeTrack/\n"
     ]
    }
   ],
   "source": [
    "#Define Path for Storing Trajectories\n",
    "#Collect Data Points Path\n",
    "#workingDirectory = \"/home/jiayu/Desktop/multicontact_learning_local_objectives/data/large_slope_flat_patches/\"\n",
    "#workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_DaggerExact/\"\n",
    "#workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_Add2Steps\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_Add2Step_KeepOutlier\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_AddVarSteps_1to2StepbeforeFail_RemovebyClip/\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/Rubbles_Add2Steps_1StepbeforeFail_RemovebyClip/\"\n",
    "#workingDirectory = \"/media/jiayu/Seagate/LargeSlope_Angle_17_26/\"\n",
    "workingDirectory = \"/home/jiayu/Desktop/MLP_DataSet/Rubbles_TimeTrack/\"\n",
    "\n",
    "#NOTE: need to have \"/\" at the end\n",
    "print(\"Double Check we provide the Correct Traj Path: \\n\", workingDirectory)\n",
    "\n",
    "#Define dataset folder\n",
    "TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingInit\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug1Time_1StepBeforeFail_TrackTrainingInit\",\n",
    "                   workingDirectory + \"/DataSet/\"+\"TrainingAug1Time_2StepBeforeFail_TrackTrainingInit\",]\n",
    "\n",
    "# TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingSet_Initial\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"TrainingAug2Steps_1StepbeforeFail_1Time_RemovebyClip\",]\n",
    "\n",
    "# TrainingSetPath = [workingDirectory + \"/DataSet/\"+\"TrainingSet\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_1Time\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_2Time\",\n",
    "#                    workingDirectory + \"/DataSet/\"+\"Training_Aug_1StepBeforeFail_3Time\"]\n",
    "\n",
    "ValidationSetPath = workingDirectory + \"/DataSet/\"+\"ValidationSet\"\n",
    "TestSetPath = workingDirectory + \"/DataSet/\"+\"TestSet\"\n",
    "\n",
    "#Path to store ML Model, create one if we dont have\n",
    "ML_Model_Path = workingDirectory + \"/ML_Models/\"\n",
    "if not (os.path.isdir(ML_Model_Path)):\n",
    "    os.mkdir(ML_Model_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Code\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset:  0\n",
      "DataSet Sizes: \n",
      "(12000, 85)\n",
      "(12000, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  1\n",
      "DataSet Sizes: \n",
      "(17362, 85)\n",
      "(17362, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "For dataset:  2\n",
      "DataSet Sizes: \n",
      "(22058, 85)\n",
      "(22058, 14)\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      " \n",
      "Final Data Set Size\n",
      "(22058, 85)\n",
      "(22058, 14)\n",
      " \n",
      "Set Up for Validation Set\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      "Validation Set Size\n",
      "(23250, 85)\n",
      "(23250, 14)\n",
      " \n",
      " \n",
      "Set Up for Test Set\n",
      "World Frame Shift:  StanceFoot\n",
      "Contact Location Representation Type:  FollowRectangelBorder\n",
      "Scaling Factor of Variables:  1.0\n",
      "Number of Preview Steps:  4\n",
      "Pre Process Mode:  OriginalForm\n",
      "Test Set Size\n",
      "(23250, 85)\n",
      "(23250, 14)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Load DataSet File\n",
    "\n",
    "#For training set\n",
    "for trainingset_idx in range(len(TrainingSetPath)):\n",
    "    trainingset_file = TrainingSetPath[trainingset_idx] + \"/data\"+'.p'\n",
    "    trainingset = pickle.load(open(trainingset_file,\"rb\"))\n",
    "    \n",
    "    print(\"For dataset: \", trainingset_idx)\n",
    "    print(\"DataSet Sizes: \")\n",
    "    \n",
    "    if trainingset_idx == 0:\n",
    "        x_train = trainingset[\"input\"]\n",
    "        y_train = trainingset[\"output\"]\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train,trainingset[\"input\"]),axis=0)\n",
    "        y_train = np.concatenate((y_train,trainingset[\"output\"]),axis=0)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    print(\"World Frame Shift: \", trainingset[\"Shift_World_Frame_Type\"])\n",
    "    print(\"Contact Location Representation Type: \",trainingset[\"Contact_Representation_Type\"])\n",
    "    print(\"Scaling Factor of Variables: \",trainingset[\"VectorScaleFactor\"])\n",
    "    print(\"Number of Preview Steps: \", trainingset[\"NumPreviewSteps\"])\n",
    "    print(\"Pre Process Mode: \",trainingset[\"PreProcessMode\"])\n",
    "    print(\" \")\n",
    "\n",
    "print(\"Final Data Set Size\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\" \")\n",
    "\n",
    "#For validation and Test\n",
    "\n",
    "#Load Validation Set and Test Set\n",
    "validationset_file = ValidationSetPath + \"/data\"+'.p'\n",
    "validationset = pickle.load(open(validationset_file,\"rb\"))\n",
    "\n",
    "testset_file = TestSetPath + \"/data\"+'.p'\n",
    "testset = pickle.load(open(testset_file,\"rb\"))\n",
    "\n",
    "x_valid = validationset[\"input\"]\n",
    "y_valid = validationset[\"output\"]\n",
    "\n",
    "x_test = testset[\"input\"]\n",
    "y_test = testset[\"output\"]\n",
    "\n",
    "print(\"Set Up for Validation Set\")\n",
    "print(\"World Frame Shift: \", validationset[\"Shift_World_Frame_Type\"])\n",
    "print(\"Contact Location Representation Type: \",validationset[\"Contact_Representation_Type\"])\n",
    "print(\"Scaling Factor of Variables: \",validationset[\"VectorScaleFactor\"])\n",
    "print(\"Number of Preview Steps: \", validationset[\"NumPreviewSteps\"])\n",
    "print(\"Pre Process Mode: \",validationset[\"PreProcessMode\"])\n",
    "print(\"Validation Set Size\")\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(\" \")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Set Up for Test Set\")\n",
    "print(\"World Frame Shift: \", testset[\"Shift_World_Frame_Type\"])\n",
    "print(\"Contact Location Representation Type: \",testset[\"Contact_Representation_Type\"])\n",
    "print(\"Scaling Factor of Variables: \",testset[\"VectorScaleFactor\"])\n",
    "print(\"Number of Preview Steps: \", testset[\"NumPreviewSteps\"])\n",
    "print(\"Pre Process Mode: \",testset[\"PreProcessMode\"])\n",
    "print(\"Test Set Size\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim:  85\n",
      "output dim: 14\n",
      " \n",
      "[ 0.          0.1         0.75        0.          0.          0.\n",
      "  0.          0.          0.          0.          0.2         0.\n",
      "  1.          0.2         1.1         0.         -0.2         1.1\n",
      "  0.         -0.2         0.1         0.          0.2         0.1\n",
      "  0.          0.2         0.1         0.         -0.2         0.1\n",
      "  0.         -0.2        -0.9         0.          0.2        -0.9\n",
      "  0.          0.775       1.1         0.08634323  0.2         1.1\n",
      "  0.08634323  0.2         0.1        -0.08634323  0.775       0.1\n",
      " -0.08634323  0.775       0.1        -0.08892003  0.2         0.1\n",
      " -0.08892003  0.2        -0.9         0.08892003  0.775      -0.9\n",
      "  0.08892003  1.35        1.1         0.04757731  0.775       1.1\n",
      " -0.04757731  0.775       0.1        -0.04757731  1.35        0.1\n",
      "  0.04757731  1.35        0.1         0.04310632  0.775       0.1\n",
      "  0.04310632  0.775      -0.9        -0.04310632  1.35       -0.9\n",
      " -0.04310632]\n"
     ]
    }
   ],
   "source": [
    "#Decide input and outpu dimensionality\n",
    "d_in = x_train[0].shape[0]\n",
    "print(\"input dim: \", d_in)\n",
    "d_out = y_train[0].shape[0]\n",
    "print(\"output dim:\", d_out)\n",
    "print(\" \")\n",
    "\n",
    "# #Double check with mean and std\n",
    "# print(\"Inputs: \")\n",
    "# print(\"Input Mean: \", x_train.mean(axis=0))\n",
    "# print(\"Input Std: \", x_train.std(axis=0))\n",
    "# print(\"Input Max: \", x_train.max(axis=0))\n",
    "# print(\"Input Min: \", x_train.min(axis=0))\n",
    "# print(\" \")\n",
    "\n",
    "\n",
    "# print(\"Output Mean: \", y_train.mean(axis=0))\n",
    "# print(\"Output Std: \", y_train.std(axis=0))\n",
    "# print(\"Output Max: \", y_train.max(axis=0))\n",
    "# print(\"Output Min: \", y_train.min(axis=0))\n",
    "\n",
    "# print(\"Final Data Set Size\")\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define learning model\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,)),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(d_out)\n",
    "# ])\n",
    "# loss: 4.6886e-04 - val_loss: 5.4786e-04\n",
    "\n",
    "# #True code\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,)), #tanh\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(d_out, activation='linear')\n",
    "# ])\n",
    "\n",
    "# #True code\n",
    "# model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(d_in,), kernel_regularizer='l1'), #tanh\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(256, activation='relu', kernel_regularizer='l1'),\n",
    "#     Dense(d_out, activation='linear')\n",
    "# ])\n",
    "\n",
    "#True code\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(d_in,), ), #tanh\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(256, activation='relu', ),\n",
    "    Dense(d_out, activation='linear')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 7.9973e-04 - val_loss: 7.1176e-04\n",
      "Epoch 2/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 5.2025e-04 - val_loss: 5.8576e-04\n",
      "Epoch 3/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 4.3580e-04 - val_loss: 5.3382e-04\n",
      "Epoch 4/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.9568e-04 - val_loss: 5.1439e-04\n",
      "Epoch 5/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.8891e-04 - val_loss: 5.0398e-04\n",
      "Epoch 6/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7741e-04 - val_loss: 5.0488e-04\n",
      "Epoch 7/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7346e-04 - val_loss: 5.0283e-04\n",
      "Epoch 8/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6780e-04 - val_loss: 5.0396e-04\n",
      "Epoch 9/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6533e-04 - val_loss: 5.0409e-04\n",
      "Epoch 10/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6428e-04 - val_loss: 5.0469e-04\n",
      "Epoch 11/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6409e-04 - val_loss: 5.0416e-04\n",
      "Epoch 12/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6352e-04 - val_loss: 5.0454e-04\n",
      "Epoch 13/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6382e-04 - val_loss: 5.0467e-04\n",
      "Epoch 14/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6425e-04 - val_loss: 5.0625e-04\n",
      "Epoch 15/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6419e-04 - val_loss: 5.0650e-04\n",
      "Epoch 16/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6729e-04 - val_loss: 5.0892e-04\n",
      "Epoch 17/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6562e-04 - val_loss: 5.0399e-04\n",
      "Epoch 18/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6384e-04 - val_loss: 5.0466e-04\n",
      "Epoch 19/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6441e-04 - val_loss: 5.0577e-04\n",
      "Epoch 20/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6449e-04 - val_loss: 5.0791e-04\n",
      "Epoch 21/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6481e-04 - val_loss: 5.0704e-04\n",
      "Epoch 22/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6693e-04 - val_loss: 5.0490e-04\n",
      "Epoch 23/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6648e-04 - val_loss: 5.1686e-04\n",
      "Epoch 24/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6689e-04 - val_loss: 5.0536e-04\n",
      "Epoch 25/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6476e-04 - val_loss: 5.0727e-04\n",
      "Epoch 26/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6493e-04 - val_loss: 5.0864e-04\n",
      "Epoch 27/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6728e-04 - val_loss: 5.0503e-04\n",
      "Epoch 28/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6614e-04 - val_loss: 5.0773e-04\n",
      "Epoch 29/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6637e-04 - val_loss: 5.1106e-04\n",
      "Epoch 30/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6620e-04 - val_loss: 5.0514e-04\n",
      "Epoch 31/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6717e-04 - val_loss: 5.0935e-04\n",
      "Epoch 32/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6554e-04 - val_loss: 5.1114e-04\n",
      "Epoch 33/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6528e-04 - val_loss: 5.0963e-04\n",
      "Epoch 34/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6961e-04 - val_loss: 5.0475e-04\n",
      "Epoch 35/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6593e-04 - val_loss: 5.0530e-04\n",
      "Epoch 36/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6594e-04 - val_loss: 5.0716e-04\n",
      "Epoch 37/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6663e-04 - val_loss: 5.0907e-04\n",
      "Epoch 38/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6723e-04 - val_loss: 5.0732e-04\n",
      "Epoch 39/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6551e-04 - val_loss: 5.1504e-04\n",
      "Epoch 40/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6763e-04 - val_loss: 5.0568e-04\n",
      "Epoch 41/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6600e-04 - val_loss: 5.1168e-04\n",
      "Epoch 42/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6816e-04 - val_loss: 5.0680e-04\n",
      "Epoch 43/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6461e-04 - val_loss: 5.0697e-04\n",
      "Epoch 44/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6616e-04 - val_loss: 5.1009e-04\n",
      "Epoch 45/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7119e-04 - val_loss: 5.1060e-04\n",
      "Epoch 46/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6688e-04 - val_loss: 5.0699e-04\n",
      "Epoch 47/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6599e-04 - val_loss: 5.1059e-04\n",
      "Epoch 48/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6678e-04 - val_loss: 5.0722e-04\n",
      "Epoch 49/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6512e-04 - val_loss: 5.0661e-04\n",
      "Epoch 50/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6447e-04 - val_loss: 5.0521e-04\n",
      "Epoch 51/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6804e-04 - val_loss: 5.0576e-04\n",
      "Epoch 52/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6651e-04 - val_loss: 5.0733e-04\n",
      "Epoch 53/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6472e-04 - val_loss: 5.0893e-04\n",
      "Epoch 54/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6679e-04 - val_loss: 5.1041e-04\n",
      "Epoch 55/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6501e-04 - val_loss: 5.0811e-04\n",
      "Epoch 56/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6913e-04 - val_loss: 5.0527e-04\n",
      "Epoch 57/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6563e-04 - val_loss: 5.0891e-04\n",
      "Epoch 58/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6746e-04 - val_loss: 5.0712e-04\n",
      "Epoch 59/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6531e-04 - val_loss: 5.0686e-04\n",
      "Epoch 60/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6698e-04 - val_loss: 5.0867e-04\n",
      "Epoch 61/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6713e-04 - val_loss: 5.0442e-04\n",
      "Epoch 62/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6568e-04 - val_loss: 5.0797e-04\n",
      "Epoch 63/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6600e-04 - val_loss: 5.0539e-04\n",
      "Epoch 64/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6367e-04 - val_loss: 5.0758e-04\n",
      "Epoch 65/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6532e-04 - val_loss: 5.0626e-04\n",
      "Epoch 66/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6460e-04 - val_loss: 5.0712e-04\n",
      "Epoch 67/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6610e-04 - val_loss: 5.1061e-04\n",
      "Epoch 68/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6702e-04 - val_loss: 5.0640e-04\n",
      "Epoch 69/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6579e-04 - val_loss: 5.0589e-04\n",
      "Epoch 70/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6761e-04 - val_loss: 5.1236e-04\n",
      "Epoch 71/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6798e-04 - val_loss: 5.2108e-04\n",
      "Epoch 72/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6723e-04 - val_loss: 5.1068e-04\n",
      "Epoch 73/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6592e-04 - val_loss: 5.2130e-04\n",
      "Epoch 74/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6975e-04 - val_loss: 5.1101e-04\n",
      "Epoch 75/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6860e-04 - val_loss: 5.0869e-04\n",
      "Epoch 76/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6665e-04 - val_loss: 5.0594e-04\n",
      "Epoch 77/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6516e-04 - val_loss: 5.0993e-04\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6525e-04 - val_loss: 5.1113e-04\n",
      "Epoch 79/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7230e-04 - val_loss: 5.2133e-04\n",
      "Epoch 80/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7382e-04 - val_loss: 5.0927e-04\n",
      "Epoch 81/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7130e-04 - val_loss: 5.0918e-04\n",
      "Epoch 82/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6736e-04 - val_loss: 5.0531e-04\n",
      "Epoch 83/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7571e-04 - val_loss: 5.4259e-04\n",
      "Epoch 84/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.8513e-04 - val_loss: 5.2477e-04\n",
      "Epoch 85/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7541e-04 - val_loss: 5.1871e-04\n",
      "Epoch 86/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7329e-04 - val_loss: 5.1041e-04\n",
      "Epoch 87/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6730e-04 - val_loss: 5.0772e-04\n",
      "Epoch 88/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6775e-04 - val_loss: 5.0503e-04\n",
      "Epoch 89/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6638e-04 - val_loss: 5.0941e-04\n",
      "Epoch 90/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6768e-04 - val_loss: 5.1040e-04\n",
      "Epoch 91/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6894e-04 - val_loss: 5.0713e-04\n",
      "Epoch 92/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6992e-04 - val_loss: 5.2333e-04\n",
      "Epoch 93/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7498e-04 - val_loss: 5.1977e-04\n",
      "Epoch 94/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6772e-04 - val_loss: 5.1203e-04\n",
      "Epoch 95/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6642e-04 - val_loss: 5.0804e-04\n",
      "Epoch 96/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6566e-04 - val_loss: 5.0646e-04\n",
      "Epoch 97/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6510e-04 - val_loss: 5.0705e-04\n",
      "Epoch 98/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6770e-04 - val_loss: 5.0723e-04\n",
      "Epoch 99/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6672e-04 - val_loss: 5.0482e-04\n",
      "Epoch 100/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6649e-04 - val_loss: 5.0906e-04\n",
      "Epoch 101/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6971e-04 - val_loss: 5.1468e-04\n",
      "Epoch 102/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7591e-04 - val_loss: 5.1523e-04\n",
      "Epoch 103/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6901e-04 - val_loss: 5.1055e-04\n",
      "Epoch 104/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6628e-04 - val_loss: 5.1358e-04\n",
      "Epoch 105/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6968e-04 - val_loss: 5.0830e-04\n",
      "Epoch 106/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7163e-04 - val_loss: 5.1408e-04\n",
      "Epoch 107/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6785e-04 - val_loss: 5.0782e-04\n",
      "Epoch 108/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6476e-04 - val_loss: 5.1112e-04\n",
      "Epoch 109/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6676e-04 - val_loss: 5.1384e-04\n",
      "Epoch 110/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6538e-04 - val_loss: 5.1236e-04\n",
      "Epoch 111/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7079e-04 - val_loss: 5.0708e-04\n",
      "Epoch 112/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6613e-04 - val_loss: 5.1022e-04\n",
      "Epoch 113/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6578e-04 - val_loss: 5.0736e-04\n",
      "Epoch 114/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6536e-04 - val_loss: 5.0631e-04\n",
      "Epoch 115/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6376e-04 - val_loss: 5.0563e-04\n",
      "Epoch 116/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6406e-04 - val_loss: 5.0620e-04\n",
      "Epoch 117/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6371e-04 - val_loss: 5.0774e-04\n",
      "Epoch 118/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6587e-04 - val_loss: 5.1171e-04\n",
      "Epoch 119/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7135e-04 - val_loss: 5.1318e-04\n",
      "Epoch 120/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6976e-04 - val_loss: 5.1057e-04\n",
      "Epoch 121/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6607e-04 - val_loss: 5.1298e-04\n",
      "Epoch 122/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6764e-04 - val_loss: 5.1011e-04\n",
      "Epoch 123/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6776e-04 - val_loss: 5.0741e-04\n",
      "Epoch 124/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6740e-04 - val_loss: 5.0494e-04\n",
      "Epoch 125/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6485e-04 - val_loss: 5.0557e-04\n",
      "Epoch 126/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6523e-04 - val_loss: 5.0798e-04\n",
      "Epoch 127/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6929e-04 - val_loss: 5.2021e-04\n",
      "Epoch 128/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6877e-04 - val_loss: 5.0629e-04\n",
      "Epoch 129/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6593e-04 - val_loss: 5.1845e-04\n",
      "Epoch 130/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6954e-04 - val_loss: 5.1041e-04\n",
      "Epoch 131/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6508e-04 - val_loss: 5.0963e-04\n",
      "Epoch 132/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6784e-04 - val_loss: 5.1891e-04\n",
      "Epoch 133/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6814e-04 - val_loss: 5.0907e-04\n",
      "Epoch 134/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6587e-04 - val_loss: 5.0753e-04\n",
      "Epoch 135/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6456e-04 - val_loss: 5.1152e-04\n",
      "Epoch 136/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6795e-04 - val_loss: 5.1772e-04\n",
      "Epoch 137/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7328e-04 - val_loss: 5.0864e-04\n",
      "Epoch 138/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6971e-04 - val_loss: 5.1350e-04\n",
      "Epoch 139/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6799e-04 - val_loss: 5.1361e-04\n",
      "Epoch 140/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7483e-04 - val_loss: 5.1386e-04\n",
      "Epoch 141/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6767e-04 - val_loss: 5.2007e-04\n",
      "Epoch 142/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7068e-04 - val_loss: 5.1181e-04\n",
      "Epoch 143/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7161e-04 - val_loss: 5.1327e-04\n",
      "Epoch 144/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6848e-04 - val_loss: 5.2055e-04\n",
      "Epoch 145/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6880e-04 - val_loss: 5.0977e-04\n",
      "Epoch 146/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6676e-04 - val_loss: 5.0949e-04\n",
      "Epoch 147/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6848e-04 - val_loss: 5.1306e-04\n",
      "Epoch 148/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6853e-04 - val_loss: 5.2223e-04\n",
      "Epoch 149/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7029e-04 - val_loss: 5.1405e-04\n",
      "Epoch 150/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6875e-04 - val_loss: 5.0688e-04\n",
      "Epoch 151/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6442e-04 - val_loss: 5.0504e-04\n",
      "Epoch 152/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6658e-04 - val_loss: 5.2264e-04\n",
      "Epoch 153/500\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 3.6822e-04 - val_loss: 5.1374e-04\n",
      "Epoch 154/500\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 3.7552e-04 - val_loss: 5.0860e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7928e-04 - val_loss: 5.3683e-04\n",
      "Epoch 156/500\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 3.8046e-04 - val_loss: 5.1391e-04\n",
      "Epoch 157/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7084e-04 - val_loss: 5.1168e-04\n",
      "Epoch 158/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7328e-04 - val_loss: 5.1398e-04\n",
      "Epoch 159/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6928e-04 - val_loss: 5.0954e-04\n",
      "Epoch 160/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6419e-04 - val_loss: 5.0672e-04\n",
      "Epoch 161/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6915e-04 - val_loss: 5.1758e-04\n",
      "Epoch 162/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7021e-04 - val_loss: 5.1665e-04\n",
      "Epoch 163/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6942e-04 - val_loss: 5.0762e-04\n",
      "Epoch 164/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6791e-04 - val_loss: 5.1292e-04\n",
      "Epoch 165/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6664e-04 - val_loss: 5.0512e-04\n",
      "Epoch 166/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6268e-04 - val_loss: 5.0784e-04\n",
      "Epoch 167/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6487e-04 - val_loss: 5.1749e-04\n",
      "Epoch 168/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6716e-04 - val_loss: 5.1335e-04\n",
      "Epoch 169/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6492e-04 - val_loss: 5.0742e-04\n",
      "Epoch 170/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6212e-04 - val_loss: 5.0511e-04\n",
      "Epoch 171/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6355e-04 - val_loss: 5.0917e-04\n",
      "Epoch 172/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6609e-04 - val_loss: 5.1056e-04\n",
      "Epoch 173/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6565e-04 - val_loss: 5.1187e-04\n",
      "Epoch 174/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6507e-04 - val_loss: 5.0881e-04\n",
      "Epoch 175/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6301e-04 - val_loss: 5.0690e-04\n",
      "Epoch 176/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6822e-04 - val_loss: 5.0760e-04\n",
      "Epoch 177/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6331e-04 - val_loss: 5.0827e-04\n",
      "Epoch 178/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6463e-04 - val_loss: 5.1124e-04\n",
      "Epoch 179/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6491e-04 - val_loss: 5.0945e-04\n",
      "Epoch 180/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6441e-04 - val_loss: 5.0565e-04\n",
      "Epoch 181/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6210e-04 - val_loss: 5.1514e-04\n",
      "Epoch 182/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6595e-04 - val_loss: 5.0753e-04\n",
      "Epoch 183/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6659e-04 - val_loss: 5.1903e-04\n",
      "Epoch 184/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6879e-04 - val_loss: 5.2305e-04\n",
      "Epoch 185/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6932e-04 - val_loss: 5.0961e-04\n",
      "Epoch 186/500\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 3.7360e-04 - val_loss: 5.1935e-04\n",
      "Epoch 187/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7061e-04 - val_loss: 5.2176e-04\n",
      "Epoch 188/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6801e-04 - val_loss: 5.1350e-04\n",
      "Epoch 189/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6771e-04 - val_loss: 5.2923e-04\n",
      "Epoch 190/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6896e-04 - val_loss: 5.1196e-04\n",
      "Epoch 191/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7088e-04 - val_loss: 5.1337e-04\n",
      "Epoch 192/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7744e-04 - val_loss: 5.3322e-04\n",
      "Epoch 193/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7505e-04 - val_loss: 5.0847e-04\n",
      "Epoch 194/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6847e-04 - val_loss: 5.1479e-04\n",
      "Epoch 195/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6889e-04 - val_loss: 5.1259e-04\n",
      "Epoch 196/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6621e-04 - val_loss: 5.0924e-04\n",
      "Epoch 197/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6456e-04 - val_loss: 5.0790e-04\n",
      "Epoch 198/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6348e-04 - val_loss: 5.1502e-04\n",
      "Epoch 199/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6455e-04 - val_loss: 5.1443e-04\n",
      "Epoch 200/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6495e-04 - val_loss: 5.0893e-04\n",
      "Epoch 201/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6435e-04 - val_loss: 5.1001e-04\n",
      "Epoch 202/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6258e-04 - val_loss: 5.0782e-04\n",
      "Epoch 203/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6256e-04 - val_loss: 5.0681e-04\n",
      "Epoch 204/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6254e-04 - val_loss: 5.2124e-04\n",
      "Epoch 205/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6660e-04 - val_loss: 5.0707e-04\n",
      "Epoch 206/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6395e-04 - val_loss: 5.1834e-04\n",
      "Epoch 207/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6722e-04 - val_loss: 5.1163e-04\n",
      "Epoch 208/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6591e-04 - val_loss: 5.0702e-04\n",
      "Epoch 209/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6255e-04 - val_loss: 5.0845e-04\n",
      "Epoch 210/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6304e-04 - val_loss: 5.0741e-04\n",
      "Epoch 211/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6116e-04 - val_loss: 5.1153e-04\n",
      "Epoch 212/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7132e-04 - val_loss: 5.2792e-04\n",
      "Epoch 213/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6839e-04 - val_loss: 5.0561e-04\n",
      "Epoch 214/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6198e-04 - val_loss: 5.0723e-04\n",
      "Epoch 215/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6429e-04 - val_loss: 5.1209e-04\n",
      "Epoch 216/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6270e-04 - val_loss: 5.1230e-04\n",
      "Epoch 217/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6671e-04 - val_loss: 5.1520e-04\n",
      "Epoch 218/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6631e-04 - val_loss: 5.1365e-04\n",
      "Epoch 219/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6482e-04 - val_loss: 5.0649e-04\n",
      "Epoch 220/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6553e-04 - val_loss: 5.1079e-04\n",
      "Epoch 221/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6436e-04 - val_loss: 5.1044e-04\n",
      "Epoch 222/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6470e-04 - val_loss: 5.2616e-04\n",
      "Epoch 223/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6898e-04 - val_loss: 5.0820e-04\n",
      "Epoch 224/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6819e-04 - val_loss: 5.2708e-04\n",
      "Epoch 225/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7538e-04 - val_loss: 5.2489e-04\n",
      "Epoch 226/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7222e-04 - val_loss: 5.0606e-04\n",
      "Epoch 227/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6389e-04 - val_loss: 5.0712e-04\n",
      "Epoch 228/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6225e-04 - val_loss: 5.0768e-04\n",
      "Epoch 229/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6287e-04 - val_loss: 5.0771e-04\n",
      "Epoch 230/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6539e-04 - val_loss: 5.1445e-04\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6418e-04 - val_loss: 5.0931e-04\n",
      "Epoch 232/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6402e-04 - val_loss: 5.1270e-04\n",
      "Epoch 233/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6534e-04 - val_loss: 5.1013e-04\n",
      "Epoch 234/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6558e-04 - val_loss: 5.1738e-04\n",
      "Epoch 235/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6499e-04 - val_loss: 5.2153e-04\n",
      "Epoch 236/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7025e-04 - val_loss: 5.0763e-04\n",
      "Epoch 237/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6784e-04 - val_loss: 5.1730e-04\n",
      "Epoch 238/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6801e-04 - val_loss: 5.4707e-04\n",
      "Epoch 239/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7363e-04 - val_loss: 5.1044e-04\n",
      "Epoch 240/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6381e-04 - val_loss: 5.1001e-04\n",
      "Epoch 241/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6444e-04 - val_loss: 5.1853e-04\n",
      "Epoch 242/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6677e-04 - val_loss: 5.0757e-04\n",
      "Epoch 243/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6973e-04 - val_loss: 5.1776e-04\n",
      "Epoch 244/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6662e-04 - val_loss: 5.1865e-04\n",
      "Epoch 245/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6828e-04 - val_loss: 5.1766e-04\n",
      "Epoch 246/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6722e-04 - val_loss: 5.1035e-04\n",
      "Epoch 247/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6686e-04 - val_loss: 5.1182e-04\n",
      "Epoch 248/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6216e-04 - val_loss: 5.0903e-04\n",
      "Epoch 249/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6212e-04 - val_loss: 5.1550e-04\n",
      "Epoch 250/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6515e-04 - val_loss: 5.1162e-04\n",
      "Epoch 251/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6493e-04 - val_loss: 5.1383e-04\n",
      "Epoch 252/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6490e-04 - val_loss: 5.1731e-04\n",
      "Epoch 253/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6605e-04 - val_loss: 5.0607e-04\n",
      "Epoch 254/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6203e-04 - val_loss: 5.1030e-04\n",
      "Epoch 255/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6448e-04 - val_loss: 5.0595e-04\n",
      "Epoch 256/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6189e-04 - val_loss: 5.0918e-04\n",
      "Epoch 257/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6676e-04 - val_loss: 5.0973e-04\n",
      "Epoch 258/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6158e-04 - val_loss: 5.0735e-04\n",
      "Epoch 259/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6395e-04 - val_loss: 5.0937e-04\n",
      "Epoch 260/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6725e-04 - val_loss: 5.1537e-04\n",
      "Epoch 261/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6683e-04 - val_loss: 5.1543e-04\n",
      "Epoch 262/500\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 3.6445e-04 - val_loss: 5.0797e-04\n",
      "Epoch 263/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6207e-04 - val_loss: 5.0942e-04\n",
      "Epoch 264/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6385e-04 - val_loss: 5.1697e-04\n",
      "Epoch 265/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6323e-04 - val_loss: 5.0970e-04\n",
      "Epoch 266/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6317e-04 - val_loss: 5.1361e-04\n",
      "Epoch 267/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6437e-04 - val_loss: 5.0774e-04\n",
      "Epoch 268/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6095e-04 - val_loss: 5.1014e-04\n",
      "Epoch 269/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6191e-04 - val_loss: 5.0547e-04\n",
      "Epoch 270/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6351e-04 - val_loss: 5.1366e-04\n",
      "Epoch 271/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6267e-04 - val_loss: 5.0791e-04\n",
      "Epoch 272/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6003e-04 - val_loss: 5.0941e-04\n",
      "Epoch 273/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6143e-04 - val_loss: 5.1187e-04\n",
      "Epoch 274/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6226e-04 - val_loss: 5.0770e-04\n",
      "Epoch 275/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6274e-04 - val_loss: 5.0866e-04\n",
      "Epoch 276/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6062e-04 - val_loss: 5.1052e-04\n",
      "Epoch 277/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6029e-04 - val_loss: 5.1122e-04\n",
      "Epoch 278/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6769e-04 - val_loss: 5.0686e-04\n",
      "Epoch 279/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6392e-04 - val_loss: 5.1345e-04\n",
      "Epoch 280/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6251e-04 - val_loss: 5.0901e-04\n",
      "Epoch 281/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6145e-04 - val_loss: 5.0875e-04\n",
      "Epoch 282/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6216e-04 - val_loss: 5.0688e-04\n",
      "Epoch 283/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5981e-04 - val_loss: 5.0897e-04\n",
      "Epoch 284/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6149e-04 - val_loss: 5.1214e-04\n",
      "Epoch 285/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6382e-04 - val_loss: 5.1366e-04\n",
      "Epoch 286/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6376e-04 - val_loss: 5.1436e-04\n",
      "Epoch 287/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6575e-04 - val_loss: 5.1593e-04\n",
      "Epoch 288/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6797e-04 - val_loss: 5.2728e-04\n",
      "Epoch 289/500\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 3.7504e-04 - val_loss: 5.0948e-04\n",
      "Epoch 290/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7003e-04 - val_loss: 5.1378e-04\n",
      "Epoch 291/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6280e-04 - val_loss: 5.0517e-04\n",
      "Epoch 292/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6465e-04 - val_loss: 5.0993e-04\n",
      "Epoch 293/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6688e-04 - val_loss: 5.1372e-04\n",
      "Epoch 294/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6108e-04 - val_loss: 5.2061e-04\n",
      "Epoch 295/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6723e-04 - val_loss: 5.1439e-04\n",
      "Epoch 296/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6507e-04 - val_loss: 5.1080e-04\n",
      "Epoch 297/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6708e-04 - val_loss: 5.2094e-04\n",
      "Epoch 298/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6589e-04 - val_loss: 5.1010e-04\n",
      "Epoch 299/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6430e-04 - val_loss: 5.2564e-04\n",
      "Epoch 300/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7346e-04 - val_loss: 5.1906e-04\n",
      "Epoch 301/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6358e-04 - val_loss: 5.0992e-04\n",
      "Epoch 302/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6513e-04 - val_loss: 5.0740e-04\n",
      "Epoch 303/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6354e-04 - val_loss: 5.1115e-04\n",
      "Epoch 304/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6304e-04 - val_loss: 5.0861e-04\n",
      "Epoch 305/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6164e-04 - val_loss: 5.0782e-04\n",
      "Epoch 306/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5972e-04 - val_loss: 5.0769e-04\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5907e-04 - val_loss: 5.0773e-04\n",
      "Epoch 308/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5918e-04 - val_loss: 5.0751e-04\n",
      "Epoch 309/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5963e-04 - val_loss: 5.0673e-04\n",
      "Epoch 310/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6200e-04 - val_loss: 5.1061e-04\n",
      "Epoch 311/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6449e-04 - val_loss: 5.2209e-04\n",
      "Epoch 312/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6998e-04 - val_loss: 5.0777e-04\n",
      "Epoch 313/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6061e-04 - val_loss: 5.2204e-04\n",
      "Epoch 314/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7078e-04 - val_loss: 5.1078e-04\n",
      "Epoch 315/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6233e-04 - val_loss: 5.1246e-04\n",
      "Epoch 316/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6539e-04 - val_loss: 5.1034e-04\n",
      "Epoch 317/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6237e-04 - val_loss: 5.1247e-04\n",
      "Epoch 318/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6232e-04 - val_loss: 5.1342e-04\n",
      "Epoch 319/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6332e-04 - val_loss: 5.1104e-04\n",
      "Epoch 320/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5798e-04 - val_loss: 5.0805e-04\n",
      "Epoch 321/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5958e-04 - val_loss: 5.0656e-04\n",
      "Epoch 322/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5965e-04 - val_loss: 5.0896e-04\n",
      "Epoch 323/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5902e-04 - val_loss: 5.1821e-04\n",
      "Epoch 324/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6394e-04 - val_loss: 5.2098e-04\n",
      "Epoch 325/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6948e-04 - val_loss: 5.3179e-04\n",
      "Epoch 326/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6888e-04 - val_loss: 5.1233e-04\n",
      "Epoch 327/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6400e-04 - val_loss: 5.1292e-04\n",
      "Epoch 328/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6327e-04 - val_loss: 5.0723e-04\n",
      "Epoch 329/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6652e-04 - val_loss: 5.1209e-04\n",
      "Epoch 330/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6583e-04 - val_loss: 5.1314e-04\n",
      "Epoch 331/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6191e-04 - val_loss: 5.0843e-04\n",
      "Epoch 332/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6229e-04 - val_loss: 5.1503e-04\n",
      "Epoch 333/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6120e-04 - val_loss: 5.1823e-04\n",
      "Epoch 334/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6145e-04 - val_loss: 5.1403e-04\n",
      "Epoch 335/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6338e-04 - val_loss: 5.0765e-04\n",
      "Epoch 336/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6001e-04 - val_loss: 5.1085e-04\n",
      "Epoch 337/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5970e-04 - val_loss: 5.0766e-04\n",
      "Epoch 338/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6202e-04 - val_loss: 5.2174e-04\n",
      "Epoch 339/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6706e-04 - val_loss: 5.0752e-04\n",
      "Epoch 340/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6148e-04 - val_loss: 5.1403e-04\n",
      "Epoch 341/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5952e-04 - val_loss: 5.0736e-04\n",
      "Epoch 342/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5890e-04 - val_loss: 5.0964e-04\n",
      "Epoch 343/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5996e-04 - val_loss: 5.0905e-04\n",
      "Epoch 344/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5906e-04 - val_loss: 5.1038e-04\n",
      "Epoch 345/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6048e-04 - val_loss: 5.1173e-04\n",
      "Epoch 346/500\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 3.5886e-04 - val_loss: 5.1322e-04\n",
      "Epoch 347/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6105e-04 - val_loss: 5.1018e-04\n",
      "Epoch 348/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6623e-04 - val_loss: 5.3391e-04\n",
      "Epoch 349/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7155e-04 - val_loss: 5.2569e-04\n",
      "Epoch 350/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6844e-04 - val_loss: 5.0970e-04\n",
      "Epoch 351/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6272e-04 - val_loss: 5.1908e-04\n",
      "Epoch 352/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6315e-04 - val_loss: 5.0922e-04\n",
      "Epoch 353/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5956e-04 - val_loss: 5.1094e-04\n",
      "Epoch 354/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5941e-04 - val_loss: 5.0861e-04\n",
      "Epoch 355/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5970e-04 - val_loss: 5.0944e-04\n",
      "Epoch 356/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6163e-04 - val_loss: 5.1699e-04\n",
      "Epoch 357/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6068e-04 - val_loss: 5.1177e-04\n",
      "Epoch 358/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5982e-04 - val_loss: 5.0877e-04\n",
      "Epoch 359/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6468e-04 - val_loss: 5.1277e-04\n",
      "Epoch 360/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6393e-04 - val_loss: 5.1213e-04\n",
      "Epoch 361/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5864e-04 - val_loss: 5.0911e-04\n",
      "Epoch 362/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6021e-04 - val_loss: 5.1040e-04\n",
      "Epoch 363/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6102e-04 - val_loss: 5.1009e-04\n",
      "Epoch 364/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6163e-04 - val_loss: 5.0951e-04\n",
      "Epoch 365/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5964e-04 - val_loss: 5.1217e-04\n",
      "Epoch 366/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6054e-04 - val_loss: 5.0914e-04\n",
      "Epoch 367/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5980e-04 - val_loss: 5.2169e-04\n",
      "Epoch 368/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6013e-04 - val_loss: 5.0824e-04\n",
      "Epoch 369/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5929e-04 - val_loss: 5.1071e-04\n",
      "Epoch 370/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6016e-04 - val_loss: 5.1981e-04\n",
      "Epoch 371/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6456e-04 - val_loss: 5.1034e-04\n",
      "Epoch 372/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6101e-04 - val_loss: 5.1654e-04\n",
      "Epoch 373/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6220e-04 - val_loss: 5.1020e-04\n",
      "Epoch 374/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6037e-04 - val_loss: 5.0903e-04\n",
      "Epoch 375/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5951e-04 - val_loss: 5.1007e-04\n",
      "Epoch 376/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5995e-04 - val_loss: 5.0819e-04\n",
      "Epoch 377/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6455e-04 - val_loss: 5.1333e-04\n",
      "Epoch 378/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6511e-04 - val_loss: 5.1251e-04\n",
      "Epoch 379/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6021e-04 - val_loss: 5.1170e-04\n",
      "Epoch 380/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6125e-04 - val_loss: 5.2350e-04\n",
      "Epoch 381/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6437e-04 - val_loss: 5.1450e-04\n",
      "Epoch 382/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5882e-04 - val_loss: 5.1347e-04\n",
      "Epoch 383/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6702e-04 - val_loss: 5.1389e-04\n",
      "Epoch 384/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7135e-04 - val_loss: 5.0932e-04\n",
      "Epoch 385/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6285e-04 - val_loss: 5.1078e-04\n",
      "Epoch 386/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6247e-04 - val_loss: 5.1665e-04\n",
      "Epoch 387/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6147e-04 - val_loss: 5.1757e-04\n",
      "Epoch 388/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6002e-04 - val_loss: 5.0891e-04\n",
      "Epoch 389/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5842e-04 - val_loss: 5.0926e-04\n",
      "Epoch 390/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6537e-04 - val_loss: 5.0812e-04\n",
      "Epoch 391/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5924e-04 - val_loss: 5.1253e-04\n",
      "Epoch 392/500\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 3.6028e-04 - val_loss: 5.1652e-04\n",
      "Epoch 393/500\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 3.5890e-04 - val_loss: 5.0877e-04\n",
      "Epoch 394/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5965e-04 - val_loss: 5.1061e-04\n",
      "Epoch 395/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5999e-04 - val_loss: 5.1272e-04\n",
      "Epoch 396/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5824e-04 - val_loss: 5.0760e-04\n",
      "Epoch 397/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5874e-04 - val_loss: 5.0885e-04\n",
      "Epoch 398/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5712e-04 - val_loss: 5.0833e-04\n",
      "Epoch 399/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5718e-04 - val_loss: 5.1253e-04\n",
      "Epoch 400/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5866e-04 - val_loss: 5.2851e-04\n",
      "Epoch 401/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6607e-04 - val_loss: 5.1083e-04\n",
      "Epoch 402/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5747e-04 - val_loss: 5.0966e-04\n",
      "Epoch 403/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5743e-04 - val_loss: 5.0897e-04\n",
      "Epoch 404/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5789e-04 - val_loss: 5.1185e-04\n",
      "Epoch 405/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6005e-04 - val_loss: 5.0793e-04\n",
      "Epoch 406/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5944e-04 - val_loss: 5.0791e-04\n",
      "Epoch 407/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5792e-04 - val_loss: 5.2277e-04\n",
      "Epoch 408/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6502e-04 - val_loss: 5.0894e-04\n",
      "Epoch 409/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6403e-04 - val_loss: 5.1135e-04\n",
      "Epoch 410/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6254e-04 - val_loss: 5.2337e-04\n",
      "Epoch 411/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6613e-04 - val_loss: 5.0526e-04\n",
      "Epoch 412/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6111e-04 - val_loss: 5.0757e-04\n",
      "Epoch 413/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6265e-04 - val_loss: 5.1533e-04\n",
      "Epoch 414/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6426e-04 - val_loss: 5.0880e-04\n",
      "Epoch 415/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6082e-04 - val_loss: 5.1157e-04\n",
      "Epoch 416/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6084e-04 - val_loss: 5.1124e-04\n",
      "Epoch 417/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5863e-04 - val_loss: 5.0863e-04\n",
      "Epoch 418/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5743e-04 - val_loss: 5.0736e-04\n",
      "Epoch 419/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6014e-04 - val_loss: 5.2365e-04\n",
      "Epoch 420/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6198e-04 - val_loss: 5.1753e-04\n",
      "Epoch 421/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6005e-04 - val_loss: 5.0589e-04\n",
      "Epoch 422/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5913e-04 - val_loss: 5.1278e-04\n",
      "Epoch 423/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6108e-04 - val_loss: 5.1868e-04\n",
      "Epoch 424/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6215e-04 - val_loss: 5.1052e-04\n",
      "Epoch 425/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5880e-04 - val_loss: 5.0918e-04\n",
      "Epoch 426/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5924e-04 - val_loss: 5.1124e-04\n",
      "Epoch 427/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5872e-04 - val_loss: 5.0921e-04\n",
      "Epoch 428/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5679e-04 - val_loss: 5.1075e-04\n",
      "Epoch 429/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6122e-04 - val_loss: 5.1048e-04\n",
      "Epoch 430/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5912e-04 - val_loss: 5.3077e-04\n",
      "Epoch 431/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6817e-04 - val_loss: 5.1243e-04\n",
      "Epoch 432/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7052e-04 - val_loss: 5.2409e-04\n",
      "Epoch 433/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6582e-04 - val_loss: 5.2333e-04\n",
      "Epoch 434/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6348e-04 - val_loss: 5.2375e-04\n",
      "Epoch 435/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6253e-04 - val_loss: 5.1621e-04\n",
      "Epoch 436/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6217e-04 - val_loss: 5.1013e-04\n",
      "Epoch 437/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5822e-04 - val_loss: 5.1075e-04\n",
      "Epoch 438/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5669e-04 - val_loss: 5.1544e-04\n",
      "Epoch 439/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5800e-04 - val_loss: 5.0749e-04\n",
      "Epoch 440/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5649e-04 - val_loss: 5.1446e-04\n",
      "Epoch 441/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6067e-04 - val_loss: 5.1259e-04\n",
      "Epoch 442/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5862e-04 - val_loss: 5.0773e-04\n",
      "Epoch 443/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6487e-04 - val_loss: 5.1438e-04\n",
      "Epoch 444/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5912e-04 - val_loss: 5.0804e-04\n",
      "Epoch 445/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5584e-04 - val_loss: 5.0594e-04\n",
      "Epoch 446/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5554e-04 - val_loss: 5.0953e-04\n",
      "Epoch 447/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6116e-04 - val_loss: 5.1940e-04\n",
      "Epoch 448/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6344e-04 - val_loss: 5.1097e-04\n",
      "Epoch 449/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6589e-04 - val_loss: 5.3332e-04\n",
      "Epoch 450/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6116e-04 - val_loss: 5.0927e-04\n",
      "Epoch 451/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5911e-04 - val_loss: 5.1269e-04\n",
      "Epoch 452/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5972e-04 - val_loss: 5.1110e-04\n",
      "Epoch 453/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6425e-04 - val_loss: 5.2706e-04\n",
      "Epoch 454/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7469e-04 - val_loss: 5.1101e-04\n",
      "Epoch 455/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7698e-04 - val_loss: 5.4362e-04\n",
      "Epoch 456/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.7676e-04 - val_loss: 5.1705e-04\n",
      "Epoch 457/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6423e-04 - val_loss: 5.1031e-04\n",
      "Epoch 458/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6080e-04 - val_loss: 5.1310e-04\n",
      "Epoch 459/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5634e-04 - val_loss: 5.0884e-04\n",
      "Epoch 460/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5865e-04 - val_loss: 5.0889e-04\n",
      "Epoch 461/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5608e-04 - val_loss: 5.1388e-04\n",
      "Epoch 462/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5957e-04 - val_loss: 5.0824e-04\n",
      "Epoch 463/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5815e-04 - val_loss: 5.1215e-04\n",
      "Epoch 464/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5921e-04 - val_loss: 5.0963e-04\n",
      "Epoch 465/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5569e-04 - val_loss: 5.0867e-04\n",
      "Epoch 466/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5604e-04 - val_loss: 5.1026e-04\n",
      "Epoch 467/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5799e-04 - val_loss: 5.0877e-04\n",
      "Epoch 468/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5599e-04 - val_loss: 5.1150e-04\n",
      "Epoch 469/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5455e-04 - val_loss: 5.1429e-04\n",
      "Epoch 470/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6097e-04 - val_loss: 5.1448e-04\n",
      "Epoch 471/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5776e-04 - val_loss: 5.1003e-04\n",
      "Epoch 472/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5419e-04 - val_loss: 5.1016e-04\n",
      "Epoch 473/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5745e-04 - val_loss: 5.0890e-04\n",
      "Epoch 474/500\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 3.5621e-04 - val_loss: 5.1325e-04\n",
      "Epoch 475/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5737e-04 - val_loss: 5.1246e-04\n",
      "Epoch 476/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5968e-04 - val_loss: 5.1591e-04\n",
      "Epoch 477/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6093e-04 - val_loss: 5.1534e-04\n",
      "Epoch 478/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6456e-04 - val_loss: 5.1456e-04\n",
      "Epoch 479/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5869e-04 - val_loss: 5.1039e-04\n",
      "Epoch 480/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5779e-04 - val_loss: 5.0831e-04\n",
      "Epoch 481/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5729e-04 - val_loss: 5.0994e-04\n",
      "Epoch 482/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6211e-04 - val_loss: 5.1829e-04\n",
      "Epoch 483/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5875e-04 - val_loss: 5.1109e-04\n",
      "Epoch 484/500\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 3.5671e-04 - val_loss: 5.1154e-04\n",
      "Epoch 485/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5573e-04 - val_loss: 5.0904e-04\n",
      "Epoch 486/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5906e-04 - val_loss: 5.1597e-04\n",
      "Epoch 487/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5889e-04 - val_loss: 5.0765e-04\n",
      "Epoch 488/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5719e-04 - val_loss: 5.1400e-04\n",
      "Epoch 489/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5855e-04 - val_loss: 5.1553e-04\n",
      "Epoch 490/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5689e-04 - val_loss: 5.1598e-04\n",
      "Epoch 491/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5702e-04 - val_loss: 5.1307e-04\n",
      "Epoch 492/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5598e-04 - val_loss: 5.0850e-04\n",
      "Epoch 493/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5738e-04 - val_loss: 5.0853e-04\n",
      "Epoch 494/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5569e-04 - val_loss: 5.1203e-04\n",
      "Epoch 495/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6071e-04 - val_loss: 5.1118e-04\n",
      "Epoch 496/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6046e-04 - val_loss: 5.1597e-04\n",
      "Epoch 497/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6029e-04 - val_loss: 5.0693e-04\n",
      "Epoch 498/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6215e-04 - val_loss: 5.1198e-04\n",
      "Epoch 499/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.6083e-04 - val_loss: 5.0893e-04\n",
      "Epoch 500/500\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 3.5671e-04 - val_loss: 5.1253e-04\n"
     ]
    }
   ],
   "source": [
    "#Train Learning Model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00005), loss='mse') #0.0001\n",
    "\n",
    "#history = model.fit(x_train, y_train, epochs = 50000, validation_split=0.0, batch_size = x_train.shape[0])\n",
    "#history = model.fit(x_train, y_train, epochs = 3000, validation_split=0.0, batch_size = 1280) #1280\n",
    "#Batch size = 1280 for remove outlier, 2560 for keep outlier\n",
    "history = model.fit(x = x_train, y = y_train, epochs =500, batch_size = 2560, validation_data = (x_valid, y_valid),shuffle=True) #1280, 1000 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005083055002614856\n"
     ]
    }
   ],
   "source": [
    "#Plot Training Progree\n",
    "plt.plot(history.history['loss'], 'r', label='loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(history.history['val_loss'], 'b', label='val_loss') if 'val_loss' in history.history else None\n",
    "plt.legend()\n",
    "plt.axhline(y=0.00049, xmin=0, xmax=5, linewidth=2, color = 'k')\n",
    "plt.show()\n",
    "print(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jiayu/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/jiayu/Desktop/MLP_DataSet/Rubbles_TimeTrack//ML_Models/NN_Model_Aug_1Time/assets\n"
     ]
    }
   ],
   "source": [
    "#Save Trained Model\n",
    "#MLmodel_name = \"NN_Model_Valid_\" + trainingset[\"PreProcessMode\"] + \"_Dagger_InitSet_2Iter\"\n",
    "#MLmodel_name = \"NN_Model\" + \"_\" + \"AugVarStep_1to2StepbeforeFail_3Time_RemovebyClip_SmallThre\"\n",
    "MLmodel_name = \"NN_Model\" + \"_\" + \"Aug_1Time\"\n",
    "model.save(ML_Model_Path + MLmodel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DataSet Setttings\n",
    "datasetSettings = {\"Shift_World_Frame_Type\":trainingset[\"Shift_World_Frame_Type\"],\n",
    "                   \"VectorScaleFactor\":trainingset[\"VectorScaleFactor\"],\n",
    "                   \"NumPreviewSteps\":trainingset[\"NumPreviewSteps\"],\n",
    "                   \"Contact_Representation_Type\":trainingset[\"Contact_Representation_Type\"],\n",
    "                   \"TrainingLoss\":history.history['loss']}\n",
    "#Validation loss\n",
    "datasetSettings[\"ValidationLoss\"] = history.history['val_loss'] if 'val_loss' in history.history else None\n",
    "\n",
    "#ProProcess\n",
    "datasetSettings[\"PreProcessMode\"] = trainingset[\"PreProcessMode\"]\n",
    "datasetSettings[\"Scaler_X\"] = trainingset[\"Scaler_X\"]\n",
    "\n",
    "datasetSettings[\"Scaler_Y\"] = trainingset[\"Scaler_Y\"]\n",
    "\n",
    "#Dump File\n",
    "pickle.dump(datasetSettings, open(ML_Model_Path + MLmodel_name+ '/datasetSettings' +'.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.57735119e-01 -1.32431439e-01  7.72655379e-01  1.54205458e-01\n",
      "  1.45736733e-01  1.94858307e-02  5.27221104e-06  9.07316125e-07\n",
      " -3.21963525e-05 -3.04132597e-01 -2.39602974e-01 -6.31444340e-03\n",
      "  0.00000000e+00  4.30208398e-01  8.88343776e-01 -2.95208302e-13\n",
      " -1.54664427e-01  8.88343776e-01 -2.95208302e-13 -1.54664427e-01\n",
      " -1.11656224e-01 -2.95208302e-13  4.30208398e-01 -1.11656224e-01\n",
      " -2.95208302e-13 -1.63228795e-01 -1.11656224e-01  4.60190034e-02\n",
      " -7.28522625e-01 -1.11656224e-01 -5.91850123e-02 -6.91814896e-01\n",
      " -1.11165622e+00 -2.56427035e-01 -1.26521066e-01 -1.11165622e+00\n",
      " -1.51223019e-01  4.11768022e-01 -1.11656224e-01  9.90858626e-02\n",
      " -1.36224052e-01 -1.11656224e-01 -9.90858626e-02 -1.36224052e-01\n",
      " -1.11165622e+00 -9.90858626e-02  4.11768022e-01 -1.11165622e+00\n",
      "  9.90858626e-02  9.67834214e-01  8.88343776e-01  2.53872845e-01\n",
      "  4.02540384e-01  8.88343776e-01  1.48668830e-01  4.38297418e-01\n",
      " -1.11656224e-01 -4.34648140e-02  1.00359125e+00 -1.11656224e-01\n",
      "  6.17392016e-02  9.67834214e-01 -1.11656224e-01  2.53872845e-01\n",
      "  4.02540384e-01 -1.11656224e-01  1.48668830e-01  4.38297418e-01\n",
      " -1.11165622e+00 -4.34648140e-02  1.00359125e+00 -1.11165622e+00\n",
      "  6.17392016e-02  1.54407601e+00  8.88343776e-01  3.00250054e-01\n",
      "  9.92643285e-01  8.88343776e-01  1.20566008e-01  9.92643285e-01\n",
      " -1.11656224e-01  1.20566008e-01  1.54407601e+00 -1.11656224e-01\n",
      "  3.00250054e-01]\n",
      "Data Kept Original Form, But need to scale back to meters\n",
      "predicted result: \n",
      " [[ 2.85575502e-02 -1.09761514e-01  7.21382022e-01  1.49514109e-01\n",
      "  -1.29188657e-01 -3.35918367e-02 -2.07732851e-03  8.67516326e-04\n",
      "  -2.05367338e-03  7.89466858e-01  9.01251733e-01  5.02771378e-01\n",
      "   1.18289089e+00  4.94804680e-01]]\n",
      "true value: \n",
      " [ 3.30138376e-02 -1.45687536e-01  7.43861234e-01  1.64111183e-01\n",
      " -1.80800815e-01 -3.01646155e-02 -2.15584593e-05  1.88402972e-05\n",
      " -4.24620590e-06  8.11229953e-01  8.41146960e-01  4.99999919e-01\n",
      "  1.19999905e+00  4.99999933e-01]\n",
      "diff: \n",
      " [[0.00445629 0.03592602 0.02247921 0.01459707 0.05161216 0.00342722\n",
      "  0.00205577 0.00084868 0.00204943 0.02176309 0.06010477 0.00277146\n",
      "  0.01710816 0.00519525]]\n",
      "Neural Network Time:  0.09671378135681152\n"
     ]
    }
   ],
   "source": [
    "#Show Prediction Result for Training\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "datapoint_num = 105\n",
    "\n",
    "x_test = np.array([x_train[datapoint_num]])\n",
    "\n",
    "start = time.time()\n",
    "y_pred_temp = model.predict(x_test)\n",
    "end = time.time()\n",
    "\n",
    "print(x_train[datapoint_num])\n",
    "\n",
    "#Recover to original format\n",
    "if trainingset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_originalform = y_pred_temp/trainingset[\"VectorScaleFactor\"]\n",
    "    y_true_originalform = y_train[datapoint_num]/trainingset[\"VectorScaleFactor\"]\n",
    "elif trainingset[\"PreProcessMode\"] == \"Standarization\" or trainingset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    y_pred_originalform = dataset[\"Scaler_Y\"].inverse_transform(y_pred_temp)\n",
    "    y_true_originalform = dataset[\"Scaler_Y\"].inverse_transform(np.array([y_train[datapoint_num]]))\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "\n",
    "print(\"predicted result: \\n\",y_pred_originalform)\n",
    "print(\"true value: \\n\",y_true_originalform)\n",
    "print(\"diff: \\n\", np.absolute(y_pred_originalform - y_true_originalform))\n",
    "\n",
    "\n",
    "print(\"Neural Network Time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.11825266 0.11826868 0.11836202 0.11838948 0.11850782 0.1185395\n",
      " 0.11863828 0.11870088 0.11877728 0.11884707 0.11889283 0.11899191\n",
      " 0.11906562 0.11911687 0.11915405 0.11916729 0.11917685 0.11923776\n",
      " 0.11927909 0.11928111 0.11928465 0.11931716 0.11935407 0.11936446\n",
      " 0.11945982 0.11946369 0.11948838 0.11954073 0.119545   0.11958719\n",
      " 0.11962969 0.11992856 0.11993253 0.1199427  0.1199954  0.12011347\n",
      " 0.12013547 0.12014177 0.12014191 0.12015071 0.12017513 0.12028507\n",
      " 0.12034187 0.1203751  0.120401   0.12041368 0.12046182 0.12049302\n",
      " 0.12055    0.12059079 0.12079847 0.12086058 0.12089985 0.12094128\n",
      " 0.12096764 0.12103329 0.12106263 0.12110893 0.1211318  0.12118288\n",
      " 0.12119687 0.12128399 0.1213223  0.12132901 0.12134058 0.12134582\n",
      " 0.12158157 0.12163898 0.12163997 0.12171688 0.12173013 0.12179118\n",
      " 0.12181813 0.12182113 0.12204731 0.12206968 0.1220828  0.12244637\n",
      " 0.12246588 0.12258644 0.1226016  0.12261158 0.1226371  0.12271067\n",
      " 0.12271456 0.12287774 0.12289336 0.12291687 0.12293733 0.12298676\n",
      " 0.12300184 0.1231396  0.12319717 0.12319957 0.12328176 0.12344801\n",
      " 0.12345339 0.12347286 0.12352242 0.12362885 0.1237096  0.12372515\n",
      " 0.12375861 0.12382668 0.12385008 0.1239547  0.12400749 0.12406677\n",
      " 0.12408517 0.12413546 0.12435921 0.12438075 0.12443058 0.12450886\n",
      " 0.12451406 0.12459202 0.12488663 0.12490009 0.12493826 0.12499725\n",
      " 0.12505347 0.12521228 0.12530811 0.12537716 0.12551928 0.12554989\n",
      " 0.12557308 0.12568714 0.12588958 0.12590363 0.1259856  0.12601942\n",
      " 0.12615068 0.12638475 0.12641834 0.12643631 0.12655554 0.1265821\n",
      " 0.12660069 0.12664125 0.12669099 0.12675428 0.12678101 0.1268031\n",
      " 0.12683287 0.1268926  0.12689672 0.12692573 0.12694367 0.12695861\n",
      " 0.12696066 0.1270204  0.12724482 0.12740125 0.1274805  0.12750918\n",
      " 0.12763433 0.12765709 0.1276792  0.12794359 0.12796263 0.12815485\n",
      " 0.12816003 0.12838711 0.1284859  0.12852188 0.12860745 0.12863769\n",
      " 0.12868064 0.12874605 0.12877054 0.1288712  0.12895033 0.1290898\n",
      " 0.12918972 0.12925196 0.1294748  0.12949493 0.12956711 0.12957282\n",
      " 0.12968965 0.12980934 0.13006042 0.13008432 0.13015644 0.13035639\n",
      " 0.13038627 0.1304302  0.13048545 0.13049368 0.1305195  0.13059706\n",
      " 0.13065816 0.13065842 0.13066808 0.13074927 0.13076123 0.13087051\n",
      " 0.13116623 0.13144576 0.1315749  0.13173799 0.1317447  0.13193085\n",
      " 0.13195361 0.13203492 0.13207245 0.13214003 0.13215065 0.13224903\n",
      " 0.13228826 0.13237394 0.13256809 0.13258596 0.13264097 0.13269559\n",
      " 0.13273793 0.13273882 0.1328448  0.13293849 0.13302844 0.13303366\n",
      " 0.13306646 0.13314934 0.13328063 0.13337718 0.13345448 0.13351992\n",
      " 0.13360029 0.13362574 0.13363727 0.13364665 0.13376153 0.13376337\n",
      " 0.13388449 0.13391474 0.13417731 0.13432461 0.13432894 0.13433495\n",
      " 0.13445424 0.13453019 0.13468823 0.13488268 0.13510057 0.13526713\n",
      " 0.13530507 0.1353727  0.13545187 0.13546603 0.13546811 0.13549778\n",
      " 0.13559406 0.13560182 0.13586111 0.1358688  0.13589934 0.13599134\n",
      " 0.13609103 0.13609454 0.13618053 0.13618489 0.13628484 0.13628922\n",
      " 0.13636299 0.13639145 0.13640248 0.13643773 0.13660974 0.13663861\n",
      " 0.13670979 0.13671549 0.13676374 0.13681162 0.13692288 0.13719013\n",
      " 0.13730927 0.13731773 0.13745317 0.13746118 0.13746588 0.13751491\n",
      " 0.13757664 0.13777997 0.13778471 0.13778877 0.13784006 0.13791528\n",
      " 0.13801663 0.13802426 0.13808047 0.13810452 0.13822092 0.13835967\n",
      " 0.1384593  0.13847907 0.13853785 0.13854276 0.13854362 0.13855342\n",
      " 0.13858114 0.1385957  0.13865027 0.13889113 0.13892963 0.13896208\n",
      " 0.13913098 0.13919118 0.13925208 0.1393566  0.13958882 0.13970366\n",
      " 0.13977219 0.13977735 0.13978297 0.14010793 0.14030984 0.14038067\n",
      " 0.14061475 0.14063902 0.14067279 0.14080754 0.14087216 0.14115316\n",
      " 0.14127858 0.14133473 0.14170476 0.14175105 0.14175402 0.14177559\n",
      " 0.14179778 0.14193683 0.14194552 0.1421784  0.14221674 0.14254079\n",
      " 0.14272433 0.14290822 0.14298456 0.14300427 0.14321625 0.14322861\n",
      " 0.14347223 0.14369793 0.14372236 0.14373151 0.14374058 0.14378506\n",
      " 0.14410047 0.14427506 0.14435078 0.14437194 0.14449031 0.14449268\n",
      " 0.14459561 0.144717   0.14503023 0.14508223 0.14526072 0.14534238\n",
      " 0.14536232 0.14538719 0.14552707 0.14603758 0.14618299 0.14632951\n",
      " 0.14658182 0.14684289 0.14689649 0.14694975 0.14697537 0.14707724\n",
      " 0.14708108 0.14710148 0.14713402 0.14753792 0.14754293 0.147579\n",
      " 0.14762917 0.14772536 0.14823457 0.14823986 0.14835297 0.14841703\n",
      " 0.14846361 0.14863425 0.14878419 0.14892466 0.14907104 0.14911187\n",
      " 0.14940692 0.14944388 0.14948776 0.14963965 0.14967256 0.14991572\n",
      " 0.15003425 0.15003892 0.15026163 0.15047364 0.15057645 0.15064684\n",
      " 0.15064962 0.15067588 0.15073768 0.15074558 0.15104358 0.15105967\n",
      " 0.15106433 0.15107293 0.15143148 0.15145506 0.15152605 0.15163369\n",
      " 0.15187942 0.15191583 0.15230975 0.15239391 0.15252329 0.1526283\n",
      " 0.15266833 0.15288093 0.15292034 0.15309763 0.15318335 0.15327134\n",
      " 0.15327669 0.15333855 0.1533921  0.15342587 0.15349511 0.15354772\n",
      " 0.1535493  0.15384412 0.15409606 0.15416312 0.1541645  0.15420669\n",
      " 0.15423663 0.15444386 0.15473155 0.15476322 0.15482897 0.15488993\n",
      " 0.1549913  0.15499362 0.15515794 0.15532458 0.15533934 0.1555614\n",
      " 0.15566211 0.15572366 0.15601687 0.15608019 0.15614389 0.15620902\n",
      " 0.15621916 0.15624585 0.15633146 0.15637915 0.15667747 0.15671268\n",
      " 0.1569934  0.15703733 0.15705674 0.1570838  0.1573103  0.15732272\n",
      " 0.15764778 0.15778235 0.15804569 0.15818169 0.15824488 0.15825273\n",
      " 0.15847607 0.15862178 0.15873138 0.15889936 0.15893169 0.15897523\n",
      " 0.15910938 0.1591974  0.15931052 0.15932231 0.15940832 0.15962716\n",
      " 0.15970832 0.15985249 0.15989299 0.16014788 0.16016627 0.16033837\n",
      " 0.16044109 0.1605611  0.16090613 0.16101417 0.16110503 0.16144229\n",
      " 0.16208994 0.16226825 0.16241361 0.16256496 0.16267806 0.16267941\n",
      " 0.16268191 0.1629149  0.1630436  0.16310293 0.16325526 0.16328627\n",
      " 0.16331531 0.16347047 0.16384586 0.16441621 0.16443993 0.16480422\n",
      " 0.1657228  0.1658991  0.16604255 0.16617178 0.16618329 0.16633219\n",
      " 0.1664975  0.16654718 0.16657602 0.16673836 0.16680883 0.16724878\n",
      " 0.16768989 0.16774682 0.16775929 0.16777741 0.16792834 0.16802128\n",
      " 0.16805366 0.16846329 0.16864374 0.16875542 0.16926028 0.16945059\n",
      " 0.16977024 0.16989472 0.16995035 0.16996129 0.17014316 0.17024613\n",
      " 0.17035476 0.17037532 0.17163008 0.17214069 0.17220459 0.17224276\n",
      " 0.17237406 0.17238424 0.17258232 0.17274079 0.17275894 0.17289528\n",
      " 0.17290126 0.17299266 0.17315129 0.17334803 0.17349289 0.17350446\n",
      " 0.17355357 0.17357416 0.17371612 0.17407596 0.17410881 0.17418597\n",
      " 0.17442594 0.17452863 0.17454309 0.17489961 0.17503881 0.17515199\n",
      " 0.17515508 0.17555904 0.17571864 0.17608059 0.17615819 0.17617492\n",
      " 0.1762796  0.17650425 0.17665516 0.17666808 0.17686186 0.17688033\n",
      " 0.17688877 0.17729204 0.17742554 0.17742935 0.17769923 0.17774912\n",
      " 0.17802575 0.17804905 0.17828423 0.17884366 0.17914093 0.17915476\n",
      " 0.17925046 0.17984104 0.18007901 0.18027346 0.18029748 0.18056709\n",
      " 0.18087217 0.18121695 0.1815457  0.18162149 0.18173597 0.18182878\n",
      " 0.18202379 0.18245899 0.18254939 0.1826268  0.18271014 0.18285095\n",
      " 0.18317223 0.18349528 0.18364601 0.18369587 0.18402745 0.18419774\n",
      " 0.18421257 0.18426177 0.1844015  0.18444251 0.18466316 0.18477876\n",
      " 0.18489228 0.18491341 0.18507517 0.18514635 0.18517215 0.18528182\n",
      " 0.18582402 0.18602539 0.18609986 0.18613844 0.18640688 0.18654737\n",
      " 0.1866954  0.1868543  0.18710352 0.18727481 0.18742589 0.1874502\n",
      " 0.18748769 0.18819306 0.1883595  0.18842416 0.1884775  0.18854999\n",
      " 0.18871894 0.18877332 0.18912867 0.18938784 0.18944507 0.18962186\n",
      " 0.18991532 0.19029096 0.19049532 0.19079259 0.19093413 0.19099623\n",
      " 0.19106432 0.19156743 0.19163701 0.19168252 0.19174904 0.19176979\n",
      " 0.19187305 0.19205663 0.19229142 0.1924213  0.19261052 0.19319718\n",
      " 0.19357465 0.19432186 0.194507   0.19468407 0.19477091 0.19486286\n",
      " 0.19537386 0.19552478 0.195578   0.19569373 0.19576148 0.19579982\n",
      " 0.19586592 0.19600922 0.1961099  0.19637153 0.1968729  0.1971473\n",
      " 0.19719648 0.19741963 0.19762363 0.19860173 0.19867185 0.19934159\n",
      " 0.19993909 0.20000809 0.20033921 0.20043551 0.20103856 0.20191675\n",
      " 0.2025992  0.20281894 0.20426141 0.20449263 0.20516423 0.20524531\n",
      " 0.20604596 0.20619466 0.20695443 0.20725714 0.20732459 0.20733359\n",
      " 0.20743706 0.20753276 0.20755128 0.20759339 0.20773124 0.20817124\n",
      " 0.20832906 0.20882357 0.20902705 0.20911756 0.20913912 0.20929577\n",
      " 0.20938038 0.20965087 0.20984895 0.20990719 0.21072452 0.21141811\n",
      " 0.21160512 0.21234533 0.21340315 0.2138696  0.21400526 0.21428653\n",
      " 0.21468066 0.21503015 0.21555395 0.21569259 0.21618594 0.21678179\n",
      " 0.21687153 0.2172082  0.21721183 0.21839747 0.21845706 0.21858116\n",
      " 0.2186901  0.21942913 0.21946317 0.21960311 0.22025417 0.22084779\n",
      " 0.2210188  0.22132339 0.22170936 0.22274572 0.22288626 0.22344087\n",
      " 0.22459793 0.22555541 0.22573421 0.22583188 0.2272775  0.2276332\n",
      " 0.22786619 0.22789023 0.22880506 0.22885379 0.22953829 0.230268\n",
      " 0.2305298  0.23068251 0.23107426 0.23113221 0.23123691 0.23130053\n",
      " 0.23164557 0.23207748 0.23269948 0.23312849 0.23506894 0.23596002\n",
      " 0.23630262 0.23692065 0.23803545 0.23836356 0.23913731 0.23914338\n",
      " 0.23974564 0.239812   0.2407858  0.24088012 0.24199179 0.24269466\n",
      " 0.24290901 0.24310035 0.24356784 0.24399349 0.2440032  0.24403856\n",
      " 0.24454804 0.24484405 0.24564504 0.24564777 0.24712423 0.24801513\n",
      " 0.24908092 0.2495797  0.25012713 0.25035757 0.25058042 0.25174317\n",
      " 0.25196097 0.25368009 0.25371976 0.25453445 0.25487785 0.25522394\n",
      " 0.25569604 0.25593134 0.25664117 0.25670889 0.25678641 0.25954305\n",
      " 0.25984996 0.26027334 0.26045004 0.26048397 0.2613774  0.26233995\n",
      " 0.2632944  0.26345275 0.26427336 0.26563181 0.26611331 0.26656082\n",
      " 0.26694149 0.26757859 0.26782138 0.26791691 0.26885262 0.26998023\n",
      " 0.27086881 0.27102054 0.27309755 0.27410645 0.27458526 0.27519017\n",
      " 0.27524964 0.27583937 0.2759207  0.27800986 0.27826092 0.2787835\n",
      " 0.27915364 0.28176858 0.28288163 0.28586416 0.28677922 0.28691244\n",
      " 0.2873603  0.28893733 0.28908382 0.29227288 0.29324697 0.29477012\n",
      " 0.2956036  0.29654293 0.29925678 0.30121709 0.30200341 0.30220303\n",
      " 0.30376131 0.30403468 0.30575352 0.30667378 0.30784862 0.30786988\n",
      " 0.31078947 0.31224764 0.3126217  0.31303729 0.31332795 0.31368846\n",
      " 0.3141345  0.31563602 0.31610345 0.31680153 0.3206551  0.32423238\n",
      " 0.32430708 0.32475291 0.32684308 0.32770259 0.32811259 0.32955984\n",
      " 0.33086446 0.33197647 0.33325611 0.33433905 0.33440381 0.33834257\n",
      " 0.33839648 0.34018454 0.34191017 0.34281805 0.3437082  0.34393037\n",
      " 0.34532667 0.34746238 0.34844295 0.34927417 0.35084206 0.35098953\n",
      " 0.35102307 0.35121348 0.35333225 0.35543698 0.35586115 0.35608559\n",
      " 0.35701556 0.35815056 0.35868657 0.36479037 0.36576366 0.36578295\n",
      " 0.3661334  0.36617603 0.36631921 0.36667525 0.37068366 0.37242308\n",
      " 0.37297535 0.37313277 0.37388831 0.37522747 0.37674673 0.37685886\n",
      " 0.37947375 0.37982794 0.38081947 0.38258708 0.38955168 0.38964366\n",
      " 0.39069166 0.39326957 0.39339921 0.39409712 0.39821292 0.4038715\n",
      " 0.40712252 0.41048455 0.4111667  0.41180958 0.41456534 0.41557872\n",
      " 0.41856828 0.41983299 0.4221569  0.4231555  0.42355962 0.42401375\n",
      " 0.42540416 0.42563614 0.426058   0.42669286 0.42761742 0.42820233\n",
      " 0.43033774 0.43284223 0.4329494  0.43774421 0.43834038 0.43941509\n",
      " 0.43973886 0.44001336 0.44286813 0.44291934 0.44311039 0.44420218\n",
      " 0.44581296 0.44791906 0.44841986 0.44859928 0.45035286 0.46050026\n",
      " 0.46155183 0.4663415  0.467431   0.64900504]\n",
      "Error Mean:  0.029912377276403255\n",
      "Error Std 0.04529884361936515\n",
      "[0.2272775  0.2276332  0.22789023 0.22880506 0.22953829 0.23113221\n",
      " 0.23269948 0.23506894 0.23596002 0.23630262 0.23803545 0.23836356\n",
      " 0.239812   0.2407858  0.24088012 0.24269466 0.24290901 0.24399349\n",
      " 0.2440032  0.24454804 0.24484405 0.24564777 0.24712423 0.25035757\n",
      " 0.25196097 0.25368009 0.25453445 0.25522394 0.25569604 0.25593134\n",
      " 0.25664117 0.25670889 0.25678641 0.25954305 0.25984996 0.26045004\n",
      " 0.26048397 0.26427336 0.26611331 0.26656082 0.26694149 0.26757859\n",
      " 0.26791691 0.26885262 0.27102054 0.27410645 0.27524964 0.27583937\n",
      " 0.27826092 0.2787835  0.27915364 0.28288163 0.28586416 0.28677922\n",
      " 0.2873603  0.28893733 0.28908382 0.29227288 0.29324697 0.30121709\n",
      " 0.30220303 0.30403468 0.30575352 0.30784862 0.30786988 0.3126217\n",
      " 0.31680153 0.32423238 0.32430708 0.32811259 0.33086446 0.33839648\n",
      " 0.34191017 0.3437082  0.34532667 0.34746238 0.34927417 0.35098953\n",
      " 0.36576366 0.36578295 0.36631921 0.37242308 0.37297535 0.37947375\n",
      " 0.37982794 0.38258708 0.38964366 0.39069166 0.39326957 0.39339921\n",
      " 0.39409712 0.41557872 0.42540416 0.42669286 0.42761742 0.42820233\n",
      " 0.43284223 0.4329494  0.44420218 0.44859928]\n",
      "[ 7  7 28 13  7 14 11 12  8  0 10 13  3 14 24 12 24 26  8 14 18 14 10 12\n",
      " 26  8 12  3  6 16  8 20 13 10  4 10 10 11 12  6  6  2 24 21  6  9 28 17\n",
      "  8  6 18 29 19 25  2 11 22  8 28 10  3  5 21 14 12  2 27 22 17  4 27  6\n",
      " 18 27 18 17  8 24 16  0  9  2 13  6 15 22 27  4  4  7 19 12  1 18  8  2\n",
      "  8 19  2 24]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2., 1., 6., 3., 4., 1., 7., 4., 9., 2., 6., 3., 7., 4., 5., 1., 2.,\n",
       "        3., 5., 3., 1., 2., 3., 0., 5., 1., 2., 4., 3., 1.]),\n",
       " array([ 0.        ,  0.96666667,  1.93333333,  2.9       ,  3.86666667,\n",
       "         4.83333333,  5.8       ,  6.76666667,  7.73333333,  8.7       ,\n",
       "         9.66666667, 10.63333333, 11.6       , 12.56666667, 13.53333333,\n",
       "        14.5       , 15.46666667, 16.43333333, 17.4       , 18.36666667,\n",
       "        19.33333333, 20.3       , 21.26666667, 22.23333333, 23.2       ,\n",
       "        24.16666667, 25.13333333, 26.1       , 27.06666667, 28.03333333,\n",
       "        29.        ]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Training Set\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred_train = model.predict(x_train)\n",
    "\n",
    "if trainingset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_train_originalform = y_pred_train/trainingset[\"VectorScaleFactor\"]\n",
    "    y_true_train_originalform = y_train/trainingset[\"VectorScaleFactor\"]\n",
    "elif trainingset[\"PreProcessMode\"] == \"Standarization\" or trainingset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", trainingset[\"PreProcessMode\"])\n",
    "    y_pred_train_originalform = trainingset[\"Scaler_Y\"].inverse_transform(y_pred_train)\n",
    "    y_true_train_originalform = trainingset[\"Scaler_Y\"].inverse_transform(y_train)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "#err = np.linalg.norm(y_true_train_originalform[:,-3:]-y_pred_train_originalform[:,-3:], axis=1)\n",
    "err = np.linalg.norm(y_true_train_originalform[:,-3:]-y_pred_train_originalform[:,-3:], axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted[-1000:])  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())\n",
    "\n",
    "##Plot prediction on the initial dataset\n",
    "err_initdata=err[0:12000+1]\n",
    "\n",
    "err_initdata_sorted = np.sort(err_initdata)\n",
    "print(err_initdata_sorted[-100:])  # print the 100 biggest error\n",
    "\n",
    "err_initdata_idx_sorted = np.argsort(err_initdata)\n",
    "print(err_initdata_idx_sorted[-100:]%30)\n",
    "selected_err=err_initdata_idx_sorted[-100:]%30\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(selected_err, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.00614227 0.00661179 0.00720222 ... 0.60391711 0.60718226 0.71030494]\n",
      "Error Mean:  0.06056017807312267\n",
      "Error Std 0.059227855889654304\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Validation Set\n",
    "\n",
    "y_pred_valid = model.predict(x_valid)\n",
    "\n",
    "\n",
    "if validationset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_valid_originalform = y_pred_valid/validationset[\"VectorScaleFactor\"]\n",
    "    y_true_valid_originalform = y_valid/validationset[\"VectorScaleFactor\"]\n",
    "elif validationset[\"PreProcessMode\"] == \"Standarization\" or validationset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", validationset[\"PreProcessMode\"])\n",
    "    y_pred_valid_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_pred_valid)\n",
    "    y_true_valid_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_valid)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_true_valid_originalform-y_pred_valid_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted)  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Kept Original Form, But need to scale back to meters\n",
      "[0.03204468 0.03336783 0.03373591 ... 1.31811443 1.34487174 1.34515033]\n",
      "Error Mean:  0.7922294691242362\n",
      "Error Std 0.37025940508621985\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Error Stat with Test Set\n",
    "\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "if testset[\"PreProcessMode\"] == \"OriginalForm\":\n",
    "    print(\"Data Kept Original Form, But need to scale back to meters\")\n",
    "    y_pred_test_originalform = y_pred_test/testset[\"VectorScaleFactor\"]\n",
    "    y_true_test_originalform = y_test/testset[\"VectorScaleFactor\"]\n",
    "elif testset[\"PreProcessMode\"] == \"Standarization\" or testset[\"PreProcessMode\"] == \"MaxAbs\":\n",
    "    print(\"PreProcessing of: \", validationset[\"PreProcessMode\"])\n",
    "    y_pred_test_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_pred_test)\n",
    "    y_true_test_originalform = validationset[\"Scaler_Y\"].inverse_transform(y_test)\n",
    "else:\n",
    "    raise Exception(\"Unknow Pre Process Mode\")\n",
    "\n",
    "#Compute Error\n",
    "err = np.linalg.norm(y_pred_test_originalform-y_true_test_originalform, axis=1)\n",
    "\n",
    "#Plot Histogram\n",
    "fig=plt.figure();   ax = fig.gca()\n",
    "plt.hist(err, bins=50, density = True, range = (0.0, 0.375))\n",
    "ax.set_xlabel(\"Normalised Error\")\n",
    "ax.set_xlim([-0.025,0.375])\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_ylim([-1,50])\n",
    "\n",
    "#### Sort the error\n",
    "\n",
    "err_sorted = np.sort(err)\n",
    "print(err_sorted)  # print the 100 biggest error\n",
    "\n",
    "print(\"Error Mean: \", err_sorted.mean())\n",
    "print(\"Error Std\", err_sorted.std())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a501000252d127e7c27d83f75df0a57bca228f59033b739034a7cde4260d0152"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
